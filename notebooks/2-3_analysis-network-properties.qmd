---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "2-3"

TRAIN_NB_ID = "2-1"
```

# Analysis of network properties

Here are some basic analyses of network weights and activities. 

1. Eigendecomposition of the Jacobian matrix at the goal steady-state.
2. Norm of the readout weights in different training conditions.
3. Correlation between top PCs and the network output -- this should be related to 2.

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict, namedtuple
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import numpy as np
import plotly
import plotly.colors as plc
import plotly.graph_objects as go
from tqdm.auto import tqdm

from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import (
    CurlField, 
    FixedField, 
    add_intervenors, 
    schedule_intervenor,
)
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
)
from rnns_learn_robust_motor_policies.misc import lohi, load_from_json, print_version_info
from rnns_learn_robust_motor_policies.train_setup_part2 import setup_task_model_pairs
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.plot import (
    add_endpoint_traces,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    figleaves,
    get_savefig_func,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    find_unique_filepath,
    get_base_task,
    set_model_noise,
    setup_train_histories,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_aligned_vars,
    get_forward_lateral_vel, 
    get_lateral_distance,
    get_pos_endpoints,
    orthogonal_field,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
    swap_model_trainables, 
)
from rnns_learn_robust_motor_policies.types import PertAmpDict, TrainStdDict
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
print_version_info(jax, eqx, plotly)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'random'
feedback_noise_std_load = 0.0
motor_noise_std_load = 0.0
feedback_delay_steps = 0
hidden_size = 50

# Specify model parameters to use for analysis (None -> use training value)
# disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps = int(feedback_delay_steps)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

```{python}
model_info = get_model_record(
    db_session,
    disturbance_type=disturbance_type_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps,
    hidden_size=hidden_size,
)
```

```{python}
models_load: TrainStdDict[float, eqx.Module] = load(
    model_info.path, partial(setup_models_only, setup_task_model_pairs),
)

replicate_info = load(
    model_info.replicate_info_path, partial(setup_replicate_info, models_load),
)
```

### Modify the system noise if needed

```{python}
models_base = jt.map(
    partial(
        set_model_noise, 
        noise_stds=dict(
            feedback=feedback_noise_std,
            motor=motor_noise_std,
        ),
        enable_noise=True,
    ),
    models_load,
    is_leaf=is_module,
)
```

### Optionally select a subset of training conditions

Depending on how training goes, we might want to leave out some of the training conditions (i.e. training disturbance stds) from the analysis.

```{python}
disturbance_train_stds = model_info.disturbance_stds

models_base = subdict(models_base, disturbance_train_stds)
```

### Optionally exclude replicates that perform much worse than average

When plotting single-replicate examples, we want to show the best replicate. Also, when lumping together replicates and plotting distributions, we want to include as many replicates as possible to show how performance may vary, but also exclude replicates whose performance is much worse than the best replicate.

:::{note}
The logic here is that systems like the brain will have much more efficient learning systems, and that we are approximating their efficiency by taking advantage of variance between model initializations. 

In other words: we are interested in the kind of performance that is feasible with these kinds of networked controllers, more than the kind of performance that we should expect on average (or in the worst case) given the technical details of network initialization etc.
:::

```{python}
exclude_underperformers = True
```

```{python}
measure_to_exclude_by = 'best_total_loss'

included_replicates = replicate_info['included_replicates'][measure_to_exclude_by]
best_replicate = replicate_info['best_replicates'][measure_to_exclude_by]

def take_replicate_or_best(tree: TrainStdDict, i_replicate=None, replicate_axis=1):
    if i_replicate is None:
        map_func = lambda tree: TrainStdDict({
            train_std: tree_take(subtree, best_replicate[train_std], replicate_axis)
            for train_std, subtree in tree.items()
        })
    else:
        map_func = lambda tree: tree_take_multi(tree, [i_replicate], [replicate_axis])
        
    return jt.map(map_func, tree, is_leaf=is_type(TrainStdDict))
```

Which replicates are included?

```{python}
print("\nReplicates included in analysis for each training condition:")
eqx.tree_pprint(jt.map(lambda x: jnp.where(x)[0], included_replicates), short_arrays=False)
```

Set the indices corresponding to the excluded replicates to NaN in each of the model arrays. This ensures that the shapes of the arrays remain consistent. NaN results will be ignored at plotting time. 

```{python}
models_all_replicates = models_base

if exclude_underperformers:
    models_base = jt.map(
        lambda models, included: tree_set_scalar(models, jnp.nan, jnp.where(~included)[0]),
        models_base, included_replicates,
        is_leaf=is_module,
    )
    n_replicates_included = jt.map(lambda x: jnp.sum(x).item(), included_replicates)
else:
    models_base = models_base
    n_replicates_included = dict.fromkeys(models.keys(), model_info.n_replicates)
```

**TODO**: Make an annotation that indicates "$n/N$ replicates are included" or something.

## Sort out the evaluation parameters

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
eval_parameters = use_record_params_where_none(dict(
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
), model_info)
```

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    # n_evals=n_evals['full'],
)
```

## Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    model_hash=model_info.hash,
    eval_parameters=eval_parameters,
    origin=NB_ID,
)
```

## Get goal steady-state fixed points

**TODO**: Vmap this calculation over the different goal positions, as well as over the values of the context input


## Eigendecomposition of goal steady-state Jacobians

**TODO**: We need a value of `h` to calculate these; i.e. we need to input the respective fixed points

```{python}
task = get_base_task()
```

```{python}
goals_pos = task.validation_trials.targets["mechanics.effector.pos"].value[:, -1]

def get_rnn_func(net):
    def rnn_func(key, pos, context, h): 
        return net(jnp.concatenate([pos, pos, context]), h, key=key)
    return rnn_func 
    
jacobian_funcs = jt.map(
    lambda model: jax.jacobian(get_rnn_func(model.step.net)),
    trained_models,
    is_leaf=is_module,
)

jacobians = jt.map(
    lambda jac_func: eqx.filter_vmap(
        eqx.filter_vmap(
            jac_func, 
            # Map over context inputs
            in_axes=(None, None, 0, None),
        ),
        # Map over goal positions
        in_axes=(None, 0, None, None),
    ),
    jacobian_funcs,
)
```