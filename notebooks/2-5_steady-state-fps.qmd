---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "2-4"

TRAIN_NB_ID = "2-1"
```

# Analysis of network properties

Here are some basic analyses of network weights and activities. 

1. Eigendecomposition of the Jacobian matrix at goal steady-states.
2. Correlation between top PCs and the network output.
3. Fixed point structures. 

In particular, we want to see how these vary 

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections.abc import Sequence
from functools import partial
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
from jaxtyping import Array, PyTree
import numpy as np
import plotly
import plotly.graph_objects as go
from sklearn.decomposition import PCA
from tqdm.auto import tqdm

from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.bodies import SimpleFeedbackState
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches, TrialSpecDependency
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.constants import WORKSPACE
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
    get_model_record,
    add_evaluation,
    add_evaluation_figure,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies.fp_finder import (
    FixedPointFinder,
    FPFilteredResults,
    fp_adam_optimizer,
    take_top_fps,
)
from rnns_learn_robust_motor_policies.misc import (
    create_arr_df, 
    log_version_info,
)
from rnns_learn_robust_motor_policies.train_setup_part2 import setup_task_model_pairs
from rnns_learn_robust_motor_policies.plot import plot_eigvals_df
from rnns_learn_robust_motor_policies.plot_utils import (
    figs_flatten_with_paths,
    figleaves,
)
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.setup_utils import (
    get_base_task,
    query_and_load_models,
    set_model_noise,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
    take_single_replicate,
)
from rnns_learn_robust_motor_policies.types import (
    ContextInputDict,
    PertAmpDict, 
    TrainingMethodDict,
    TrainStdDict,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
log_version_info(jax, eqx, plotly)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

```{python}
#| tags: [parameters]

# Specify which trained models to load 
training_methods_load = ['amplitude', 'std']
disturbance_type_load: Literal['curl', 'constant'] = 'curl'
disturbance_stds_load = [0.4, 0.8, 1.2]
feedback_noise_std_load = 0.0
motor_noise_std_load = 0.0
feedback_delay_steps = 0
hidden_size = 100

# Specify model parameters to use for analysis (None -> use training value)
# disturbance_type: Optional[Literal['curl', 'constant']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None

training_methods = None
```

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps = int(feedback_delay_steps)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
context_inputs = [-2., -1., 0., 1., 2.]
```

### Colors setup

```{python}
context_input_colors, context_input_colors_dark = get_colors_dicts(
    context_inputs, COLORSCALES['context_inputs'],
)
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

```{python}
all_models, model_info, replicate_info, n_replicates_included = query_and_load_models(
    db_session,
    setup_task_model_pairs,
    params_query=dict(
        origin=TRAIN_NB_ID,
        training_methods=training_methods_load,
        disturbance_type=disturbance_type_load,
        disturbance_stds=disturbance_stds_load,
        feedback_noise_std=feedback_noise_std_load,
        motor_noise_std=motor_noise_std_load,
        feedback_delay_steps=feedback_delay_steps,
        hidden_size=hidden_size,
    ),
    noise_stds=dict(
        feedback=feedback_noise_std,
        motor=motor_noise_std,
    ),
    tree_inclusions={
        TrainStdDict: None,
        TrainingMethodDict: training_methods,
    },
    exclude_underperformers_by='best_total_loss',
)
```

## Sort out the evaluation parameters and create a database record

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
eval_parameters = use_record_params_where_none(
    dict(
        feedback_noise_std=feedback_noise_std,
        motor_noise_std=motor_noise_std,
    ), 
    model_info,
)
```

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    context_inputs=context_inputs,
    # n_evals=n_evals['full'],
)
```

### Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    origin=NB_ID,
    model_hash=model_info.hash,
    eval_parameters=eval_parameters,
)
```

## Get steady-state fixed points

These are the "goal-goal" FPs; i.e. the point mass is at the goal, so the network will stabilize on some hidden state that outputs a constant force that does not change the position of the point mass on average.

This contrasts with non-steady-state fixed points, which correspond to network outputs which should cause the point mass to move, and thus the network's feedback input (and thus fixed point) to change.

### Define a task where the point mass is already at the goal

This is the same set of tasks (varying by context input) we used for the feedback perturbation analysis, except a bit shorter in duration.

```{python}
eval_grid_n = 5
EVAL_N_DIRECTIONS = 1
EVAL_REACH_LENGTH = 0.0  
```

```{python}
# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=WORKSPACE, 
    n_steps=model_info.n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=EVAL_N_DIRECTIONS,
    eval_reach_length=EVAL_REACH_LENGTH,  
)
```

```{python}
goals_pos = task.validation_trials.targets["mechanics.effector.pos"].value[:, -1]
```

```{python}
def get_context_input_func(x, n_steps, n_trials):
    return lambda trial_spec, key: (
        jnp.full((n_trials, n_steps - 1), x, dtype=float)
    )

all_tasks = ContextInputDict({
    context_input: eqx.tree_at(
        lambda task: task.input_dependencies,
        task, 
        {
            'context': TrialSpecDependency(get_context_input_func(
                context_input, model_info.n_steps, task.n_validation_trials
            ))
        },
    )
    for context_input in context_inputs
})
```

### Evaluate the model on the task, to get candidate hidden states

```{python}
def evaluate_all_states():
    # Normally this should return a TrainStdDict of ContextInputDict of SimpleFeedbackState
    return jt.map(
        lambda models: jt.map(
            lambda task: task.eval_ensemble(models, model_info.n_replicates, key=key_eval),
            # lambda task: task.eval(model, key=key_eval),
            all_tasks,
            is_leaf=is_module,
        ),
        all_models,
        is_leaf=is_module,
    )
```

```{python}
all_states_bytes = tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states))

print(f"\nEstimate {all_states_bytes / 1e9:.2f} GB of memory needed for all states.")
```

```{python}
all_states = evaluate_all_states()
```

### Instantiate the fixed point (FP) finder

```{python}
# Fixed point optimization hyperparameters
fp_tol = 1e-7  # Used for both fp_tol and opt_stop_tol
fp_num_batches = 10000         # Total number of batches to train on.
# fp_batch_size = 128          # How many examples in each batch
fp_step_size = 0.2          # initial learning rate
fp_decay_factor = 0.9999     # decay the learning rate this much
fp_decay_steps = 1           #
fp_adam_b1 = 0.9             # Adam parameters
fp_adam_b2 = 0.999
fp_adam_eps = 1e-5
fp_opt_print_every = 200   # Print training information during optimziation every so often

# Fixed point finding thresholds and other HPs
fp_noise_var = 0.0      # Gaussian noise added to fixed point candidates before optimization.
# fp_opt_stop_tol = 0.00001  # Stop optimizing when the average value of the batch is below this value.
# fp_tol = 0.00001        # Discard fps with squared speed larger than this value.
fp_unique_tol = 0.025   # tolerance for determination of identical fixed points
fp_outlier_tol = 1.0    # Anypoint whos closest fixed point is greater than tol is an outlier.
```

```{python}
fp_optimizer = fp_adam_optimizer(
    learning_rate=fp_step_size, 
    decay_steps=fp_decay_steps, 
    decay_rate=fp_decay_factor, 
    b1=fp_adam_b1, 
    b2=fp_adam_b2, 
    eps=fp_adam_eps, 
)

fpfinder = FixedPointFinder(fp_optimizer)

fpf_func = partial(
    fpfinder.find_and_filter,
    outlier_tol=fp_outlier_tol,
    unique_tol=fp_unique_tol,    
)
```

### Define functions to calculate FPs

```{python}
def get_ss_network_input_with_context(pos, context, rnn_cell):
    input_star = jnp.zeros((rnn_cell.input_size,))
    # Set target and feedback inputs to the same position
    input_star = input_star.at[1:3].set(pos) 
    input_star = input_star.at[5:7].set(pos)
    return input_star.at[0].set(context)
    

def get_ss_rnn_func_at_context(pos, context, rnn_cell):
    input_star = get_ss_network_input_with_context(pos, context, rnn_cell)
    def rnn_func(h):
        return rnn_cell(input_star, h, key=key_eval)
    return rnn_func


def get_ss_rnn_fps(pos, rnn_cell, candidate_states, context):
    fps = fpf_func(
        get_ss_rnn_func_at_context(pos, context, rnn_cell), 
        candidate_states, 
        fp_tol,
    )
    return fps
```

```{python}
@eqx.filter_jit
def get_ss_fps_at_positions(
    positions: float | Array, 
    all_models: PyTree[eqx.Module, 'T'], 
    all_states: PyTree[ContextInputDict[float, SimpleFeedbackState], 'T'],
    context_inputs: Sequence[float],
    stride_candidates: int = 8,
):
    """For each of one or more workspace positions, find the respective fixed point of the RNN
    whose inputs indicate it's reached a goal at that position.
    
    Repeat over a range of fixed values of the RNN's context input.
    
    Exclude any replicate for which no fixed point meeting the acceptance criteria was found, 
    for at least one context input.
    """
    if isinstance(positions, Array) and len(positions.shape) == 2:
        get_fps_func = eqx.filter_vmap(
            get_ss_rnn_fps, 
            in_axes=(0, None, None, None),  # Over grid positions
        )
    else:
        get_fps_func = get_ss_rnn_fps
        
    return jt.map(
        lambda models, states_by_context: ContextInputDict({
            context_input: eqx.filter_vmap(
                get_fps_func,
                in_axes=(None, 0, None, None),  # Over replicates
            )(
                positions,
                models.step.net.hidden,
                jnp.reshape(
                    states_by_context[context_input].net.hidden, 
                    (-1, model_info.hidden_size),
                )[::stride_candidates],
                context_input,
            )
            for context_input in context_inputs
        }),
        all_models, all_states,
        is_leaf=is_module,
    )
    
    
def process_fps(all_fps):
    """Only keep FPs/replicates that meet criteria."""
    n_fps_meeting_criteria = jt.map(
        lambda fps: fps.counts['meets_all_criteria'], 
        all_fps, 
        is_leaf=is_type(FPFilteredResults),
    )

    satisfactory_replicates = jt.map(
        lambda n_matching_fps_by_context: jnp.all(
            jnp.stack(jt.leaves(n_matching_fps_by_context), axis=0), 
            axis=0,
        ),
        n_fps_meeting_criteria,
        is_leaf=is_type(ContextInputDict),
    )

    # NOTE: We aren't actually indexing out `fps[mask]` here, since we want to be able to vmap 
    # the replicate dimension here, with that of the models, when calculating the Jacobians. 
    # Later, we can exclude replicates from the final results based on `satisfactory_replicates`.

    all_top_fps = take_top_fps(all_fps, n_keep=6)
    
    # Average over the top fixed points, to get a single one for each included replicate and 
    # control input.
    fps_final = jt.map(
        lambda top_fps_by_context, mask: jt.map(
            lambda fps: jnp.nanmean(fps, axis=-2),  
            top_fps_by_context,
            is_leaf=is_type(FPFilteredResults),
        ),
        all_top_fps, satisfactory_replicates,
        is_leaf=is_type(ContextInputDict),
    )
    
    return fps_final, all_top_fps, n_fps_meeting_criteria, satisfactory_replicates
```

### Get steady-state fixed points at the workspace origin

```{python}
ORIGIN = 0.0  # Broadcasts because we use at-set assignment
```

```{python}
fps = get_ss_fps_at_positions(
    ORIGIN, all_models, all_states, context_inputs,
)

fps_origin, all_top_fps, n_fps_meeting_criteria, satisfactory_replicates = process_fps(fps)
```

How many fixed points met all criteria? 

```{python}
print("\nNumber of fixed points meeting all criteria:")
eqx.tree_pprint(
    n_fps_meeting_criteria,
    short_arrays=False,
)
```

Note that *no* points may be found for some replicates/context inputs, but that's OK. 

:::{note}
Sometimes more than one "fixed point" meets all criteria, but typically these are actually the same fixed point, except that due to differences in optimization, some context inputs converged better than others. 

To ensure that all 'redundant' FPs actually converge to a single FP, we could try to vary `fp_tol`, or use some kind of adaptive convergence criteria. Instead, it is simple enough for now to test if the "different" points are very far from each other, and if not, just average them and keep going. 

To see whether they are far away from each other, we can look at the standard deviation over the (up to) top 5 returned FPs, averaged over units.

```{python}
top_fps_std_mean = jt.map(
    lambda fps: jnp.nanmean(jnp.nanstd(fps, axis=1), axis=-1),
    all_top_fps,
)

print("\nStandard deviation of top 5 fixed points, mean over dimensions (network units):")
eqx.tree_pprint(top_fps_std_mean, short_arrays=False)
```

At this point we could exclude replicates based on a threshold for these standard deviation means. However, I've found that for our networks in general, there is a single actual fixed point for each fixed network input. I will assume that's the case, and in the following analyses we'll simply use whatever fixed points were returned as `fps_final`.

If the above printout of standard deviations has any large values -- say, much larger than 0.01 -- then this policy may need to be revisited. Thus, raise an error if that's the case, but otherwise do not worry about it.

```{python}
threshold = 0.01

all_top_fps_std_mean = jnp.stack(jt.leaves(top_fps_std_mean))
below_threshold = all_top_fps_std_mean < threshold

assert jnp.all(below_threshold | jnp.isnan(all_top_fps_std_mean)), (
    "\nStandard deviation between multiple candidate fixed points is high. " 
    "Consider excluding replicates for which this is true. "
)
```
:::

For which replicates was a FP found for every context input?

```{python}
print("\nReplicates for which a fixed point was found for every context input:")
eqx.tree_pprint(satisfactory_replicates, short_arrays=False)
```

### Get fixed points at a grid of steady-state positions

```{python}
fps = get_ss_fps_at_positions(
    goals_pos, all_models, all_states, context_inputs,
)

fps_grid, all_top_fps_grid, n_fps_meeting_criteria_grid, satisfactory_replicates_grid = process_fps(fps)
```

## Visualization of steady-state grid FPs

How much do the fixed points vary across workspace positions? 

Is there translation invariance, or at the other extreme, does every steady-state point correspond to a different zero-force fixed point?

## Eigendecomposition of the steady-state Jacobians

Now that we have fixed points (network states) for the steady states, we can linearize the network around these points.

:::{note}
Ideally, for each model we want a function that takes a context input and returns a square Jacobian matrix (i.e. from hidden states to hidden states). However, note that we had to compute the fixed points *given* the context input. So for now we will simply calculate a set of Jacobians for the context inputs we *did* evaluate. 

In the future, we may be able to get those functions-of-context by wrapping the fixed point calculation up with the Jacobian calculation, however that might be kind of messy given that the fixed point calculation involves some filtering of candidates. 
:::

### Define functions

```{python}
def get_jac_func(position, context, func):
    return jax.jacobian(get_ss_rnn_func_at_context(position, context, func))
    
def get_jacobian(position, context, fp, func):
    return get_jac_func(position, context, func)(fp)
```

```{python}
@eqx.filter_jit
def get_all_jacobians(positions, all_fps, all_models):
    get_jac = eqx.filter_vmap(  # Over replicates
        get_jacobian, 
        in_axes=(None, None, 0, 0),
    )
    
    if isinstance(positions, Array) and len(positions.shape) == 2:
        get_jac = eqx.filter_vmap(  # Over positions
            get_jac,
            in_axes=(0, None, 1, None),  
        )
    
    jacobians = jt.map(
        lambda models, fps_by_context: ContextInputDict({
            context_input: get_jac(  
                positions, context_input, fps, models.step.net.hidden,
            )
            for context_input, fps in fps_by_context.items()
        }),
        all_models, all_fps,
        is_leaf=is_module,
    )

    jacobians_stacked = jt.map(
        lambda jacobians_by_context: jnp.stack(jt.leaves(jacobians_by_context), axis=0),
        jacobians,
        is_leaf=is_type(ContextInputDict),
    )
    
    return jacobians_stacked
```

### Compute all Jacobians

```{python}
jacobians_origin = get_all_jacobians(
    ORIGIN, fps_origin, all_models,
)

jacobians_grid = get_all_jacobians(
    goals_pos, fps_grid, all_models,
)
```

### Eigendecomposition of Jacobians

How does the context input affect the stability the local linearization at steady-state fixed points? To see this, we can compare the eigenspectra of the Jacobians.

It's easier to just do this on the CPU since we need non-symmetric decomposition and the matrix is only 100x100 or so.

```{python}
eig_cpu = jax.jit(
    lambda *args, **kwargs: tuple(jax.lax.linalg.eig(*args, **kwargs)), 
    device=jax.devices('cpu')[0],
)
```

```{python}
eigvals, eigvecs_l, eigvecs_r = tree_unzip(jt.map(eig_cpu, jacobians_grid))
```

:::{note}
Where are the eigenvalues non-NaN across all context inputs?

```{python}
eigvals_valid = jt.map(
    lambda eigvals: jnp.all(~jnp.isnan(
        jnp.nanmean(eigvals, axis=-1)  
    ), axis=0), 
    eigvals,
)
```

Sanity check: this should look the same (nearly?) as `satisfactory_replicates`:

```{python}
eqx.tree_pprint(tree_stack([eigvals_valid, satisfactory_replicates_grid]), short_arrays=False)
```

```{python}
eigvals_valid_idxs = jt.map(
    lambda x: jnp.where(x), 
    eigvals_valid, 
)
```

In the origin-steady-state case, we could exclude entire replicates based on whether the eigenvalues are NaN for at least one context input. 

**However, for the grid case, for a single replicate, some steady-state points may have all valid eigenvalues while others do not, for the same context input. Thus while some replicates are clearly worse than others (e.g. for *every* steady-state point, at least one of the context inputs has NaN eigenvalues, and thus an entire row of `eigvals_valid` is `False`), it may not be the case that any row of `eigvals_valid` is all `True`.** This is why I've put this material in a callout; the indexing by `eigvals_valid_idxs` does not work in the grid case. Instead, for finding distributions I will lump over all replicates, and assuming the variation between replicates is smaller than the variation between context inputs, this should work well.

```{python}
# eigvals_final = jt.map(
#     lambda eigvals, idxs: jt.map(lambda vals: vals[:, idxs], eigvals),
#     eigvals, eigvals_valid_idxs,
#     is_leaf=is_type(ContextInputDict),
# )
```
:::

```{python}
i_replicate = 0
```

The following is the original plotting function I used. 

```{python}
# from plotly.colors import sample_colorscale

# # Construct a boundec colorscale, so plotly will discretize the colorbar
# colors = list(context_input_colors_dark.values())

# def get_discretized_colorscale(colors):
#     colors = list(colors)
#     return [
#         [x + j, colors[i]] 
#         for i, x in enumerate(np.linspace(0, 1, len(colors), endpoint=False)) 
#         for j in [0, 1 / len(colors)]
#     ]

# def plot_eigvals(eigvals, i_replicate, colorscale='Viridis'):
#     tickvals = list(range(len(context_inputs)))

#     return fbp.plot_eigvals(
#         jnp.take(eigvals, i_replicate, axis=-2),
#         colors=np.arange(0, len(context_inputs)).tolist(),
#         colorscale=colorscale,
#         marker_size=3,
#         marker_colorbar=dict(
#             title="Context input",
#             tickvals=tickvals,
#             labelalias=dict(zip(tickvals, [str(x) for x in context_inputs])),
#         ),
#         layout_kws=dict(
#             width=600,
#             height=500,
#         )
#     )

# colorscale = get_discretized_colorscale(context_input_colors_dark.values())

# eigval_figs = jt.map(
#     lambda eigvals: plot_eigvals(
#         eigvals,
#         i_replicate, 
#         colorscale=colorscale,
#     ),
#     eigvals,
# )
```

However, using `px.scatter` is very convenient since it allows us to add marginal distributions, and to change the dimension we're coloring over, very easily. 

```{python}
# Create the dataframe
eigval_dfs = jt.map(
    lambda arr: create_arr_df(
        jnp.take(arr, i_replicate, axis=-2),
        col_names=['context', 'pos', 'eigenvalue']
    ).astype({'context': 'str'}),
    eigvals,
)
```

```{python}
eigval_figs = jt.map(
    lambda eigvals: plot_eigvals_df(
        eigvals,
        marginals='box',
        color='context',
        color_discrete_sequence=list(context_input_colors_dark.values()),
        trace_kws=dict(marker_size=3),
        layout_kws=dict(
            legend_title='Context input', 
            legend_itemsizing='constant',
            xaxis_title='Re',
            yaxis_title='Im',
        ),
    ),
    eigval_dfs,
)

# Reverse all the traces, since this improves the visualization in this particular case
jt.map(
    lambda fig: setattr(fig, 'data', fig.data[::-1]),
    eigval_figs,
    is_leaf=is_type(go.Figure),
)

# Label the traces/legend with actual context inputs, rather than indices 
jt.map(
    lambda fig: fig.for_each_trace(
        lambda t: t.update(name=context_inputs[int(t.name)]) 
        if t.name is not None else t
    ),
    eigval_figs,
    is_leaf=is_type(go.Figure),
)
```

```{python}
plot_id = 'steady_state_jacobian_eigvals/grid'

for path, fig in tqdm(figs_flatten_with_paths(eigval_figs)):
    fig_params = dict(
        training_method=path[0].key,
        disturbance_type_train=model_info.disturbance_type,
        disturbance_train_std=path[1].key,
        eval_grid_n=eval_grid_n,  # These plots lump eigenvalues from a bunch of workspace grid positions
    )
    
    add_evaluation_figure(db_session, eval_info, fig, plot_id, **fig_params)
    
    print(path[0].key, path[1].key)
    fig.show()
```

### Distributions of eigenvalues




## Feedback perturbation at steady-state

Here, the "init-goal" FPs correspond to a steady-state network input where one of the feedback channels has been perturbed. 

In particular, we could ramp up the perturbation from zero, and see how the FP changes, relative to the steady-state one.

**TODO**: We don't need to perturb the task here necessarily -- not even to get the candidate FPs. We can probably use the steady-state FPs as the candidates (perhaps augmented with some jitter to get a batch) and simply ramp the relevant network input and repeat the FP calculation. We can do this on a smaller grid than before, since we want to make sure that there isn't huge variation between FPs but also this variation isn't our primary concern here. 