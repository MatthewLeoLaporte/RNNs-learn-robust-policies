---
jupyter: python3
---

# Training models for Part 1

In this notebook we train several models separately from each other, to support later analyses of whether RNNs can be learn robust policies for a simple continuous control task. In particular, we train an RNN to drive a point mass from an initial position to a target position (i.e. simple reaching) in a 2D workspace. One model will be trained to do this undisturbed, while another will experience a disturbance which cannot be modeled (curl force fields of random direction and magnitude) on each trial/episode. In the latter case, two variants of the model will be trained, at two levels of average disturbance magnitude.

Multiple replicates of the model are trained in each case, to examine the effect of network weight initialization. 

Each model consists of a single-layer RNN with a linear readout layer, which controls a biomechanical model, forming a closed loop. On each time step of a trial, the RNN outputs a 2D force vector which accelerates a Newtonian point mass across a 2D workspace. The input to the RNN on each time step includes feedback observations of the point mass's position and velocity, as well as the target/reference position and velocity. In the case of simple reaching, the target state is constant across the trial -- the goal position and velocity of the reach.

Symmetric (Gaussian) noise is added to the network's outputs and to the feedback observations on each time step, to model motor and sensory noise in the system.

The training objective is to minimize a continuous cost function which penalizes a sum of the following squared terms:

- the Euclidean distance between the point mass's position and the target position on each time step;
- the **difference** between the point mass's velocity and the target velocity on each time step;
- the force vector output by the neural network;
- the activations of the network units.

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict
from datetime import datetime
import json
from pathlib import Path
from typing import Literal

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt 
import numpy as np
import optax 
from tqdm.auto import tqdm

from feedbax import (
    AbstractModel, 
    get_ensemble,
    is_module,
    load,
    save,
    tree_unzip,
    tree_stack,
    tree_take_multi,
    is_type,
)
from feedbax.channel import toggle_channel_noise
from feedbax.misc import git_commit_id, where_func_to_labels
import feedbax.plotly as fbp
import feedbax.plot as fbplt
from feedbax.xabdeef.losses import simple_reach_loss
from feedbax.xabdeef.models import point_mass_nn
from feedbax.task import SimpleReaches
from feedbax.train import TaskTrainer

from feedbax.intervene import (
    CurlField, 
    FixedField,
    schedule_intervenor,
)

from rnns_learn_robust_motor_policies.plot_utils import get_savefig_func
from rnns_learn_robust_motor_policies.setup_utils import filename_join as join
from rnns_learn_robust_motor_policies.misc import write_to_json
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx, optax): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"feedbax commit: {git_commit_id()}\n")
```

Unique ID for notebook, for naming outputs.
 
```{python}
NB_ID = "1-1"
```

Datetime string generator, for timestamping saved files.

```{python}
get_datestr = lambda: datetime.now().strftime("%Y%m%d-%Hh%M")
```

### Hyperparameters

These are parameters that can be [varied](https://quarto.org/docs/computations/parameters.html) through the command line interface to Quarto. 

```{python}
#| tags: [parameters]

disturbance_type: Literal['curl', 'random'] = 'random'  
feedback_delay_steps = 0
feedback_noise_std = 0.0
motor_noise_std = 0.0
hidden_size = 25
n_replicates = 10
dt = 0.05
```

```{python}
suffix = '_'.join([
    f"{hidden_size}-units",
    f"{disturbance_type}",
    f"noise-{feedback_noise_std}-{motor_noise_std}",
    f"delay-{feedback_delay_steps}",
])
```

### Directories setup

```{python}
FIGS_DIR = Path(f'../figures/{NB_ID}/{suffix}')
MODELS_DIR = Path('../models')

for d in (FIGS_DIR, MODELS_DIR):
    d.mkdir(parents=True, exist_ok=True)
```

### Plotting setup

Get function to save figures to figure directory.

```{python}
savefig = get_savefig_func(FIGS_DIR)
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Base training task setup

```{python}
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

task_train = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=2,
    eval_n_directions=8,
    eval_reach_length=0.5,    
)
```

## Model setup

```{python}
mass = 1.0

models = get_ensemble(
    point_mass_nn,
    task_train,
    n_ensemble=n_replicates,
    dt=dt,
    mass=mass,
    hidden_size=hidden_size, 
    n_steps=n_steps,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    key=key_init,
)
```

## Training setup

### Create model-task pairings for different disturbance conditions

Schedule force fields in the training trials.

```{python}
if disturbance_type == 'curl':
    disturbance_stds = [0, 0.2, 0.4, 0.8, 1.6, 2.4]
    # disturbance_stds = [0, 0.4, 1.2]
   
    def disturbance(curl_std):
        return CurlField.with_params(
            scale=curl_std,
            amplitude=lambda trial_spec, *, key: jr.normal(key, (1,)),
        )

elif disturbance_type == 'random':
    disturbance_stds = [0, 0.001, 0.01, 0.1, 1.0, 2.0]
    # disturbance_stds = [0, 0.01, 1.0]
    
    # Uniform in angle, Gaussian in amplitude, constant over trial
    def disturbance(field_std):
        def vector_with_gaussian_length(trial_spec, *, key):
            key1, key2 = jr.split(key)
            
            angle = jr.uniform(key1, (), minval=-jnp.pi, maxval=jnp.pi)
            length = jr.normal(key2, ())
        
            return jnp.array([jnp.cos(angle), jnp.sin(angle)]) * length
            
        return FixedField.with_params(
            scale=field_std,
            field=vector_with_gaussian_length,
        )
else:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")


task_model_pairs = jt.map(
    lambda disturbance_std: schedule_intervenor(
        task_train, models,
        lambda model: model.step.mechanics,
        disturbance(disturbance_std),
        default_active=False,
    ),
    disturbance_stds,
)
```

### Training hyperparameters and optimizer

```{python}
learning_rate = 0.01
n_batches = 10000
batch_size = 250

where_train = lambda model: (
    model.step.net.hidden,
    model.step.net.readout, 
)

# Define the training iterations on which to retain the model weights:
# Every iteration until iteration 10, then every 10 until 100, every 100 until 1000, etc.
save_model_parameters = jnp.concatenate([jnp.array([0])] + [
    jnp.arange(10**i, 10**(i+1), 10**i)
    for i in range(0, int(np.log10(n_batches)))
])
```

```{python}
trainer = TaskTrainer(
    optimizer=optax.inject_hyperparams(optax.adam)(
        learning_rate=learning_rate
    ),
    checkpointing=True,
)
```

## Train the models

```{python}
trained_models = OrderedDict({})
train_histories = OrderedDict({})

for disturbance_std, (task_, models_) in tqdm(list(zip(disturbance_stds, task_model_pairs))):
    trained_models[disturbance_std], train_histories[disturbance_std] = trainer(
        task_, 
        models_,
        ensembled=True,
        n_batches=n_batches, 
        batch_size=batch_size, 
        log_step=n_batches // 10,
        where_train=where_train,
        save_model_parameters=save_model_parameters,
        disable_tqdm=True,
        key=key_train,
    )
```

## Save the models with their parameters on the final iteration

```{python}
model_file_label = "trained_models"
train_histories_file_label = "train_histories"
```

```{python}
# datestr = get_datestr()

model_basename = join([
    NB_ID,
    suffix,
    model_file_label,
])
```

```{python}
training_hyperparameters = dict(
    learning_rate=learning_rate,
    n_batches=n_batches,
    batch_size=batch_size,
    save_model_parameters=save_model_parameters.tolist(),
    where_train_strs=where_func_to_labels(where_train),
)
```

```{python}
model_hyperparameters = dict(
    n_replicates=n_replicates,
    hidden_size=hidden_size,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    dt=dt,
    n_steps=n_steps,
    mass=mass,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds,
)
```

```{python}
save(
    str(MODELS_DIR / model_basename) + '.eqx',
    trained_models,
    hyperparameters=model_hyperparameters,
)

write_to_json(
    model_hyperparameters | training_hyperparameters,
    str(MODELS_DIR / model_basename.replace(model_file_label, 'hyperparameters')) + '.json',
)
```

To reload our models in later notebooks, we will need a function that regenerates a skeleton PyTree with the same structure as `saved_models`, given the stored `model_hyperparameters`. See the function `setup_models` in `src/rnns_learn_robust_motor_policies/part1_setup.py`.

### Save training histories

In later analyses we may be interested in the history of the losses or model parameters across training, so these should also be saved to disk. 

```{python}
train_histories_basename = model_basename.replace(
    model_file_label, 
    train_histories_file_label,
)

save(
    str(MODELS_DIR / train_histories_basename) + ".eqx",
    train_histories,
    hyperparameters=dict(
        disturbance_stds=disturbance_stds,
        n_batches=n_batches,
        batch_size=batch_size,
        n_replicates=n_replicates,
        where_train_strs=where_func_to_labels(where_train),
        save_model_parameters=save_model_parameters.tolist(),
    ),
)
```

Note that we cannot save lambda functions to disk, so we save the string representation of the `where_train` PyTree.

#### Reloading parameter histories

I'm including this here for reference.

Because we'll generally have already loaded the trained models by the time we also want to load the train histories, our `setup_train_histories` function can use it to determine the correct PyTree structure. Thus we can avoid reconstructing the PyTree of models from scratch -- and storing all the required hyperparameters -- a second time.  

```{python}
from functools import partial
from rnns_learn_robust_motor_policies.part1_setup import setup_train_histories

model_parameter_histories_ = load(
    str(MODELS_DIR / train_histories_basename) + ".eqx",
    partial(setup_train_histories, trained_models),
)
```

## For each training condition, find the best-performing replicate, and the best parameters for each replicate

**TODO**: Refactor `part1_post_training.py` into functions, and (probably) remove this section.

Some model replicates may diverge at some point during training. Thus, considering the iterations for which we kept model parameters, we will keep the parameters from the iteration with the lowest total loss, for each replicate. 

```{python}
# For each ensemble, what are the indices in `save_model_parameters` of the lowest-loss parameters?
argmin_loss_by_replicate = jt.map(
    lambda history: jnp.argmin(
        history.loss.total[save_model_parameters], 
        axis=0,
    ), 
    train_histories, 
    is_leaf=is_module,
)

# What are the corresponding training iterations? 
best_iteration_by_replicate = jt.map(
    lambda idx: save_model_parameters[idx].tolist(), 
    argmin_loss_by_replicate, 
)
```

```{python}
best_parameters_by_replicate = tree_stack([
    jt.map(
        lambda train_history, best_idx_by_replicate: tree_take_multi(
            train_history.model_parameters, 
            [int(best_idx_by_replicate[i]), i], 
            [0, 1],
        ),
        train_histories, argmin_loss_by_replicate,
        is_leaf=is_module,
    )
    for i in range(n_replicates)
])
```

```{python}
loss_at_best_iteration_by_replicate = jt.map(
    lambda history, saved_iterations: (
        history.loss.total[jnp.array(saved_iterations), jnp.arange(n_replicates)]
    ),
    train_histories, best_iteration_by_replicate,
    is_leaf=is_module,
)

loss_at_final_iteration_by_replicate = jt.map(
    lambda history: history.loss.total[-1],
    train_histories,
    is_leaf=is_module,
)
```

```{python}
best_replicate_by_loss = jt.map(
    lambda best_losses: jnp.argmin(best_losses).item(), 
    loss_at_best_iteration_by_replicate,
    is_leaf=is_module,
)
```

### Save another copy of the models, with the best parameters for each replicate

```{python}
models_with_best_parameters = jt.map(
    lambda model, best_params: eqx.tree_at(
        where_train,
        model, 
        where_train(best_params),
    ),
    trained_models, best_parameters_by_replicate,
    is_leaf=is_module,
)
```

```{python}
save(
    str(MODELS_DIR / model_basename) + '_best_params.eqx',
    models_with_best_parameters,
    hyperparameters=model_hyperparameters,
)
```

### Compute exclusion criteria for replicates

Determine which replicates have a best total loss in the 20th percentile, in case we want to exclude these from further analysis. 

**TODO**: the best losses are not terribly variable, so it might be better to exclude models based on a behavioural measure, such as the endpoint error on the control (no perturbation) task. Also, in cases where most of the replicates perform poorly, then excluding by percentile may end up including some models that perform poorly.

```{python}
percentile = 75

exclusion_loss_bound = jt.map(
    lambda losses: jnp.percentile(losses, percentile).item(),
    loss_at_best_iteration_by_replicate,
)

included_replicates = jt.map(
    lambda losses, bound: losses < bound,
    loss_at_best_iteration_by_replicate, exclusion_loss_bound,
)
```

### Save the extra info about replicates to JSON

```{python}
extras = dict(
    best_iteration_by_replicate=best_iteration_by_replicate,
    loss_at_best_iteration_by_replicate=loss_at_best_iteration_by_replicate,
    best_replicate_by_loss=best_replicate_by_loss,
    included_replicates=included_replicates,    
)
```

```{python}
write_to_json(
    extras,
    str(MODELS_DIR / model_basename.replace(model_file_label, 'extras')) + '.json',
)
```

## Generate training plots

### Loss histories 

```{python}

for disturbance_std, history in train_histories.items():
    label = join([
        NB_ID,
        "loss-history",
        f"{disturbance_type}",
        f"std-{disturbance_std}",
        f"replicates-{n_replicates}",
    ])
    
    p1, _ = fbplt.loss_history(history.loss)
    # p2, _ = fbplt.loss_mean_history(history.loss)
    p3 = fbp.loss_history(history.loss)
    
    savefig(p1, join([label, 'replicates']))
    # savefig(p2, label)
    savefig(p3, label)
```

TODO: these plots are large/sluggish for `n_steps` >> 1e3. Can probably downsample; start with every 1 to 1000, then every 10 to 10000.

### Distribute of losses over replicates

i.e. for different disturbance levels, what is the distribution over replicates of the minimum (over training iterations) of the total loss?

```{python}
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px

for label, losses_by_replicate in {
    "Best": loss_at_best_iteration_by_replicate,
    "Final": loss_at_final_iteration_by_replicate,
}.items():
    df = pd.DataFrame(losses_by_replicate).reset_index().melt(id_vars='index')
    df["index"] = df["index"].astype(str)

    fig = go.Figure()

    strips = px.scatter(
        df,
        x='variable',
        y='value',
        color="index",
        labels=dict(x="Train disturbance std.", y=f"{label} total loss"),
        title=f"{label} total loss across training by disturbance std.",
        color_discrete_sequence=px.colors.qualitative.Plotly,
        # stripmode='overlay',
    )
    
    strips.update_traces(
        marker_size=10,
        marker_symbol='circle-open',
        marker_line_width=3,
    )
    
    # strips.for_each_trace(
    #     lambda trace: trace.update(x=list(trace.x))
    # )

    violins = [
        go.Violin(
            x=[disturbance_std] * n_replicates,
            y=losses_by_replicate[disturbance_std],
            # box_visible=True,
            line_color='black',
            meanline_visible=True,
            fillcolor='lightgrey',
            opacity=0.6,
            name=f"{disturbance_std}",
            showlegend=False,     
        )
        for disturbance_std in disturbance_stds
    ]
    
    fig.add_traces(violins)
    fig.add_traces(strips.data)

    fig.update_layout(
        xaxis_type='category',
        width=800,
        height=500,
        xaxis_title="Train disturbance std.",
        yaxis_title=f"{label} total loss",
        # xaxis_range=[-0.5, len(disturbance_stds) + 0.5],
        # xaxis_tickvals=np.linspace(0,1.2,4),
        # yaxis_type='log',
        violingap=0.1,
        # showlegend=False,
        legend_title='Replicate',
        legend_tracegroupgap=4,
        # violinmode='overlay',  
        barmode='overlay',
        # boxmode='group',
    )

    fig.show()

    savefig(fig, f"{label.lower()}-loss-distn-by-replicate")
```