---
jupyter: python3
---

# Training models for Part 1

In this notebook we train several models separately from each other, to support later analyses of whether RNNs can be learn robust policies for a simple continuous control task. In particular, we train an RNN to drive a point mass from an initial position to a target position (i.e. simple reaching) in a 2D workspace. One model will be trained to do this undisturbed, while another will experience a disturbance which cannot be modeled (curl force fields of random direction and magnitude) on each trial/episode. In the latter case, two variants of the model will be trained, at two levels of average disturbance magnitude.

Multiple replicates of the model are trained in each case, to examine the effect of network weight initialization. 

Each model consists of a single-layer RNN with a linear readout layer, which controls a biomechanical model, forming a closed loop. On each time step of a trial, the RNN outputs a 2D force vector which accelerates a Newtonian point mass across a 2D workspace. The input to the RNN on each time step includes feedback observations of the point mass's position and velocity, as well as the target/reference position and velocity. In the case of simple reaching, the target state is constant across the trial -- the goal position and velocity of the reach.

Symmetric (Gaussian) noise is added to the network's outputs and to the feedback observations on each time step, to model motor and sensory noise in the system.

The training objective is to minimize a continuous cost function which penalizes a sum of the following squared terms:

- the Euclidean distance between the point mass's position and the target position on each time step;
- the **difference** between the point mass's velocity and the target velocity on each time step;
- the force vector output by the neural network;
- the activations of the network units.

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict
from functools import partial
import json
from pathlib import Path
from typing import Literal

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt 
import numpy as np
import optax 
from tqdm.auto import tqdm

from feedbax import (
    AbstractModel, 
    get_ensemble,
    is_module,
    is_type,
    load,
    save,
    tree_map_tqdm,
    tree_stack,
    tree_take_multi,
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import (
    CurlField, 
    FixedField,
    schedule_intervenor,
)
from feedbax.misc import git_commit_id, where_func_to_labels
import feedbax.plot as fbplt
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.train import TaskTrainer
from feedbax.xabdeef.losses import simple_reach_loss
from feedbax.xabdeef.models import point_mass_nn

from rnns_learn_robust_motor_policies.constants import MASS, WORKSPACE
from rnns_learn_robust_motor_policies.misc import write_to_json
from rnns_learn_robust_motor_policies.part1_setup import setup_task_model_pairs
from rnns_learn_robust_motor_policies.plot_utils import get_savefig_func
from rnns_learn_robust_motor_policies.setup_utils import filename_join as join
from rnns_learn_robust_motor_policies.tree_utils import pp
from rnns_learn_robust_motor_policies.types import TaskModelPair
from rnns_learn_robust_motor_policies.train_setup import (
    iterations_to_save_model_parameters,
    make_delayed_cosine_schedule,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx, optax): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"feedbax commit: {git_commit_id()}\n")
```

Unique ID for notebook, for naming outputs.
 
```{python}
NB_ID = "1-1"
```

### Hyperparameters

These are parameters that can be [varied](https://quarto.org/docs/computations/parameters.html) through the command line interface to Quarto. 

```{python}
#| tags: [parameters]

disturbance_type: Literal['curl', 'random'] = 'curl'  
feedback_delay_steps = 0
feedback_noise_std = 0.0
motor_noise_std = 0.0
hidden_size = 50
n_replicates = 5
n_steps = 100
dt = 0.05
cosine_annealing_alpha = 0.01
```

```{python}
suffix = '_'.join([
    # f"{hidden_size}-units",
    # f"alpha-{cosine_annealing_alpha}",
    f"{disturbance_type}",
    f"noise-{feedback_noise_std}-{motor_noise_std}",
    f"delay-{feedback_delay_steps}",
])
```

### Directories and filenames setup

```{python}
FIGS_BASE_DIR = Path('/mnt/storage/tmp/figures')
MODELS_DIR = Path('../models')

figs_dir = FIGS_BASE_DIR / f'{NB_ID}/{suffix}'

for d in (figs_dir, MODELS_DIR):
    d.mkdir(parents=True, exist_ok=True)
```

```{python}
MODEL_FILE_LABEL = "trained_models"
TRAIN_HISTORIES_FILE_LABEL = "train_histories"
HYPERPARAMS_FILE_LABEL = "hyperparameters"
```

### Plotting setup

Get function to save figures to figure directory.

```{python}
savefig = get_savefig_func(figs_dir)
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Create model-task pairings for different disturbance conditions

```{python}
disturbance_stds = {
    'curl': [0, 0.8, 1.6], # [0, 0.4, 0.8, 1.6, 2.4],
    'random': [0, 0.01, 0.1, 1.0, 2.0],
}

if not disturbance_type in ['curl', 'random']:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")

task_model_pairs = setup_task_model_pairs(
    n_replicates=n_replicates,
    dt=dt,
    mass=MASS,
    hidden_size=hidden_size,
    n_steps=n_steps,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
    key=key_init,    
)
```


## Training setup

### Training hyperparameters and optimizer

```{python}
learning_rate_0 = 0.01
constant_lr_iterations = 10  # Number of initial training iterations to hold lr constant
alpha = cosine_annealing_alpha  # Max learning rate factor decrease during cosine annealing (see notebook hyperparameters)
weight_decay = 0

n_batches = 1000
batch_size = 250

optimizer_class = partial(
    optax.adamw,
    weight_decay=weight_decay,
)

where_train = lambda model: (
    model.step.net.hidden,
    model.step.net.readout, 
)

save_model_parameters = iterations_to_save_model_parameters(n_batches)
```

```{python}
schedule = make_delayed_cosine_schedule(
    learning_rate_0, 
    constant_lr_iterations, 
    n_batches, 
    alpha,
) 

trainer = TaskTrainer(
    optimizer=optax.inject_hyperparams(optimizer_class)(
        learning_rate=schedule,
    ),
    checkpointing=True,
)
```

## Train the models

```{python}
def train_pair(pair):
    return trainer(
        pair.task, 
        pair.model,
        ensembled=True,
        n_batches=n_batches, 
        batch_size=batch_size, 
        log_step=n_batches // 10,
        where_train=where_train,
        save_model_parameters=save_model_parameters,
        # disable_tqdm=True,
        key=key_train,
    )

trained_models, train_histories = tree_unzip(tree_map_tqdm(
    train_pair,
    task_model_pairs,
    is_leaf=is_type(TaskModelPair),
))
```

## Save the models with their parameters on the final iteration

```{python}
# datestr = get_datestr()

model_basename = join([
    NB_ID,
    suffix,
    MODEL_FILE_LABEL,
])
```

```{python}
training_hyperparameters = dict(
    nb_id=NB_ID,
    suffix=suffix,
    learning_rate_0=learning_rate_0,
    constant_lr_iterations=constant_lr_iterations,
    cosine_annealing_alpha=cosine_annealing_alpha,
    weight_decay=weight_decay,
    n_batches=n_batches,
    batch_size=batch_size,
    save_model_parameters=save_model_parameters.tolist(),
    where_train_strs=where_func_to_labels(where_train),
)
```

```{python}
model_hyperparameters = dict(
    n_replicates=n_replicates,
    hidden_size=hidden_size,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    mass=MASS,
    dt=dt,
    n_steps=n_steps,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
)
```

```{python}
save(
    str(MODELS_DIR / model_basename) + '.eqx',
    trained_models,
    hyperparameters=model_hyperparameters,
)

write_to_json(
    model_hyperparameters | training_hyperparameters,
    str(MODELS_DIR / model_basename.replace(MODEL_FILE_LABEL, HYPERPARAMS_FILE_LABEL)) + '.json',
)
```

To reload our models in later notebooks, we will need a function that regenerates a skeleton PyTree with the same structure as `saved_models`, given the stored `model_hyperparameters`. See the function `setup_models` in `src/rnns_learn_robust_motor_policies/part1_setup.py`.

### Save training histories

In later analyses we may be interested in the history of the losses or model parameters across training, so these should also be saved to disk. 

```{python}
train_histories_basename = model_basename.replace(
    MODEL_FILE_LABEL, 
    TRAIN_HISTORIES_FILE_LABEL,
)

save(
    str(MODELS_DIR / train_histories_basename) + ".eqx",
    train_histories,
    hyperparameters=dict(
        disturbance_stds=disturbance_stds,
        n_batches=n_batches,
        batch_size=batch_size,
        n_replicates=n_replicates,
        where_train_strs=where_func_to_labels(where_train),
        save_model_parameters=save_model_parameters.tolist(),
    ),
)
```

Note that we cannot save lambda functions to disk, so we save the string representation of the `where_train` PyTree.

#### Reloading parameter histories

I'm including this here for reference.

Because we'll generally have already loaded the trained models by the time we also want to load the train histories, our `setup_train_histories` function can use it to determine the correct PyTree structure. Thus we can avoid reconstructing the PyTree of models from scratch -- and storing all the required hyperparameters -- a second time.  

```{python}
from functools import partial
from rnns_learn_robust_motor_policies.setup_utils import setup_train_histories

model_parameter_histories_ = load(
    str(MODELS_DIR / train_histories_basename) + ".eqx",
    partial(setup_train_histories, trained_models),
)
```

