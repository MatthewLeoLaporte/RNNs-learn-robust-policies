---
jupyter: python3
---

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
NB_ID = "1-1"
```

# Training models for Part 1

In this notebook we train several models separately from each other, to support later analyses of whether RNNs can be learn robust policies for a simple continuous control task. In particular, we train an RNN to drive a point mass from an initial position to a target position (i.e. simple reaching) in a 2D workspace. One model will be trained to do this undisturbed, while another will experience a disturbance which cannot be modeled (curl force fields of random direction and magnitude) on each trial/episode. In the latter case, two variants of the model will be trained, at two levels of average disturbance magnitude.

Multiple replicates of the model are trained in each case, to examine the effect of network weight initialization. 

Each model consists of a single-layer RNN with a linear readout layer, which controls a biomechanical model, forming a closed loop. On each time step of a trial, the RNN outputs a 2D force vector which accelerates a Newtonian point mass across a 2D workspace. The input to the RNN on each time step includes feedback observations of the point mass's position and velocity, as well as the target/reference position and velocity. In the case of simple reaching, the target state is constant across the trial -- the goal position and velocity of the reach.

Symmetric (Gaussian) noise is added to the network's outputs and to the feedback observations on each time step, to model motor and sensory noise in the system.

The training objective is to minimize a continuous cost function which penalizes a sum of the following squared terms:

- the Euclidean distance between the point mass's position and the target position on each time step;
- the **difference** between the point mass's velocity and the target velocity on each time step;
- the force vector output by the neural network;
- the activations of the network units.

## Environment setup

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict
from functools import partial
import json
from pathlib import Path
from typing import Literal

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt 
import numpy as np
import optax 
from tqdm.auto import tqdm

from feedbax import (
    AbstractModel, 
    get_ensemble,
    is_module,
    is_type,
    load,
    save,
    tree_map_tqdm,
    tree_stack,
    tree_take_multi,
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import (
    CurlField, 
    FixedField,
    schedule_intervenor,
)
from feedbax.misc import print_version_info, where_func_to_labels
from feedbax.task import SimpleReaches
from feedbax.train import TaskTrainer
from feedbax.xabdeef.losses import simple_reach_loss
from feedbax.xabdeef.models import point_mass_nn

from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
    save_model_and_add_record,
)
from rnns_learn_robust_motor_policies.train_setup_part1 import setup_task_model_pairs
from rnns_learn_robust_motor_policies.tree_utils import pp
from rnns_learn_robust_motor_policies.types import TaskModelPair
from rnns_learn_robust_motor_policies.train_setup import (
    iterations_to_save_model_parameters,
    make_delayed_cosine_schedule,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
print_version_info(jax, eqx, optax)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

These are parameters that can be [varied](https://quarto.org/docs/computations/parameters.html) through the command line interface to Quarto. 

```{python}
#| tags: [parameters]

disturbance_type: Literal['curl', 'random'] = 'random'  
feedback_delay_steps = 0
feedback_noise_std = 0.0
motor_noise_std = 0.0
hidden_size = 50
n_replicates = 5
n_steps = 100
dt = 0.05
cosine_annealing_alpha = 0.01
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Create model-task pairings for different disturbance conditions

```{python}
disturbance_stds = {
    'curl': [0, 0.4, 0.8, 1.6, 2.0],
    'random': [0, 0.01, 0.1, 1.0, 2.0],
}

if not disturbance_type in ['curl', 'random']:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")

task_model_pairs = setup_task_model_pairs(
    n_replicates=n_replicates,
    dt=dt,
    hidden_size=hidden_size,
    n_steps=n_steps,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
    key=key_init,    
)
```


## Training setup

### Training hyperparameters and optimizer

```{python}
learning_rate_0 = 0.01
constant_lr_iterations = 10  # Number of initial training iterations to hold lr constant
alpha = cosine_annealing_alpha  # Max learning rate factor decrease during cosine annealing (see notebook hyperparameters)
weight_decay = 0

n_batches = 10000
batch_size = 250

optimizer_class = partial(
    optax.adamw,
    weight_decay=weight_decay,
)

where_train = lambda model: (
    model.step.net.hidden,
    model.step.net.readout, 
)

save_model_parameters = iterations_to_save_model_parameters(n_batches)
```

```{python}
schedule = make_delayed_cosine_schedule(
    learning_rate_0, 
    constant_lr_iterations, 
    n_batches, 
    alpha,
) 

trainer = TaskTrainer(
    optimizer=optax.inject_hyperparams(optimizer_class)(
        learning_rate=schedule,
    ),
    checkpointing=True,
)
```

## tmp

```{python}
training_hyperparameters = dict(
    learning_rate_0=learning_rate_0,
    constant_lr_iterations=constant_lr_iterations,
    cosine_annealing_alpha=cosine_annealing_alpha,
    weight_decay=weight_decay,
    n_batches=n_batches,
    batch_size=batch_size,
    save_model_parameters=save_model_parameters.tolist(),
    where_train_strs=where_func_to_labels(where_train),
)
```

```{python}
model_hyperparameters = dict(
    n_replicates=n_replicates,
    hidden_size=hidden_size,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    dt=dt,
    n_steps=n_steps,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
)
```

## Train the models

```{python}
def train_pair(pair):
    return trainer(
        pair.task, 
        pair.model,
        ensembled=True,
        n_batches=n_batches, 
        batch_size=batch_size, 
        log_step=n_batches // 10,
        where_train=where_train,
        save_model_parameters=save_model_parameters,
        # disable_tqdm=True,
        key=key_train,
    )

trained_models, train_histories = tree_unzip(tree_map_tqdm(
    train_pair,
    task_model_pairs,
    is_leaf=is_type(TaskModelPair),
))
```

## Save the models with their parameters on the final iteration

```{python}
training_hyperparameters = dict(
    learning_rate_0=learning_rate_0,
    constant_lr_iterations=constant_lr_iterations,
    cosine_annealing_alpha=cosine_annealing_alpha,
    weight_decay=weight_decay,
    n_batches=n_batches,
    batch_size=batch_size,
    save_model_parameters=save_model_parameters.tolist(),
    where_train_strs=where_func_to_labels(where_train),
)
```

```{python}
model_hyperparameters = dict(
    n_replicates=n_replicates,
    hidden_size=hidden_size,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    dt=dt,
    n_steps=n_steps,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
)
```

```{python}
train_histories_hyperparameters=dict(
    disturbance_stds=disturbance_stds,
    n_batches=n_batches,
    batch_size=batch_size,
    n_replicates=n_replicates,
    where_train_strs=where_func_to_labels(where_train),
    save_model_parameters=save_model_parameters.tolist(),
)
```

```{python}
model_record = save_model_and_add_record(
    db_session,
    origin=NB_ID,
    model=trained_models,
    model_hyperparameters=model_hyperparameters,
    other_hyperparameters=training_hyperparameters,
    train_history=train_histories,
    train_history_hyperparameters=train_histories_hyperparameters,
)
```

