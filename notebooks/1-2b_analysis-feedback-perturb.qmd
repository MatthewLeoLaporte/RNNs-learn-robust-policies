# Analysis of feedback perturbations

We continue our perturbation analysis by examining the response of the trained models to perturbations of their feedback inputs at steady state. 

We will apply an impulse perturbation to each of the feedback input channels (velocity and position), and examine and measure the response profiles, in particular the velocities and the control forces.


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict, namedtuple
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import numpy as np
import plotly.colors as plc
import plotly.graph_objects as go

from feedbax import (
    load_with_hyperparameters, 
    is_module, 
    is_type,
    tree_stack,
    tree_take, 
    tree_take_multi,
    tree_unzip,
    tree_prefix_expand,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import ConstantInput, CurlField, schedule_intervenor
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax._tree import tree_key_tuples
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies.misc import dict_str
from rnns_learn_robust_motor_policies.part1_setup import (
    setup_models, 
    setup_model_parameter_histories,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    add_context_annotation,
    add_endpoint_traces,
    get_savefig_func,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_forward_lateral_vel, 
    get_lateral_distance,
    project_onto_direction,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp, 
    swap_model_trainables, 
    subdict,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    set_model_noise,
    find_unique_filepath,
)
```

```{python}
jax.config.update("jax_compilation_cache_dir", "/tmp/jax_cache")
jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
# jax.config.update("jax_explain_cache_misses", True)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"\nFeedbax commit hash: {git_commit_id()}")
```

Unique ID for notebook, for naming outputs.

```{python}
NB_ID = "1-2b"
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'curl'
feedback_noise_std_load = 0.02
motor_noise_std_load = 0.02
feedback_delay_steps_load = 0  

# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
noise_stds = dict(
    feedback=feedback_noise_std,
    motor=motor_noise_std,
)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### Directories setup

```{python}
MODELS_DIR = Path('../models')
    
if not MODELS_DIR.exists():
    raise FileNotFoundError(f"Models directory not found: {MODELS_DIR.absolute()}")
```

### Plotting setup 

The following is a workaround to get LaTeX to display in Plotly figures in VSCode.

```{python}
if os.environ.get('VSCODE_PID') is not None:
    import plotly
    from IPython.display import HTML
    
    plotly.offline.init_notebook_mode()
    display(HTML(
        '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
    ))
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

### Specify file by noise and delay hyperparameters

First, we can specify all the hyperparameters to load the corresponding model, assuming it has been trained.

```{python}
load_from_parameters = True
```

```{python}
suffix_load = '_'.join([
    f"{disturbance_type_load}",
    f"noise-{feedback_noise_std_load}-{motor_noise_std_load}",
    f"delay-{feedback_delay_steps_load}",
])

if load_from_parameters and 'None' in suffix_load:
    raise ValueError("If loading from parameters, all parameters must be specified.")
```

### Specify file by user selection

On the other hand, maybe the user wants to browse and select a trained model file. In that case we can display a file chooser.

```{python}
if not load_from_parameters:
    fc = display_model_filechooser(MODELS_DIR, filter_pattern='1-1_*trained_models.eqx')
```

The default filename is the one that sorts last. If the user does not select a file or if the following cell is run before they do, then the default file will be loaded.

### Load the specified model

```{python}
if not load_from_parameters:
    if fc.selected is None:
        models_filepath = f"{fc.default_path}/{fc.default_filename}"
    else:
        models_filepath = fc.selected
else: 
    models_filepath = str(find_unique_filepath(MODELS_DIR, f"1-1__{suffix_load}__trained_models"))
    if models_filepath is None:
        raise FileNotFoundError(f"No models found with file label: {suffix_load}")

trained_models_load, hyperparameters = load_with_hyperparameters(
    # MODELS_DIR / models_filepath, setup_models,
    MODELS_DIR / models_filepath, setup_models,
)

# We'll use this for creating figure subdirectories according to the training conditions
suffix_train = models_filepath.split('__')[1]
```

### Modify the system noise if needed

```{python}
trained_models_load = jt.map(
    partial(
        set_model_noise, 
        noise_stds=noise_stds,
        enable_noise=True,
    ),
    trained_models_load,
    is_leaf=is_module,
)

disturbance_type_train = hyperparameters['disturbance_type']
disturbance_stds_train = hyperparameters['disturbance_stds']
disturbance_stds_train_lohi = [disturbance_stds_train[0], disturbance_stds_train[-1]]

trained_models = OrderedDict(zip(disturbance_stds_train, trained_models_load))
```

### Load parameters from earlier training iterations, where necessary

Also depending on how training goes, we might want to use model parameters from an earlier training iteration. 

```{python}
model_parameter_histories, train_hyperparameters = load_with_hyperparameters(
    MODELS_DIR / models_filepath.replace('trained_models', 'model_parameter_histories'),
    partial(setup_model_parameter_histories, list(trained_models.values())),
)

where_train = attr_str_tree_to_where_func(train_hyperparameters['where_train_strs'])

model_parameter_histories = dict(zip(disturbance_stds_train, model_parameter_histories))

# load_spec = {0.4: -1, 0.5: -1}

# for curl_std, i in load_spec.items():
#     trained_models[curl_std] = swap_model_trainables(
#         trained_models[curl_std], tree_take(model_parameter_histories[curl_std], i), where_train
#     )
```

### Assign some variables to indicate which parameters we used/will be using

The following will not vary between training and analysis.

```{python}
feedback_delay_steps = hyperparameters['feedback_delay_steps']
n_replicates = hyperparameters['n_replicates']
```

The following may vary in analysis but we may want to refer to the training values.

```{python}
noise_stds_train = dict(
    feedback=hyperparameters['feedback_noise_std'],
    motor=hyperparameters['motor_noise_std'],
)
```

If we didn't alter the disturbance type or noise levels for the analysis, we can infer they'll be the same as the ones used during training.

```{python}
noise_stds = {
    k: v if v is not None else noise_stds_train[k]
    for k, v in noise_stds.items()
} 

any_system_noise = any(jt.leaves(noise_stds))
```

## Setup tasks with impulse perturbations to different feedback channels

### Setup the base task

```{python}
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

# Steady state trials at different grid positions
eval_grid_n = 5
eval_n_directions = 1
eval_reach_length = 0.0  

# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=eval_n_directions,
    eval_reach_length=eval_reach_length,  
)

# "broadcast" this to match the shape of `trained_models`
# tasks_ = dict(zip_longest(trained_models.keys(), [task]))
```

### Schedule impulse perturbations

```{python}
max_impulse_amplitude = dict(
    pos=2.0,
    vel=1.2,
)
impulse_amplitude = dict(
    pos=2.0,
    vel=1.2,
)
n_impulse_amplitudes = 4

impulse_start_step = 30  
impulse_duration = 5  # steps
```

```{python}
# TODO
impulse_amplitudes = jt.map(
    lambda max_amp: jnp.linspace(0, max_amp, n_impulse_amplitudes + 1)[1:],
    max_impulse_amplitude,
)

impulse_end_step = impulse_start_step + impulse_duration
impulse_time_idxs = slice(impulse_start_step, impulse_end_step)
```

```{python}
feedback_var_names = ['pos', 'vel']
coord_names = ['x', 'y']
feedback_var_idxs = dict(zip(feedback_var_names, range(len(feedback_var_names))))
coord_idxs = dict(zip(coord_names, range(len(coord_names))))
```

```{python}
from collections.abc import Callable
from typing import Optional
from jaxtyping import Array, PRNGKeyArray

from feedbax.intervene.schedule import TimeSeriesParam
from feedbax._tree import is_type


def random_unit_vector(key, dim):
    # Could do `jnp.zeros((dim,)).at[impulse_dim].set(1)` for vector toward one dimension
    v = jr.normal(key, (dim,))
    return v / jnp.linalg.norm(v)
    

def impulse_active(
    n_steps: int,
    impulse_duration: int,
    start_bounds: Optional[tuple[int, int]] = None,
    start_idx_func: Callable[[PRNGKeyArray, tuple[int, int]], Array] = (
        lambda key, start_bounds: jr.randint(key, (1,), *start_bounds)[0]
    ),
):  
    """Return a function that determines when a field is active on a given trial."""
    if start_bounds is None:
        start_bounds = (0, n_steps)
    
    def f(trial_spec, key):
        start_idx = start_idx_func(key, start_bounds)
        return TimeSeriesParam(unmask_1d_at_idx(
            n_steps - 1, start_idx, impulse_duration
        ))
    
    return f    


def feedback_impulse(
    amplitude, 
    duration,  # in time steps
    feedback_var,  # 0 (pos) or 1 (vel)
    start_timestep, 
    feedback_dim=None,  # x or y
):
    idxs_impulse = slice(start_timestep, start_timestep + duration)
    trial_mask = jnp.zeros((n_steps - 1,), bool).at[idxs_impulse].set(True)
    
    if feedback_dim is None:
        array = lambda trial_spec, key: random_unit_vector(key, 2)
    else:
        array = jnp.zeros((2,)).at[feedback_dim].set(1)
    
    return ConstantInput.with_params(
        out_where=lambda channel_state: channel_state.output[feedback_var],
        scale=amplitude,
        arrays=array,
        active=TimeSeriesParam(trial_mask),
        # active=impulse_active(
        #     n_steps, 
        #     impulse_duration,
        #     # Always apply the impulse 25% of the way through the trial
        #     start_idx_func=lambda key, start_bounds: (
        #         int(0.66 * (start_bounds[1] - start_bounds[0]))
        #     ),
        # ),
    )
```

#### Perturbations along x/y axes

```{python}
from math import copysign

tasks_imp = dict()
models_imp = dict()
impulse_directions = dict()

impulse_xy_conditions = dict.fromkeys(feedback_var_names, dict.fromkeys(coord_names))
impulse_xy_condition_keys = tree_key_tuples(
    impulse_xy_conditions, keys_to_strs=True, is_leaf=lambda x: x is None,
)

tasks_imp['xy'], models_imp['xy'] = tree_unzip(jt.map(
    lambda ks: schedule_intervenor(
        task, trained_models,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(
            impulse_amplitude[ks[0]],
            impulse_duration,
            feedback_var_idxs[ks[0]],  
            impulse_start_step,
            feedback_dim=coord_idxs[ks[1]],  
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    impulse_xy_condition_keys,
    is_leaf=is_type(tuple),
))
```

```{python}
impulse_directions['xy'] = jt.map(
    lambda task, ks: jnp.zeros(
        (task.n_validation_trials, 2)
    ).at[:, coord_idxs[ks[1]]].set(copysign(1, impulse_amplitude[ks[0]])),
    tasks_imp['xy'], impulse_xy_condition_keys,
    is_leaf=is_module,
)
```

#### Perturbations in random directions

```{python}
tasks_imp['rand'], models_imp['rand'] = tree_unzip(jt.map(
    lambda feedback_var_idx: schedule_intervenor(
        task, trained_models,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(  
            impulse_amplitude[feedback_var_names[feedback_var_idx]],
            impulse_duration,
            feedback_var_idx,   
            impulse_start_step,
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    dict(pos=0, vel=1),
    is_leaf=is_type(tuple),
))
```

Get the perturbation directions, for later:

```{python}
#? I think these values are equivalent to `line_vec` in the functions in `state_utils`
impulse_directions['rand'] = jt.map(
    lambda task: task.validation_trials.intervene['ConstantInput'].arrays[:, impulse_start_step],
    tasks_imp['rand'],
    is_leaf=is_module,
)
```

## Evaluate the trained models on the perturbed task

```{python}
n_trials = 5

if not any_system_noise:
    n_trials = 1
```

```{python}
all_states_imp = jt.map(
    lambda task, models: jt.map(
        lambda models: vmap_eval_ensemble(models, task, n_trials, key_eval),
        models,
        is_leaf=is_module,
    ),
    tasks_imp, models_imp,
    is_leaf=is_module,
)
```

Make sure the subdirectories exist for the figures we'll generate:

```{python}
# for label in models_imp:
#     fig_subdir = FIGS_DIR / f"fb_impulse_{label}",
#     os.makedirs(fig_subdir, exist_ok=True)
```

## Choose a task variant for analysis

```{python}
analysis_variant: Literal['xy', 'rand'] = 'rand'

all_states = all_states_imp[analysis_variant]
directions = impulse_directions[analysis_variant]
tasks = tasks_imp[analysis_variant]
models = models_imp[analysis_variant]
```

## Further plotting setup

### Figure saving

```{python}
suffix = f"imp-steps-{impulse_start_step}-to-{impulse_end_step}_noise-{noise_stds['feedback']}-{noise_stds['motor']}_delay-{feedback_delay_steps}"

FIGS_DIR = Path(f'../figures/{NB_ID}/train__{suffix_train}/{suffix}')

FIGS_DIR.mkdir(parents=True, exist_ok=True)
```

```{python}
savefig = get_savefig_func(FIGS_DIR)
```

### Define colorscales

**TODO** these should probably be defined in a central location along with the ones from 1-2a

```{python}
lighten_factor = 0.7
```

```{python}
def get_colors_dict_from_discrete(keys, colors, lighten_factor: Optional[float] = None, colortype='rgb'):
    if lighten_factor is not None:
        colors = fbp.adjust_color_brightness(colors, lighten_factor)
    colors = plc.convert_colors_to_same_type(colors, colortype=colortype)[0]
    return dict(zip(keys, colors))
    
def get_colors_dict(keys, colorscale, lighten_factor: Optional[float] = None, colortype='rgb'):
    colors = fbp.sample_colorscale_unique(colorscale, len(keys))
    return get_colors_dict_from_discrete(keys, colors, lighten_factor, colortype)

# when coloring by training condition
disturbance_stds_train_colorscale = 'viridis'
disturbance_stds_train_colors = get_colors_dict(
    disturbance_stds_train, disturbance_stds_train_colorscale
)
disturbance_stds_train_colors_dark = get_colors_dict(
    disturbance_stds_train, 
    disturbance_stds_train_colorscale, 
    lighten_factor=lighten_factor,
)
```

```{python}
# when coloring by perturbed feedback variable

# note this colorscale is discrete, unlike the ones used in 1-2a
pert_vars_colorscale = plc.qualitative.D3 
pert_vars_colors = get_colors_dict_from_discrete(feedback_var_names, pert_vars_colorscale)
pert_vars_colors_dark = get_colors_dict_from_discrete(
    feedback_var_names, pert_vars_colorscale, lighten_factor=lighten_factor,
)
```

## Plot some example trial sets

```{python}
fig_subdir = 'example_trial_sets'
```

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

### A single trial set, for a single replicate

```{python}
i_trial = 0
i_replicate = 0

plot_states = tree_take_multi(all_states, [i_trial, i_replicate], [0, 1])

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        mode='markers+lines',
        ms=3,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)    
```

In case we're examining the orthogonal x/y perturbations case, we'll plot them on the same figures.

```{python}
def merge_xy_trial_set_figs(figs):
    fig = figs['x']
    figs['y'].update_traces(showlegend=False)
    fig.add_traces(figs['y'].data)
    return fig
 
if analysis_variant == 'xy':
    figs = {
        label: {
            std: merge_xy_trial_set_figs(figs_xy)
            for std, figs_xy in fs.items()
        }
        for label, fs in {
            label: jt.transpose(
                jt.structure(dict.fromkeys(coord_names, '*')),
                jt.structure(OrderedDict.fromkeys(disturbance_stds_train, '*')),
                fs,
            )
            for label, fs in figs.items()
        }.items()
    }
```

```{python}
for path, fig in jax.tree_util.tree_flatten_with_path(figs, is_leaf=is_type(go.Figure))[0]:
    pert_var = path[0].key
    pert_condition = '-'.join(dict_key.key for dict_key in path[:-1])
    disturbance_std_train = path[-1].key
    
    add_context_annotation(
        fig,
        train_condition_strs=[
            f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0, {disturbance_std_train})"
        ],
        perturbations={
            f"{pert_condition} impulse": (
                impulse_amplitude[pert_var], impulse_start_step, impulse_end_step
            )
        },
        i_trial=i_trial,
        i_replicate=i_replicate,
    )
    # add_endpoint_traces(
    #     fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
    # )
    savefig(
        fig, 
        join([
            f"imp-{analysis_variant}-{pert_var}_amp-{impulse_amplitude[pert_var]}",
            f"disturbance-std-train-{disturbance_std_train}",
            f"rep-{i_replicate}",
            f"eval-{i_trial}",
        ]),
        subdir=fig_subdir,
    )
        
    fig.show()
```

## Compare response trajectories

Toggle whether to plot x/y components or aligned components.

```{python}
components_plot: Literal['xy', 'aligned'] = 'aligned'
components_labels = dict(
    xy=('x', 'y'),
    aligned=(r'\parallel', r'\bot')
)
components_names = dict(
    xy=('x', 'y'),
    aligned=('parallel', 'orthogonal'),
)
```

```{python}
impulse_responses = dict(xy=dict(), aligned=dict())
```

```{python}
# i_replicate = 1
n_preceding_steps = 0

# plot_ts = slice(impulse_start_step - n_preceding_steps, None)
plot_ts = slice(None)
```

### Control forces 

```{python}
fig_subdir = "force_profiles"
```

It is easy to obtain the x/y force components:

```{python}
impulse_responses['xy']['force'] = jt.map(
    lambda states: states.net.output[..., plot_ts, :],
    all_states,
    is_leaf=is_module,
)
```

The force components aligned to the perturbation direction are a little harder:

```{python}
from jaxtyping import Array, Float, PyTree
from feedbax.bodies import SimpleFeedbackState

def project_and_lump(
    all_states: PyTree[SimpleFeedbackState], 
    where_var: Callable[[SimpleFeedbackState], PyTree[Float[Array, "... conditions time xy=2"]]], 
    directions: PyTree[Float[Array, "... conditions xy=2"]],
):
    """For each state PyTree `states` in `all_states`, get `where_var(states)`
    and then project it onto `directions`.
    """
    all_var = jt.map(where_var, all_states, is_leaf=is_module)

    all_var_align = jt.map(
        lambda var, directions: project_onto_direction(var, directions),
        all_var, tree_prefix_expand(directions, all_var),
    )
    
    return {
        key: tree_stack(jt.leaves(var_tree, is_leaf=is_type(OrderedDict)))
        for key, var_tree in all_var_align.items()
    }
```

```{python}
impulse_responses['aligned']['force'] = project_and_lump(
    all_states, 
    lambda states: states.efferent.output,
    directions,
)
```

#### Generate figures for each perturbation variable, comparing all training conditions

```{python}
def get_all_profiles(all_plot, var_label, disturbance_stds=None, colors=None):    
    y_axes_labels = [fr'${var_label}_{sub}$' for sub in components_labels[components_plot]]
    
    if colors is None:
        colors = disturbance_stds_train_colors_dark
    
    if disturbance_stds is not None:
        all_plot = jt.map(
            lambda d: subdict(d, disturbance_stds),
            all_plot,
            is_leaf=is_type(OrderedDict),
        )
        
        colors = subdict(colors, disturbance_stds)
    
    figs = {
        imp_condn: {
            label: fbp.profiles(
                tree_take_multi(var_by_train_condn, [i], [-1]),
                timesteps=jnp.arange(-n_preceding_steps, n_steps),
                mode='std', 
                varname=fr"${y_axes_labels[i]}$",
                colors=list(colors.values()),
                layout_kws=dict(legend_title="Train disturbance std."),
            )
            for i, label in enumerate(components_names[components_plot])
        }
        for imp_condn, var_by_train_condn in all_plot.items()
    }
    
    n = np.prod(jt.leaves(all_plot)[0].shape[:-2])

    for path, fig in jax.tree_util.tree_flatten_with_path(figs, is_leaf=is_type(go.Figure))[0]:
        pert_var = path[0].key
        pert_condition = '-'.join(dict_key.key for dict_key in path[:-1])
        component_name = path[-1].key
        
        fig.add_vrect(
            x0=impulse_start_step, 
            x1=impulse_end_step,
            fillcolor='grey', 
            opacity=0.1, 
            line_width=0,
            name='Perturbation',
        )
        
        add_context_annotation(
            fig,
            perturbations={
                f"{pert_condition} impulse ({analysis_variant} direction)": (
                    impulse_amplitude[pert_var], impulse_start_step, impulse_end_step
                )
            },
            train_condition_strs=[f"{disturbance_type_train} force fields"],
            n=n,
        )
    
    return figs
```

```{python}
var_label = 'F'

figs = get_all_profiles(impulse_responses[components_plot]['force'], var_label)
```

```{python}
for path, fig in jax.tree_util.tree_flatten_with_path(figs, is_leaf=is_type(go.Figure))[0]:
    pert_var = path[0].key
    pert_condition = '-'.join(dict_key.key for dict_key in path[:-1])
    component_name = path[-1].key  
    
    savefig(
        fig, 
        join([
            f"{var_label}-{component_name}",
            f"pert-amp-{impulse_amplitude[pert_var]}",
        ]),
        subdir=f"{var_label}_response/pert-{pert_condition}",
    )
    fig.show()
```

#### Generate figures comparing the perturbation variables, and the low vs. high training conditions

```{python}
fig_subdir = "force_response/compare-pert-vars"
```

There are only two figures here, for the forward/parallel and lateral/orthogonal directions respectively.

```{python}
figs = get_all_profiles(
    impulse_responses[components_plot]['force'], 
    var_label, 
    disturbance_stds_train_lohi,
)
```

```{python}
def toggle_bounds_visibility(fig):
    def toggle_visibility_if_bound(trace):
        if 'bound' in trace.name:
            if trace.visible is None:
                trace.visible = False
            else:
                trace.visible = not trace.visible
    
    fig.for_each_trace(toggle_visibility_if_bound)


def get_merged_context_annotation(*figs):
    
    annotations_text = [
        next(iter(fig.select_annotations(selector=dict(name="context_annotation")))).text 
        for fig in figs
    ]
    annotation_unique_lines = set(sum([text.split('<br>') for text in annotations_text], []))
    merged_annotation = '<br>'.join(reversed(sorted(annotation_unique_lines)))
    return merged_annotation 
    

# now merge the two figures in each inner dict
def merge_profile_figs(figs):
    figs['pos'].update_traces(
        line_dash='dot',
        showlegend=False,
    )
    figs['vel'].add_traces(figs['pos'].data)
    
    figs['vel'].update_annotations(
        selector=dict(name="context_annotation"),
        text=get_merged_context_annotation(*figs.values()),
    )
    
    toggle_bounds_visibility(figs['vel'])
    
    return figs['vel']
    

figs_t = jt.transpose(jt.structure(dict.fromkeys(feedback_var_names, '*')), None, figs)

figs_merged = {
    label: merge_profile_figs(fs)
    for label, fs in figs_t.items()
}
```

```{python}
for path, fig in jax.tree_util.tree_flatten_with_path(figs_merged, is_leaf=is_type(go.Figure))[0]:
    pert_var = path[0].key
    pert_condition = '-'.join(dict_key.key for dict_key in path[:-1])
    component_name = path[-1].key  
      
    savefig(
        fig, 
        join([
            f"{var_label}-{component_name}",
            f"pert-amp-{dict_str(impulse_amplitude)}",
        ]),
        subdir=fig_subdir,
    )
    fig.show()
```

**TODO** plot detail of during-perturbation region, *with* error bars (i.e. use `toggle_bounds_visibility` again), to show how the difference in response from low to high training disturbance is much larger for a velocity perturbation than a position perturbation

### Velocity profiles 

```{python}
fig_subdir = "vel_profiles"
```

The x/y components:

```{python}
impulse_responses['xy']['vel'] = jt.map(
    lambda states: states.mechanics.effector.vel,
    all_states,
    is_leaf=is_module,
)
```

And the velocity components parallel and orthogonal to the perturbation directions:

```{python}
impulse_responses['aligned']['vel'] = project_and_lump(
    all_states, 
    lambda states: states.mechanics.effector.vel,
    directions,
)
```

#### Generate figures for each perturbation variable, comparing all train disturbance stds

```{python}
var_label = 'v'

figs = get_all_profiles(impulse_responses[components_plot]['vel'], var_label)
```

```{python}
for fig in jt.leaves(figs):
    fig.show()
```

#### Generate figures comparing the perturbation variables, and the low vs. high training conditions

```{python}
fig_subdir = "vel_response/compare-pert-vars"
```

There are only two figures here, for the forward/parallel and lateral/orthogonal directions respectively.

```{python}
figs = get_all_profiles(
    impulse_responses[components_plot]['vel'], 
    var_label, 
    disturbance_stds_train_lohi,
)

figs_t = jt.transpose(jt.structure(dict.fromkeys(feedback_var_names, '*')), None, figs)

figs_merged = {
    label: merge_profile_figs(fs)
    for label, fs in figs_t.items()
}
```

```{python}
for path, fig in jax.tree_util.tree_flatten_with_path(figs_merged, is_leaf=is_type(go.Figure))[0]:
    pert_var = path[0].key
    pert_condition = '-'.join(dict_key.key for dict_key in path[:-1])
    component_name = path[-1].key  
      
    savefig(
        fig, 
        join([
            f"{var_label}-{component_name}",
            f"pert-amp-{dict_str(impulse_amplitude)}",
        ]),
        subdir=fig_subdir,
    )
    fig.show()
```

## Summary comparison of performance measures

```{python}
fig_subdir = "performance_measures"
```

We'll base these measures on the impulse response states which have already been aligned with the impulse directions.

```{python}
# We'll copy the top level so we can add stuff without affecting what happened earlier
all_responses = dict(impulse_responses['aligned'])  
```

Thus note that in what follows, "forward", "backward", "lateral" are all relative to the perturbation direction.

Also note that the outer keys (including `'vel'` etc.) of `all_responses` refer to response variables, whereas the second-level keys (also including `'vel'` etc.) refer to the perturbed feedback variables.

### Add some other state variables to the impulse response tree

We didn't manipulate the position, but we will want to compute deviations with respect to the steady state (i.e. target) position.

```{python}
all_states = all_states_imp[analysis_variant]
```

```{python}
target_pos = jt.map(
    lambda task: task.validation_trials.targets['mechanics.effector.pos'].value,
    tasks,
    is_leaf=is_module,
)

all_responses['pos'] = jt.map(
    lambda states, target_pos: (
        states.mechanics.effector.pos
        - target_pos[:, 0:1, :]  # only the first timestep, though the goal should be constant
    ),
    all_states, tree_prefix_expand(target_pos, all_states, is_leaf=is_module),
    is_leaf=is_module,
)
```

**TODO** We may also be interested in the sensitivity to noise.

```{python}
# all_responses['feedback-noise'] = ...
```

```{python}
# all_responses['motor-noise'] = ...
```

### Transpose tree of measures 

We'll want to map over each combination of perturbation condition (feedback variable) and training condition (disturbance std), in each case passing the responses to the functions which calculate the measures. However, the response variables are currently in the outermost level of the `all_responses` tree, and the conditions we want to map over are inside. Thus we'll transpose the response variables to the inside of the array.

```{python}
Responses = namedtuple('Responses', all_responses.keys())

all_responses_tuple = Responses(**all_responses) 
```

```{python}
all_responses_tuples = jt.transpose(
    jt.structure(all_responses_tuple, is_leaf=is_type(dict)), 
    None, 
    all_responses_tuple,
)
```

```{python}
pp(all_responses_tuples)
```

:::{note}
There's certainly some alternative way to have approached this. In particular, we could have defined all the transformations we needed to plot the profiles, and apply them all at once to get the `Responses` leaves (or equivalent) of the pytree. 

However, the current approach works just fine as well.
:::

### Define all performance measures

Max parallel (forward/backward, depending on mul=1 or mul=-1) force

```{python}
def get_max_parallel_force(responses, mul=1., timesteps=slice(None)):
    return jnp.max(mul * responses.force[..., timesteps, 0], axis=-1)
```

Max net control force 

```{python}
def get_net_force(responses, timesteps=slice(None)):
    return jnp.linalg.norm(responses.force[..., timesteps, :], axis=-1)
    
def get_max_net_force(responses, **kwargs):
    return jnp.max(get_net_force(responses, **kwargs), axis=-1)
```

Sum control forces

```{python}
def get_sum_net_forces(responses):
    return jnp.sum(get_net_force(responses), axis=-1)
```

Max parallel (forward/backward) velocity 

```{python}
def get_max_parallel_vel(responses, mul=1.):
    return jnp.max(mul * responses.vel[..., 0], axis=-1)
```

Max lateral (left/right) velocity

```{python}
def get_max_lateral_vel(responses, mul=1.):
    return jnp.max(mul * responses.vel[..., 1], axis=-1)
```

Max deviation from steady state position (projection doesn't matter)

```{python}
def get_deviation(responses):
    return jnp.linalg.norm(responses.pos, axis=-1)

def get_max_deviation(responses):
    return jnp.max(get_deviation(responses), axis=-1)
```

Sum deviations from steady state position

```{python}
def get_sum_deviations(responses):
    return jnp.sum(get_deviation(responses), axis=-1)
```

**TODO** covariance between force output and system noise variables.

### Calculate all performance measures 

```{python}
all_measure_funcs, measure_handles = tree_unzip(OrderedDict({
    "Max net force during pert.": (
        partial(get_max_net_force, timesteps=impulse_time_idxs),
        "force-net-max-during-pert",
    ),
    "Max backward force": (
        partial(get_max_parallel_force, mul=-1.), 
        "force-backward-max",
    ),
    "Max net force after pert.": (
        partial(
            get_max_net_force, 
            timesteps=slice(impulse_end_step, None),
        ),
        "force-net-max-after-pert",
    ),
    f"Max forward force within {impulse_duration} steps of pert. end": (
        partial(
            get_max_parallel_force, 
            mul=1.,
            timesteps=slice(impulse_end_step, impulse_end_step + impulse_duration),
        ),
        "force-net-max-after-pert",
    ),
    "Max forward force": (
        partial(get_max_parallel_force, mul=1.), 
        "force-forward-max",
    ),
    "Max net force": (get_max_net_force, "force-net-max"),
    "Sum net forces": (get_sum_net_forces, "force-net-sum"),
    "Max forward velocity": (
        partial(get_max_parallel_vel, mul=1.), 
        "vel-forward-max",
    ),
    "Max backward velocity": (
        partial(get_max_parallel_vel, mul=-1.), 
        "vel-backward-max",
    ),
    "Max lateral velocity (left)": (
        partial(get_max_lateral_vel, mul=1.), 
        "vel-lateral-left-max",
    ),
    "Max lateral velocity (right)": (
        partial(get_max_lateral_vel, mul=-1.), 
        "vel-lateral-right-max",
    ),
    "Max deviation": (get_max_deviation, "deviation-max"),
    "Sum deviations": (get_sum_deviations, "deviation-sum")
}))
```

```{python}
def get_all_measures(measure_funcs: PyTree[Callable], all_responses: PyTree[Responses]):
    return jt.map(
        lambda func: jt.map(
            lambda responses: func(responses),
            all_responses,
            is_leaf=is_type(Responses),
        ),
        measure_funcs,
    )
    
all_measures = get_all_measures(all_measure_funcs, all_responses_tuples)

all_measures_lohi = {
    measure_name: {
        pert_var: subdict(train_condn_measures, disturbance_stds_train_lohi)
        for pert_var, train_condn_measures in all_measures[measure_name].items()
    }
    for measure_name in all_measures
}
```

### Comparison of all training conditions

```{python}
fig_subdir = "performance_measures/all_train_conditions"
```

```{python}
def get_violins(measure_data, measure_name, annotation_kws=None, layout_kws=None):
    n_dist = np.prod(jt.leaves(measure_data)[0].shape)
    
    if layout_kws is None:
        layout_kws = dict()
        
    if annotation_kws is None:
        annotation_kws = dict()    
        
    fig = go.Figure(
        data=jt.leaves([
            [
                go.Violin(
                    x=np.full((n_dist,), train_curl_std),
                    y=data.flatten(),
                    name=pert_var,
                    box_visible=False,
                    meanline_visible=True,
                    line_color=pert_vars_colors_lohi[i],
                    showlegend=(j == 0),
                    spanmode='hard',
                )
                for j, (train_curl_std, data) in enumerate(measure_data[pert_var].items())
            ]
            for i, pert_var in enumerate(measure_data)
        ]),
        layout=dict(
            xaxis_title="Train disturbance std.",
            yaxis_title=measure_name,
            # xaxis_range=[-0.5, len(measure_data) - 0.5],
            # xaxis_tickvals=list(measure_data.keys()),
            yaxis_range=[0, None],
            legend_title="Perturbed feedback var.",
            # violinmode='overlay',
            violingap=0,
            **layout_kws,
        )
    )

    add_context_annotation(
        fig, 
        perturbations={
            f"{pert_var} impulse ({analysis_variant} direction)": (
                impulse_amplitude[pert_var], impulse_start_step, impulse_end_step
            )
            for pert_var in impulse_amplitude
        },
        train_condition_strs=[f"{disturbance_type_train} force fields"],
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig
```

```{python}
figs = {
    measure_name: get_violins(
        measure, 
        measure_name,
        layout_kws=dict(
            width=800,
            height=600,
        )
    )
    for measure_name, measure in all_measures.items()
}
```

```{python}
for measure_name, measure_handle in measure_handles.items():
    savefig(
        figs[measure_name], 
        join([
            f"{measure_handle}",
            f"pert-amp-{dict_str(impulse_amplitude)}",
        ]),
        subdir=fig_subdir,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name].show()
```

### Comparison of low-high training conditions

```{python}
fig_subdir = "performance_measures/lohi_train_conditions"
```

```{python}
figs = {
    measure_name: get_violins(
        measure, 
        measure_name,
        layout_kws=dict(
            width=500,
            height=400,
        )
    )
    for measure_name, measure in all_measures_lohi.items()
}
```

```{python}
for measure_name, measure_handle in measure_handles.items():
    savefig(
        figs[measure_name], 
        join([
            f"{measure_handle}",
            f"pert-amp-{impulse_amplitude}",
        ]),
        subdir=fig_subdir,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name].show()
```