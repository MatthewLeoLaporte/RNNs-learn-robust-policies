---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "1-2b"

TRAIN_NB_ID = "1-1"
```

# Analysis of feedback perturbations

We continue our perturbation analysis by examining the response of the trained models to perturbations of their feedback inputs at steady state. 

We will apply an impulse perturbation to each of the feedback input channels (velocity and position), and examine and measure the response profiles, in particular the velocities and the control forces.

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict, namedtuple
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
from jaxtyping import Array, Float, PyTree
import matplotlib.pyplot as plt
import numpy as np
import plotly
import plotly.colors as plc
import plotly.graph_objects as go
from tqdm.auto import tqdm

from feedbax import (
    is_module, 
    is_type,
    load, 
    move_level_to_outside,
    tree_key_tuples, 
    tree_prefix_expand,
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unstack, 
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import ConstantInput, CurlField, schedule_intervenor
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES,
    get_colors_dicts,
    get_colors_dicts_from_discrete,
)
from rnns_learn_robust_motor_policies.constants import WORKSPACE
from rnns_learn_robust_motor_policies.database import (
    add_evaluation,
    add_evaluation_figure,
    get_db_session,
    get_model_record,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies.misc import (
    dict_str, 
    lohi, 
    lomidhi,
    load_from_json,
    print_version_info,
)
from rnns_learn_robust_motor_policies.train_setup_part1 import (
    setup_model_parameter_histories,
    setup_task_model_pairs,
)
from rnns_learn_robust_motor_policies.plot import (
    add_endpoint_traces,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    toggle_bounds_visibility,
    get_merged_context_annotation,
    figs_flatten_with_paths,
    figleaves,
    plotly_vscode_latex_fix,
)
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.state_utils import (
    get_forward_lateral_vel, 
    get_lateral_distance,
    project_onto_direction,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp, 
    swap_model_trainables, 
    subdict,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    set_model_noise,
    find_unique_filepath,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.types import (
    ImpulseAmpTuple, 
    PertVarDict, 
    TrainStdDict,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
print_version_info(jax, eqx, plotly)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'curl'
feedback_noise_std_load = 0.0
motor_noise_std_load = 0.0
feedback_delay_steps_load = 0
hidden_size = 50

# Specify model parameters to use for analysis (None -> use training value)
# disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

### Plotting setup 

The following is a workaround to get LaTeX to display in Plotly figures in VSCode.

```{python}
plotly_vscode_latex_fix()
```

## Load and adjust trained models

```{python}
model_info = get_model_record(
    db_session,
    disturbance_type=disturbance_type_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps_load,
    hidden_size=hidden_size,
)
```

```{python}
models_load: TrainStdDict[float, eqx.Module] = load(
    model_info.path, partial(setup_models_only, setup_task_model_pairs),
)

replicate_info = load(
    model_info.replicate_info_path, partial(setup_replicate_info, models_load),
)
```

### Modify the system noise if needed

```{python}
models_base = jt.map(
    partial(
        set_model_noise, 
        noise_stds=dict(
            feedback=feedback_noise_std,
            motor=motor_noise_std,
        ),
        enable_noise=True,
    ),
    models_load,
    is_leaf=is_module,
)
```

### Optionally select a subset of training conditions

Depending on how training goes, we might want to leave out some of the training conditions (i.e. training disturbance stds) from the analysis.

```{python}
# exclude the highest disturbance std (2.0), since the policy started to be unstable
disturbance_train_stds = model_info.disturbance_stds[:-1]

models_base = subdict(models_base, disturbance_train_stds)
```


### Optionally exclude replicates that perform much worse than average

```{python}
exclude_underperformers = True
```

```{python}
measure_to_exclude_by = 'best_total_loss'

included_replicates = replicate_info['included_replicates'][measure_to_exclude_by]
best_replicate = replicate_info['best_replicates'][measure_to_exclude_by]

def take_replicate_or_best(tree: TrainStdDict, i_replicate=None, replicate_axis=1):
    if i_replicate is None:
        return TrainStdDict({
            train_std: tree_take(subtree, best_replicate[train_std], replicate_axis)
            for train_std, subtree in tree.items()
        })
    else:
        return tree_take_multi(tree, [i_replicate], [replicate_axis])
```

Which replicates are included?

```{python}
print("\nReplicates included in analysis for each training condition:")
eqx.tree_pprint(included_replicates, short_arrays=False)
```

Set the indices corresponding to the excluded replicates to NaN in each of the model arrays. This ensures that the shapes of the arrays remain consistent. NaN results will be ignored at plotting time. 

```{python}
models_all_replicates = models_base

if exclude_underperformers:
    models_base = jt.map(
        lambda models, included: tree_set_scalar(models, jnp.nan, jnp.where(~included)[0]),
        models_base, included_replicates,
        is_leaf=is_module,
    )
    n_replicates_included = jt.map(lambda x: jnp.sum(x).item(), included_replicates)
else:
    models_base = models_base
    n_replicates_included = dict.fromkeys(models.keys(), model_info.n_replicates)
```

## Sort out the evaluation parameters

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
eval_parameters = use_record_params_where_none(dict(
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
), model_info)
```

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Task parameters

We'll do feedback perturbations on a grid of steady state (i.e. "stabilization") trials.

```{python}
eval_grid_n = 5
EVAL_N_DIRECTIONS = 1
EVAL_REACH_LENGTH = 0.0  
```

### Perturbation parameters

```{python}
max_impulse_amplitude = dict(
    pos=1.8,
    vel=1.2,
)
# impulse_amplitude = dict(
#     pos=2.0,
#     vel=1.2,
# )
n_impulse_amplitudes = 3

impulse_start_step = 30  
impulse_duration = 5  # steps
```

### Number of evaluations per condition (i.e. grid position) and model

```{python}
n_evals = 5

if not any_system_noise:
    n_evals = 1
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    n_evals=n_evals,
    eval_grid_n=eval_grid_n,
    impulse_start_step=impulse_start_step,
    impulse_duration=impulse_duration,
    max_impulse_amplitude=max_impulse_amplitude,
    n_impulse_amplitudes=n_impulse_amplitudes,
)
```

## Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    model_hash=model_info.hash,
    eval_parameters=eval_parameters,
    origin=NB_ID,
)
```

## Setup tasks with impulse perturbations to different feedback channels

### Setup the base task

```{python}
# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=WORKSPACE, 
    n_steps=model_info.n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=EVAL_N_DIRECTIONS,
    eval_reach_length=EVAL_REACH_LENGTH,  
)
```

### Schedule impulse perturbations

```{python}
# TODO
impulse_amplitudes = jt.map(
    lambda max_amp: jnp.linspace(0, max_amp, n_impulse_amplitudes + 1)[1:],
    max_impulse_amplitude,
)

impulse_end_step = impulse_start_step + impulse_duration
impulse_time_idxs = slice(impulse_start_step, impulse_end_step)
```

```{python}
analysis_variants = ('xy', 'rand')

pert_var_names = ('pos', 'vel')
feedback_var_idxs = dict(zip(pert_var_names, range(len(pert_var_names))))

coord_names = ('x', 'y')
coord_idxs = dict(zip(coord_names, range(len(coord_names))))
```

```{python}
from collections.abc import Callable
from typing import Optional
from jaxtyping import Array, PRNGKeyArray

from feedbax.intervene.schedule import TimeSeriesParam
from feedbax._tree import is_type


def random_unit_vector(key, dim):
    # Could do `jnp.zeros((dim,)).at[impulse_dim].set(1)` for vector toward one dimension
    v = jr.normal(key, (dim,))
    return v / jnp.linalg.norm(v)
    

def impulse_active(
    n_steps: int,
    impulse_duration: int,
    start_bounds: Optional[tuple[int, int]] = None,
    start_idx_func: Callable[[PRNGKeyArray, tuple[int, int]], Array] = (
        lambda key, start_bounds: jr.randint(key, (1,), *start_bounds)[0]
    ),
):  
    """Return a function that determines when a field is active on a given trial."""
    if start_bounds is None:
        start_bounds = (0, n_steps)
    
    def f(trial_spec, key):
        start_idx = start_idx_func(key, start_bounds)
        return TimeSeriesParam(unmask_1d_at_idx(
            n_steps - 1, start_idx, impulse_duration
        ))
    
    return f    


def feedback_impulse(
    amplitude, 
    duration,  # in time steps
    feedback_var,  # 0 (pos) or 1 (vel)
    start_timestep, 
    feedback_dim=None,  # x or y
):
    idxs_impulse = slice(start_timestep, start_timestep + duration)
    trial_mask = jnp.zeros((model_info.n_steps - 1,), bool).at[idxs_impulse].set(True)
    
    if feedback_dim is None:
        array = lambda trial_spec, key: random_unit_vector(key, 2)
    else:
        array = jnp.zeros((2,)).at[feedback_dim].set(1)
    
    return ConstantInput.with_params(
        out_where=lambda channel_state: channel_state.output[feedback_var],
        scale=amplitude,
        arrays=array,
        active=TimeSeriesParam(trial_mask),
        # active=impulse_active(
        #     model_info.n_steps, 
        #     impulse_duration,
        #     # Always apply the impulse 25% of the way through the trial
        #     start_idx_func=lambda key, start_bounds: (
        #         int(0.66 * (start_bounds[1] - start_bounds[0]))
        #     ),
        # ),
    )
```

#### Perturbations along x/y axes

```{python}
# from math import copysign

all_tasks_imp = dict()
all_models_imp = dict()
impulse_directions = dict()

impulse_xy_conditions = PertVarDict.fromkeys(pert_var_names, dict.fromkeys(coord_names))
impulse_xy_conditions_keys = tree_key_tuples(
    impulse_xy_conditions, keys_to_strs=True, is_leaf=lambda x: x is None,
)

all_tasks_imp['xy'], all_models_imp['xy'] = tree_unzip(jt.map(
    lambda ks: schedule_intervenor(
        task, models_base,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(
            1.0, # impulse_amplitude[ks[0]],
            impulse_duration,
            feedback_var_idxs[ks[0]],  
            impulse_start_step,
            feedback_dim=coord_idxs[ks[1]],  
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    impulse_xy_conditions_keys,
    is_leaf=is_type(tuple),
))
```

```{python}
impulse_directions['xy'] = jt.map(
    lambda task, ks: jnp.zeros(
        (task.n_validation_trials, 2)
    # ).at[:, coord_idxs[ks[1]]].set(copysign(1, impulse_amplitude[ks[0]])),
    # Assume x-y impulses are in the positive direction.
    ).at[:, coord_idxs[ks[1]]].set(1),
    all_tasks_imp['xy'], impulse_xy_conditions_keys,
    is_leaf=is_module,
)
```

#### Perturbations in random directions

```{python}
all_tasks_imp['rand'], all_models_imp['rand'] = tree_unzip(jt.map(
    lambda feedback_var_idx: schedule_intervenor(
        task, models_base,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(  
            1.0, #impulse_amplitude[pert_var_names[feedback_var_idx]],
            impulse_duration,
            feedback_var_idx,   
            impulse_start_step,
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    dict(pos=0, vel=1),
    is_leaf=is_type(tuple),
))
```

Get the perturbation directions, for later:

```{python}
#? I think these values are equivalent to `line_vec` in the functions in `state_utils`
impulse_directions['rand'] = jt.map(
    lambda task: task.validation_trials.intervene['ConstantInput'].arrays[:, impulse_start_step],
    all_tasks_imp['rand'],
    is_leaf=is_module,
)
```

## Evaluate the trained models on the perturbed tasks

Evaluate multiple times on each trial (i.e. task condition), when there is system noise to cause variation.

```{python}
def eval_with_imp_amplitude(impulse_amplitude, models, task, n_evals, key_eval):
    task = eqx.tree_at(
        lambda task: task.intervention_specs.validation['ConstantInput'].intervenor.params.scale,
        task,
        impulse_amplitude,
    ) 
    return vmap_eval_ensemble(models, task, n_evals, key_eval)


def evaluate_all_impulse_responses():
    # Wrap as a function for the convenience of estimating the amount of memory needed for the result.
    return {
        analysis_variant: {
            pert_var: jt.map(  # Necessary to map over possibly an extra tree level in different analysis variants
                lambda task, models: jt.map(  # Maps over the models in the OrderedDict (train conditions)
                    lambda models: eqx.filter_vmap(
                        eval_with_imp_amplitude,
                        in_axes=(0, None, None, None, None)
                    )(
                        impulse_amplitudes[pert_var], 
                        models, 
                        task, 
                        n_evals, 
                        key_eval,
                    ),
                    models,
                    is_leaf=is_module,
                ),
                all_tasks_imp[analysis_variant][pert_var],
                all_models_imp[analysis_variant][pert_var],
                is_leaf=is_module,
            )
            for pert_var in pert_var_names
        }
        for analysis_variant in analysis_variants  # TODO: maybe call these `task_variants` or something
    }
```

```{python}
all_states_bytes = tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_impulse_responses))

print(f"\nEstimate {all_states_bytes / 1e9:.2f} GB of memory needed for all responses.")
```

```{python}
all_states_imp = evaluate_all_impulse_responses()
```

## Choose a task variant for analysis

```{python}
analysis_variant: Literal['xy', 'rand'] = 'rand'

# Unstack the first array dimension (impulse amplitude) into another PyTree level
all_states = {
    pert_var: ImpulseAmpTuple(tree_unstack(states))
    for pert_var, states in all_states_imp[analysis_variant].items()
}

directions = impulse_directions[analysis_variant]
tasks = all_tasks_imp[analysis_variant]
models = all_models_imp[analysis_variant]
```

### Define colorscales

```{python}
# when coloring by training condition
disturbance_train_stds_colors, disturbance_train_stds_colors_dark = get_colors_dicts(
    disturbance_train_stds, COLORSCALES['disturbance_train_stds'],
)
```

```{python}
# when coloring by perturbed feedback variable

pert_vars_colors, pert_vars_colors_dark = get_colors_dicts_from_discrete(
    pert_var_names, COLORSCALES['fb_pert_vars']
)
```

## Plot some example trial sets

```{python}
plot_id = 'example_trial_sets'
```

```{python}
if not any_system_noise:
    ExamplePlotVars = namedtuple("ExamplePlotVars", ['pos', 'vel', 'force'])
    var_labels = ExamplePlotVars('Position', 'Velocity', 'Control force')
    where_plot = lambda states: ExamplePlotVars(
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
        states.efferent.output,
    )
else:
    ExamplePlotVars = namedtuple("ExamplePlotVars", ['pos', 'vel'])
    var_labels = ExamplePlotVars('Position', 'Velocity')
    # Forces are very messy when there's noise,
    # and we'll visualize the aligned forces anyway
    where_plot = lambda states: ExamplePlotVars(
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
    )
```

### A single trial set, for a single replicate

```{python}
i_trial = 0
i_replicate = None

# Index the trial and replicate
# plot_states = tree_take_multi(all_states, [i_trial, i_replicate], [0, 1])

# Select the variables to plot
plot_states = jt.map(where_plot, all_states, is_leaf=is_module)
```

```{python}
if i_replicate is None:
    get_replicate = lambda train_std: best_replicate[train_std]
else:
    get_replicate = lambda _: i_replicate

figs = jt.map(
    lambda states: {
        train_std: fbp.trajectories_2D(
            tree_take_multi(
                plot_vars, 
                [i_trial, get_replicate(train_std)],
                [0, 1]
            ),
            var_labels=var_labels,
            axes_labels=('x', 'y'),
            curves_mode='markers+lines',
            ms=3,
            scatter_kws=dict(line_width=0.75),
            layout_kws=dict(
                width=100 + len(var_labels) * 300,
                height=400,
                legend_tracegroupgap=1,
            ),
        )
        for train_std, plot_vars in states.items()
    },
    plot_states,
    is_leaf=is_type(TrainStdDict),
)    
```

In case we're examining the orthogonal x/y perturbations case, we'll plot them on the same figures.

```{python}
# def merge_xy_trial_set_figs(figs):
#     fig = figs['x']
#     figs['y'].update_traces(showlegend=False)
#     fig.add_traces(figs['y'].data)
#     return fig

# TODO: Alter something (e.g. color) so it is easier to tell the directions apart
# TODO: Make sure this is still working, if xy perturbations ever become relevant again
# if analysis_variant == 'xy':
#     figs = {
#         label: jt.map(
#             lambda train_std_figs: OrderedDict({
#                 std: merge_xy_trial_set_figs(figs_xy)
#                 for std, figs_xy in train_std_figs.items()
#             }),
#             figs_t,
#             is_leaf=is_type(OrderedDict),
#         )
#         for label, figs_t in {
#             label: jt.transpose(
#                 jt.structure(dict.fromkeys(coord_names, '*')),
#                 jt.structure(OrderedDict.fromkeys(disturbance_train_stds, '*')),
#                 fs,
#             )
#             for label, pert_var_figs in figs.items()
#         }.items()
#     }
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    pert_var = path[0].key
    pert_condition = '-'.join(str(dict_key.key) for dict_key in path[:-2])  # -2: omit impulse amp & train std
    pert_amplitude = impulse_amplitudes[pert_var][path[-2].key]
    disturbance_train_std = path[-1].key
    
    i_rep = best_replicate[disturbance_train_std]

    fig_parameters = dict(
        disturbance_train_std=disturbance_train_std,
        # analysis_variant=analysis_variant,
        disturbance_type=f"impulse/feedback/{pert_var}",
        disturbance_amplitude=pert_amplitude,
        i_model_replicate=i_rep,
        i_random_trial=i_trial,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
    
    if disturbance_train_std in lohi(disturbance_train_stds) and pert_var == 'vel':
        fig.show()
```

## Compare response trajectories

Toggle whether to plot x/y components or aligned components.

```{python}
components_plot: Literal['xy', 'aligned'] = 'aligned'
components_labels = dict(
    xy=('x', 'y'),
    aligned=(r'\parallel', r'\bot')
)
components_names = dict(
    xy=('x', 'y'),
    aligned=('parallel', 'orthogonal'),
)
```

```{python}
# i_replicate = 1
n_preceding_steps = 0

# plot_ts = slice(impulse_start_step - n_preceding_steps, None)
plot_ts = slice(None)
```

### Obtain the profiles

```{python}
# Short label for the axes/filenames
profile_vars_labels = dict(
    force='F',
    vel='v',
)

# What part of the state to obtain the profiles from
profile_vars_where = dict(
    force=lambda states: states.efferent.output,
    vel=lambda states: states.mechanics.effector.vel,
)

# Different ways to align the profiles with the reach directions
alignment_funcs = dict(
    xy=lambda var, _: var, 
    aligned=lambda var, directions: project_onto_direction(var, directions),
)
```

```{python}
impulse_responses = jt.map(
    lambda alignment_func: jt.map(
        lambda where: jt.map(
            lambda states, directions: alignment_func(where(states)[..., plot_ts, :], directions),
            all_states, tree_prefix_expand(directions, all_states, is_leaf=is_module),
            is_leaf=is_module,
        ),
        profile_vars_where,
    ),
    alignment_funcs,
)
```

```{python}
# pp(impulse_responses)
```

### Define some plotting functions

**TODO**: Make the figures shorter (too much vertical whitespace) 

```{python}
def get_all_profiles(all_plot, var_label, disturbance_stds=None, colors=None):    
    y_axes_labels = [fr'${var_label}_{sub}$' for sub in components_labels[components_plot]]
    
    if colors is None:
        colors = disturbance_train_stds_colors_dark
    
    if disturbance_stds is not None:
        all_plot = jt.map(
            lambda d: subdict(d, disturbance_stds),
            all_plot,
            is_leaf=is_type(TrainStdDict),
        )
        
        colors = subdict(colors, disturbance_stds)
    
    figs = {
        coord_label: {
            pert_condn: jt.map(
                lambda var_by_train_condn: fbp.profiles(
                    tree_take(var_by_train_condn, i, -1),
                    timesteps=jnp.arange(-n_preceding_steps, model_info.n_steps),
                    mode='std', 
                    varname=fr"${y_axes_labels[i]}$",
                    colors=list(colors.values()),
                    layout_kws=dict(
                        legend_title="Train<br>field std.",
                        width=600,
                        height=400,
                        legend_tracegroupgap=1,
                    ),
                ),
                var_by_pert_amp,
                is_leaf=is_type(TrainStdDict),
            )
            for pert_condn, var_by_pert_amp in all_plot.items()
        }
        for i, coord_label in enumerate(components_names[components_plot])
    }

    for path, fig in figs_flatten_with_paths(figs):
        component_name = path[0].key
        pert_var = path[1].key
        pert_condition = '-'.join(str(dict_key.key) for dict_key in path[1:-1])
        pert_amp = path[-1].key
        
        fig.add_vrect(
            x0=impulse_start_step, 
            x1=impulse_end_step,
            fillcolor='grey', 
            opacity=0.1, 
            line_width=0,
            name='Perturbation',
        )
    
    return figs
```

```{python}
def merge_profile_figs(figs):
    figs['pos'].update_traces(
        line_dash='dot',
        showlegend=False,
    )
    figs['vel'].add_traces(figs['pos'].data)
    
    figs['vel'].update_annotations(
        selector=dict(name="context_annotation"),
        # text=get_merged_context_annotation(*figs.values()),
    )
    
    toggle_bounds_visibility(figs['vel'])
    
    return figs['vel']
```

### Generate figures for each perturbation variable, comparing all training conditions

```{python}
plot_id = "response_profiles/compare_train_conditions"
```

```{python}
figs = {
    var_name: get_all_profiles(impulse_responses[components_plot][var_name], var_label)
    for var_name, var_label in profile_vars_labels.items()
}

for path, fig in tqdm(figs_flatten_with_paths(figs)):
    var_name = path[0].key
    var_label = profile_vars_labels[var_name]
    component_name = path[1].key
    pert_var = path[2].key
    pert_condition = '-'.join(str(dict_key.key) for dict_key in path[2:-1])
    pert_amplitude = impulse_amplitudes[pert_var][path[-1].key]
    
    n = int(np.prod(jt.leaves(impulse_responses[components_plot][var_name])[0].shape[:-2]))
    
    fig_parameters = dict(
        disturbance_train_std=disturbance_train_std,
        # analysis_variant=analysis_variant,
        variable_name=var_label,
        component_name=component_name,
        disturbance_type=f"impulse/feedback/{pert_var}",
        disturbance_amplitude=pert_amplitude,
        n=n,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
    
    if pert_amplitude in lohi(impulse_amplitudes[pert_var]):
        fig.show()
```


### Generate figures comparing the perturbation variables, and the low vs. high training conditions

**TODO**: add legend for dotted lines -> pos, solid lines -> vel

There are only two figures here, for the forward/parallel and lateral/orthogonal directions respectively.

For each figure, we also generate a detail of the peri-perturbation region of the trials.

```{python}
plot_id = "response_profiles/compare_pert_vars"
```

```{python}
def get_summary_profiles(var, var_label):
    figs = get_all_profiles(var, var_label, lohi(disturbance_train_stds))
    
    # Collapses the perturbation amplitude level across perturbation variables (into a tuple).
    # This implicitly pairs up the perturbation amplitudes for the two variables, in order from lowest to highest.
    # We just obtained the individual figures using `get_all_profiles` so there should be no issue 
    # with their contents.
    # However, in the next cell we retrieve the actual amplitudes, for labeling the filenames.
    figs_t = {
        coord_name: jt.transpose(
            # replace the pert_var level with an ordered dict so we can map over it in the next step
            jt.structure(PertVarDict.fromkeys(pert_var_names, '*')), 
            None, 
            PertVarDict(figs_by_pert_var),
        )
        for coord_name, figs_by_pert_var in figs.items()
    }
    
    figs_merged = jt.map(
        lambda fs: merge_profile_figs(fs),
        figs_t,
        is_leaf=is_type(PertVarDict),
    )
    
    return figs_merged
```

```{python}
t_range_detail = [impulse_start_step - 1, impulse_end_step + 5]

figs = {
    var_name: get_summary_profiles(impulse_responses[components_plot][var_name], var_label)
    for var_name, var_label in profile_vars_labels.items()
}

for path, fig in tqdm(figs_flatten_with_paths(figs)):
    var_name = path[0].key
    var_label = profile_vars_labels[var_name]
    component_name = path[1].key
    
    pert_amp_idx = path[-1].key
    impulse_amplitude = {k: v[pert_amp_idx] for k, v in impulse_amplitudes.items()}

    fig_parameters = dict(
        # analysis_variant=analysis_variant,
        variable_name=var_label,
        component_name=component_name,
        disturbance_amplitude=impulse_amplitude['pos'],
        disturbance_amplitude1=impulse_amplitude['vel'],
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
    
    if impulse_amplitude['pos'] in lohi(impulse_amplitudes['pos']):
        fig.show()
    
    # Generate detail of the perturbation region
    fig_detail = go.Figure(fig)

    all_y_in_t_range = np.concatenate([trace.y[slice(*t_range_detail)] for trace in fig_detail.data])
    y_range = [np.min(all_y_in_t_range), np.max(all_y_in_t_range)]     
    
    fig_detail.update_layout(
        xaxis_range=t_range_detail,
        yaxis_range=y_range,
    )
    
    toggle_bounds_visibility(fig_detail)
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id + "/detail",
        **fig_parameters,
    )
    
    if impulse_amplitude['pos'] in lohi(impulse_amplitudes['pos']):
        fig_detail.show()
```

## Summary comparison of performance measures

```{python}
plot_id = "performance_measures"
```

We'll base these measures on the impulse response states which have already been aligned with the impulse directions.

```{python}
# We'll copy the top level so we can add stuff without affecting what happened earlier
all_responses = dict(impulse_responses['aligned'])  
```

Thus note that in what follows, "forward", "backward", "lateral" are all relative to the perturbation direction.

Also note that the outer keys (including `'vel'` etc.) of `all_responses` refer to response variables, whereas the second-level keys (also including `'vel'` etc.) refer to the perturbed feedback variables.

### Add some other state variables to the impulse response tree

We didn't manipulate the position, but we will want to compute deviations with respect to the steady state (i.e. target) position.

**TODO**: Calculate for each perturbation amplitude

```{python}
target_pos = jt.map(
    lambda task: task.validation_trials.targets['mechanics.effector.pos'].value,
    tasks,
    is_leaf=is_module,
)

all_responses['pos'] = jt.map(
    lambda states, target_pos: (
        states.mechanics.effector.pos
        - target_pos[:, 0:1, :]  # only the first timestep, though the goal should be constant
    ),
    all_states, tree_prefix_expand(target_pos, all_states, is_leaf=is_module),
    is_leaf=is_module,
)
```

**TODO** We may also be interested in the sensitivity to noise.

```{python}
# all_responses['feedback-noise'] = ...
```

```{python}
# all_responses['motor-noise'] = ...
```

### Rerrange the tree of responses

We'll want to map over each combination of perturbation condition (feedback variable) and training condition (disturbance std), in each case passing the responses to the functions which calculate the measures. However, the response variables are currently in the outermost level of the `all_responses` tree, and the conditions we want to map over are inside. Thus we'll transpose the response variables to the inside of the array. 

Also, move the `ImpulseAmpTuple` level to the outside.

```{python}
Responses = namedtuple('Responses', all_responses.keys())

all_responses_tuple = Responses(**all_responses) 
```

```{python}
all_responses_tuples = move_level_to_outside(
    jt.transpose(
        jt.structure(all_responses_tuple, is_leaf=is_type(dict)), 
        None, 
        all_responses_tuple,
    ), 
    ImpulseAmpTuple,
)
```

```{python}
# pp(all_responses_tuples)
```

:::{note}
There's certainly some alternative way to have approached this. In particular, we could have defined all the transformations we needed to plot the profiles, and apply them all at once to get the `Responses` leaves (or equivalent) of the pytree. 

However, the current approach works just fine as well.
:::

### Define all performance measures

Max parallel (forward/backward, depending on mul=1 or mul=-1) force

```{python}
def get_max_parallel_force(responses, mul=1., timesteps=slice(None)):
    return jnp.max(mul * responses.force[..., timesteps, 0], axis=-1)
```

Max net control force 

```{python}
def get_net_force(responses, timesteps=slice(None)):
    return jnp.linalg.norm(responses.force[..., timesteps, :], axis=-1)
    
def get_max_net_force(responses, **kwargs):
    return jnp.max(get_net_force(responses, **kwargs), axis=-1)
```

Sum control forces

```{python}
def get_sum_net_forces(responses):
    return jnp.sum(get_net_force(responses), axis=-1)
```

Max parallel (forward/backward) velocity 

```{python}
def get_max_parallel_vel(responses, mul=1.):
    return jnp.max(mul * responses.vel[..., 0], axis=-1)
```

Max lateral (left/right) velocity

```{python}
def get_max_lateral_vel(responses, mul=1.):
    return jnp.max(mul * responses.vel[..., 1], axis=-1)
```

Max deviation from steady state position (projection doesn't matter)

```{python}
def get_deviation(responses):
    return jnp.linalg.norm(responses.pos, axis=-1)

def get_max_deviation(responses):
    return jnp.max(get_deviation(responses), axis=-1)
```

Sum deviations from steady state position

```{python}
def get_sum_deviations(responses):
    return jnp.sum(get_deviation(responses), axis=-1)
```

**TODO** covariance between force output and system noise variables.

```{python}
all_measure_funcs, measure_handles = tree_unzip(OrderedDict({
    "Max net force during pert.": (
        partial(get_max_net_force, timesteps=impulse_time_idxs),
        "force-net-max-during-pert",
    ),
    "Max backward force": (
        partial(get_max_parallel_force, mul=-1.), 
        "force-backward-max",
    ),
    "Max net force after pert.": (
        partial(
            get_max_net_force, 
            timesteps=slice(impulse_end_step, None),
        ),
        "force-net-max-after-pert",
    ),
    f"Max forward force within {impulse_duration} steps of pert. end": (
        partial(
            get_max_parallel_force, 
            mul=1.,
            timesteps=slice(impulse_end_step, impulse_end_step + impulse_duration),
        ),
        "force-net-max-after-pert",
    ),
    "Max forward force": (
        partial(get_max_parallel_force, mul=1.), 
        "force-forward-max",
    ),
    "Max net force": (get_max_net_force, "force-net-max"),
    "Sum net forces": (get_sum_net_forces, "force-net-sum"),
    "Max forward velocity": (
        partial(get_max_parallel_vel, mul=1.), 
        "vel-forward-max",
    ),
    "Max backward velocity": (
        partial(get_max_parallel_vel, mul=-1.), 
        "vel-backward-max",
    ),
    "Max lateral velocity (left)": (
        partial(get_max_lateral_vel, mul=1.), 
        "vel-lateral-left-max",
    ),
    "Max lateral velocity (right)": (
        partial(get_max_lateral_vel, mul=-1.), 
        "vel-lateral-right-max",
    ),
    "Max deviation": (get_max_deviation, "deviation-max"),
    "Sum deviations": (get_sum_deviations, "deviation-sum")
}))
```

### Calculate all performance measures 

```{python}
def get_all_measures(measure_funcs: PyTree[Callable], all_responses: PyTree[Responses]):
    return jt.map(
        lambda func: jt.map(
            lambda responses: func(responses),
            all_responses,
            is_leaf=is_type(Responses),
        ),
        measure_funcs,
    )
    
all_measures = get_all_measures(all_measure_funcs, all_responses_tuples)

all_measures_lohi = {
    measure_name: jt.map(
        lambda train_condn_measures: subdict(train_condn_measures, lohi(disturbance_train_stds)),
        measure,
        is_leaf=is_type(TrainStdDict),
    )
    for measure_name, measure in all_measures.items()
}
```

### Comparison of all training conditions

```{python}
plot_id = "performance_measures/all_train_conditions"
```

```{python}
def get_violins(measure_data, measure_name, annotation_kws=None, layout_kws=None):
    n_dist = np.prod(jt.leaves(measure_data)[0].shape)
    n_violins = len(jt.leaves(measure_data, is_leaf=is_type(TrainStdDict))[0])
    
    if layout_kws is None:
        layout_kws = dict()
        
    if annotation_kws is None:
        annotation_kws = dict()    
        
    fig = go.Figure(
        data=jt.leaves([
            [
                go.Violin(
                    x=np.full((n_dist,), disturbance_train_std),
                    y=data.flatten(),
                    name=pert_var,
                    legendgroup=pert_var,
                    box_visible=False,
                    meanline_visible=True,
                    line_color=pert_vars_colors[pert_var],
                    showlegend=(j == 0),
                    spanmode='hard',
                )
                for j, (disturbance_train_std, data) in enumerate(measure_data[pert_var].items())
            ]
            for i, pert_var in enumerate(measure_data)
        ]),
        layout=dict(
            xaxis_title="Train field std.",
            xaxis_type='category',
            yaxis_title=measure_name,
            xaxis_range=[-0.5, n_violins - 0.5],
            xaxis_tickvals=np.arange(n_violins),
            # yaxis_range=[0, None],
            legend_title="Perturbed<br>feedback var.",
            legend_tracegroupgap=1,
            # violinmode='overlay',
            violingap=0,
            **layout_kws,
        )
    )
    
    return fig
```

We'll find the maximum measure value for each measure type, and apply the same y-axis maximum for each plot of the same measure type, across the different impulse magnitudes. 

```{python}
measure_ranges = {
    measure_name: (
            jnp.nanmin(measure_data_stacked),
            jnp.nanmax(measure_data_stacked),   
    )
    for measure_name, measure_data_stacked in {
        measure_name: jnp.stack(jt.leaves(measure_data))
        for measure_name, measure_data in all_measures.items()
    }.items()
}

figs = {
    measure_name: [
        get_violins(
            measure, 
            measure_name,
            layout_kws=dict(
                width=800,
                height=600,
                yaxis_range=[0, measure_ranges[measure_name][1]],
            ),
            # Leave this annotation in, for now, since I'm not sure how I want to report 
            # two different perturbation amplitudes in the figure table (i.e. `fig_parameters` below)
            annotation_kws=dict(
                perturbations={
                    f"{pert_var} impulse ({analysis_variant} direction)": (
                        impulse_amplitudes[pert_var][impulse_amplitude_idx], 
                        impulse_start_step, 
                        impulse_end_step,
                    )
                    for pert_var in impulse_amplitude
                }
            )
        )
        for impulse_amplitude_idx, measure in enumerate(measure_by_pert_amp)
    ]
    for measure_name, measure_by_pert_amp in all_measures.items()
}
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    measure_name = path[0].key
    measure_handle = measure_handles[measure_name]
    impulse_amplitude_idx = path[1].idx
    impulse_amplitude = {k: v[impulse_amplitude_idx] for k, v in impulse_amplitudes.items()}
    
    fig_parameters = dict(
        # analysis_variant=analysis_variant,
        measure_name=measure_handle,
        disturbance_amplitude=impulse_amplitude['pos'],
        disturbance_amplitude1=impulse_amplitude['vel'],
        # TODO
        # n=?,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name][-1].show()
```

### Comparison of low-high training conditions

```{python}
plot_id = "performance_measures/lohi_train_conditions"
```

```{python}
measure_ranges_lohi = {
    measure_name: (
            jnp.nanmin(measure_data_stacked),
            jnp.nanmax(measure_data_stacked),   
    )
    for measure_name, measure_data_stacked in {
        measure_name: jnp.stack(jt.leaves(measure_data))
        for measure_name, measure_data in all_measures_lohi.items()
    }.items()
}

figs = {
    measure_name: [
        get_violins(
            measure, 
            measure_name,
            layout_kws=dict(
                width=500,
                height=400,
                yaxis_range=[0, measure_ranges_lohi[measure_name][1]],
            ),
            annotation_kws=dict(
                perturbations={
                    f"{pert_var} impulse ({analysis_variant} direction)": (
                        impulse_amplitudes[pert_var][impulse_amplitude_idx], 
                        impulse_start_step, 
                        impulse_end_step,
                    )
                    for pert_var in impulse_amplitude
                }
            )
        )
        for impulse_amplitude_idx, measure in enumerate(measure_by_pert_amp)
    ]
    for measure_name, measure_by_pert_amp in all_measures_lohi.items()
}
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    measure_name = path[0].key
    measure_handle = measure_handles[measure_name]
    impulse_amplitude_idx = path[1].idx
    impulse_amplitude = {k: v[impulse_amplitude_idx] for k, v in impulse_amplitudes.items()}
    
    fig_parameters = dict(
        # analysis_variant=analysis_variant,
        measure_name=measure_handle,
        disturbance_amplitude=impulse_amplitude['pos'],
        disturbance_amplitude1=impulse_amplitude['vel'],
        # TODO
        # n=?,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name][0].show()
    figs[measure_name][-1].show()
```

### Comparison across impulse amplitudes

First, generate separate figures for pos and vel perturbations. The x axis will be the impulse amplitude, and each violin will be split to compare the lowest and highest training conditions.

```{python}
plot_id = "performance_measures/compare_impulse_amplitudes"
```

```{python}
def get_violins_comparing_amplitudes(measure_data, measure_name, pert_var, annotation_kws=None, layout_kws=None):
    example_arr = jt.leaves(measure_data)[0]
    n_dist = np.prod(example_arr.shape)
    
    if layout_kws is None:
        layout_kws = dict()
        
    if annotation_kws is None:
        annotation_kws = dict()    
        
    plot_disturbance_train_stds = [disturbance_train_stds[0], disturbance_train_stds[-2], disturbance_train_stds[-1]]
    
    fig = go.Figure()
    
    # Get trial, replicate, and task condition indices for hoverinfo
    customdata = jnp.tile(
        jnp.stack(jnp.unravel_index(
            jnp.arange(n_dist), 
            example_arr.shape,
        ), axis=-1), 
        (len(impulse_amplitudes[pert_var]), 1),
    )
    
    # Disturbance train std. (split)
    for j, disturbance_train_std in enumerate(plot_disturbance_train_stds):
        measure_data_j = [
            d[pert_var][disturbance_train_std].flatten() for d in measure_data
        ]
        
        xs = [
            np.full_like(arr, impulse_amplitudes[pert_var][i]) 
            for i, arr in enumerate(measure_data_j)
        ]
        
        fig.add_trace(
            go.Violin(
                x=jnp.concatenate(xs),
                y=jnp.concatenate(measure_data_j),
                name=disturbance_train_std,
                scalegroup=disturbance_train_std,
                legendgroup=disturbance_train_std,
                box_visible=False,
                meanline_visible=True,
                # side='positive' if j == 1 else 'negative',
                line_color=disturbance_train_stds_colors_dark[disturbance_train_std],
                # showlegend=True,
                # spanmode='hard',
                scalemode='width',
                customdata=customdata,
                hovertemplate='<br>'.join([
                    "%{y:.2f}",
                    "Task condition: %{customdata[2]}",
                    "Replicate: %{customdata[1]}",
                    "Evaluation: %{customdata[0]}",
                    "<extra></extra>",
                ]),
            )
        )
            
    fig.update_layout(
        width=600,
        height=400,
        xaxis_title="Impulse amplitude",
        yaxis_title=measure_name,
        # xaxis_range=[-0.5, len(measure_data) - 0.5],
        # xaxis_tickvals=list(measure_data.keys()),
        # yaxis_range=[0, None],
        legend_title="Train<br>field std.",
        legend_tracegroupgap=1,
        violinmode='group',
        violingap=0.25,
        violingroupgap=0,
        **layout_kws,
    )
    
    return fig
```

```{python}
figs = {
    measure_name: {
        pert_var: get_violins_comparing_amplitudes(
            measure, 
            measure_name,
            pert_var,
            layout_kws=dict(
                yaxis_range=[0, measure_ranges[measure_name][1]],
            ),
        )
        for pert_var in pert_var_names
    }
    for measure_name, measure in all_measures.items()
}
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    measure_name = path[0].key
    measure_handle = measure_handles[measure_name]
    pert_var = path[1].key
    n_dist = int(np.prod(jt.leaves(all_measures[measure_name])[0].shape))
    
    fig_parameters = dict(
        # analysis_variant=analysis_variant,
        measure_name=measure_handle,
        disturbance_type=f"impulse/feedback/{pert_var}",
        n=n_dist,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name]['pos'].show()
    figs[measure_name]['vel'].show()
```
