# Analysis of feedback perturbations

We continue our perturbation analysis by examining the response of the trained models to perturbations of their feedback inputs at steady state. 

In particular, we will apply an impulse perturbation to each of the feedback input channels, and observe 

1) the magnitude of the instantaneous network response (i.e. control force); 
2) the trajectory of the point mass following the perturbation.


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objects as go

from feedbax import (
    load_with_hyperparameters, 
    is_module, 
    is_type,
    tree_stack,
    tree_take, 
    tree_take_multi,
    tree_unzip,
    tree_prefix_expand,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import ConstantInput, CurlField, schedule_intervenor
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies.part1_setup import (
    setup_models, 
    setup_model_parameter_histories,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    add_context_annotation,
    add_endpoint_traces,
    get_savefig_func,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_forward_lateral_vel, 
    get_lateral_distance,
    project_onto_direction,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp, 
    swap_model_trainables, 
    subdict,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    set_model_noise,
    find_unique_filepath,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"\nFeedbax commit hash: {git_commit_id()}")
```

Unique ID for notebook, for naming outputs.

```{python}
NB_ID = "1-2b"
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'curl'
feedback_noise_std_load = 0.02
motor_noise_std_load = 0.02
feedback_delay_steps_load = 0  

# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
noise_stds = dict(
    feedback=feedback_noise_std,
    motor=motor_noise_std,
)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### Directories setup

```{python}
FIGS_DIR = Path(f'../figures/{NB_ID}/')
MODELS_DIR = Path('../models')

for d in (FIGS_DIR,):
    d.mkdir(parents=True, exist_ok=True)
    
if not MODELS_DIR.exists():
    raise FileNotFoundError(f"Models directory not found: {MODELS_DIR.absolute()}")
```

### Plotting setup 

```{python}
trials_cmap = 'viridis'  # for trials
trials_cmap_func = plt.get_cmap(trials_cmap)

savefig = get_savefig_func(FIGS_DIR)
```

The following is a workaround to get LaTeX to display in Plotly figures in VSCode.

```{python}
if os.environ.get('VSCODE_PID') is not None:
    import plotly
    from IPython.display import HTML
    
    plotly.offline.init_notebook_mode()
    display(HTML(
        '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
    ))
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

### Specify file by noise and delay hyperparameters

First, we can specify all the hyperparameters to load the corresponding model, assuming it has been trained.

```{python}
load_from_parameters = True
```

```{python}
suffix_load = '_'.join([
    f"{disturbance_type_load}",
    f"noise-{feedback_noise_std_load}-{motor_noise_std_load}",
    f"delay-{feedback_delay_steps_load}",
])

if load_from_parameters and 'None' in suffix_load:
    raise ValueError("If loading from parameters, all parameters must be specified.")
```

### Specify file by user selection

On the other hand, maybe the user wants to browse and select a trained model file. In that case we can display a file chooser.

```{python}
if not load_from_parameters:
    fc = display_model_filechooser(MODELS_DIR, filter_pattern='1-1_*trained_models.eqx')
```

The default filename is the one that sorts last. If the user does not select a file or if the following cell is run before they do, then the default file will be loaded.

### Load the specified model

```{python}
if not load_from_parameters:
    if fc.selected is None:
        models_filepath = f"{fc.default_path}/{fc.default_filename}"
    else:
        models_filepath = fc.selected
else: 
    models_filepath = str(find_unique_filepath(MODELS_DIR, f"1-1__{suffix_load}__trained_models"))
    if models_filepath is None:
        raise FileNotFoundError(f"No models found with file label: {suffix_load}")

trained_models, hyperparameters = load_with_hyperparameters(
    # MODELS_DIR / models_filepath, setup_models,
    MODELS_DIR / models_filepath, setup_models,
)

# We'll use this for creating figure subdirectories according to the training conditions
suffix_train = models_filepath.split('__')[1]
```

### Modify the system noise if needed

```{python}
trained_models = jt.map(
    partial(
        set_model_noise, 
        noise_stds=noise_stds,
        enable_noise=True,
    ),
    trained_models,
    is_leaf=is_module,
)

disturbance_type_train = hyperparameters['disturbance_type']
disturbance_stds_train = hyperparameters['disturbance_stds']

trained_models = OrderedDict(zip(disturbance_stds_train, trained_models))
```

### Load parameters from earlier training iterations, where necessary

Also depending on how training goes, we might want to use model parameters from an earlier training iteration. 

```{python}
model_parameter_histories, train_hyperparameters = load_with_hyperparameters(
    MODELS_DIR / models_filepath.replace('trained_models', 'model_parameter_histories'),
    partial(setup_model_parameter_histories, list(trained_models.values())),
)

where_train = attr_str_tree_to_where_func(train_hyperparameters['where_train_strs'])

model_parameter_histories = dict(zip(disturbance_stds_train, model_parameter_histories))

# load_spec = {0.4: -1, 0.5: -1}

# for curl_std, i in load_spec.items():
#     trained_models[curl_std] = swap_model_trainables(
#         trained_models[curl_std], tree_take(model_parameter_histories[curl_std], i), where_train
#     )
```

### Assign some variables to indicate which parameters we used/will be using

The following will not vary between training and analysis.

```{python}
feedback_delay_steps = hyperparameters['feedback_delay_steps']
n_replicates = hyperparameters['n_replicates']
```

The following may vary in analysis but we may want to refer to the training values.

```{python}
noise_stds_train = dict(
    feedback=hyperparameters['feedback_noise_std'],
    motor=hyperparameters['motor_noise_std'],
)
```

If we didn't alter the disturbance type or noise levels for the analysis, we can infer they'll be the same as the ones used during training.

```{python}
noise_stds = {
    k: v if v is not None else noise_stds_train[k]
    for k, v in noise_stds.items()
} 

any_system_noise = any(jt.leaves(noise_stds))
```

## Setup tasks with impulse perturbations to different feedback channels

### Setup the base task

```{python}
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

# Steady state trials at different grid positions
eval_grid_n = 5
eval_n_directions = 1
eval_reach_length = 0.0  

# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=eval_n_directions,
    eval_reach_length=eval_reach_length,  
)

# "broadcast" this to match the shape of `trained_models`
# tasks_ = dict(zip_longest(trained_models.keys(), [task]))
```

### Schedule impulse perturbations

```{python}
impulse_amplitude = 1.0
impulse_start_step = 30
impulse_duration = 5
impulse_end_step = impulse_start_step + impulse_duration
impulse_time_idxs = slice(impulse_start_step, impulse_start_step + 1)
```

```{python}
from collections.abc import Callable
from typing import Optional
from jaxtyping import Array, PRNGKeyArray

from feedbax.intervene.schedule import TimeSeriesParam
from feedbax._tree import is_type

def random_unit_vector(key, dim):
    # Could do `jnp.zeros((dim,)).at[impulse_dim].set(1)` for vector toward one dimension
    v = jr.normal(key, (dim,))
    return v / jnp.linalg.norm(v)
    

def impulse_active(
    n_steps: int,
    impulse_duration: int,
    start_bounds: Optional[tuple[int, int]] = None,
    start_idx_func: Callable[[PRNGKeyArray, tuple[int, int]], Array] = (
        lambda key, start_bounds: jr.randint(key, (1,), *start_bounds)[0]
    ),
):  
    """Return a function that determines when a field is active on a given trial."""
    if start_bounds is None:
        start_bounds = (0, n_steps)
    
    def f(trial_spec, key):
        start_idx = start_idx_func(key, start_bounds)
        return TimeSeriesParam(unmask_1d_at_idx(
            n_steps - 1, start_idx, impulse_duration
        ))
    
    return f    


def feedback_impulse(
    amplitude, 
    duration,  # in time steps
    feedback_var,  # pos or vel
    start_timestep, 
    feedback_dim=None,  # x or y
):
    idxs_impulse = slice(start_timestep, start_timestep + duration)
    trial_mask = jnp.zeros((n_steps - 1,), bool).at[idxs_impulse].set(True)
    
    if feedback_dim is None:
        array = lambda trial_spec, key: random_unit_vector(key, 2)
    else:
        array = jnp.zeros((2,)).at[feedback_dim].set(1)
    
    return ConstantInput.with_params(
        out_where=lambda channel_state: channel_state.output[feedback_var],
        scale=amplitude,
        arrays=array,
        active=TimeSeriesParam(trial_mask),
        # active=impulse_active(
        #     n_steps, 
        #     impulse_duration,
        #     # Always apply the impulse 25% of the way through the trial
        #     start_idx_func=lambda key, start_bounds: (
        #         int(0.66 * (start_bounds[1] - start_bounds[0]))
        #     ),
        # ),
    )
```

#### Perturbations along axes

```{python}
from math import copysign

tasks_imp = dict()
models_imp = dict()
impulse_directions = dict()

impulse_conditions = {
    'pos': dict(
        x=(0, 0),
        y=(0, 1),
    ),
    'vel': dict(
        x=(1, 0),
        y=(1, 1),
    )
}

tasks_imp['xy'], models_imp['xy'] = tree_unzip(jt.map(
    lambda feedback_vardim: schedule_intervenor(
        task, trained_models,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(
            impulse_amplitude,
            impulse_duration,
            feedback_vardim[0],  
            impulse_start_step,
            feedback_dim=feedback_vardim[1],  
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    impulse_conditions,
    is_leaf=is_type(tuple),
))

#? I think these values are equivalent to `line_vec` in the functions in `state_utils`
impulse_directions['xy'] = jt.map(
    lambda task, idxs: jnp.zeros(
        (task.n_validation_trials, 2)
    ).at[:, idxs[1]].set(copysign(1, impulse_amplitude)),
    tasks_imp['xy'], impulse_conditions, 
    is_leaf=is_module,
)
```

#### Perturbations in random directions

```{python}
tasks_imp['rand'], models_imp['rand'] = tree_unzip(jt.map(
    lambda feedback_var: schedule_intervenor(
        task, trained_models,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(  
            impulse_amplitude,
            impulse_duration,
            feedback_var,   
            impulse_start_step,
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    dict(pos=0, vel=1),
    is_leaf=is_type(tuple),
))
```

Get the perturbation directions, for later:

```{python}
#? I think these values are equivalent to `line_vec` in the functions in `state_utils`
impulse_directions['rand'] = jt.map(
    lambda task: jnp.squeeze(
        task.validation_trials.intervene['ConstantInput'].arrays[:, impulse_time_idxs]
    ),
    tasks_imp['rand'],
    is_leaf=is_module,
)
```

## Evaluate the trained models on the perturbed task

```{python}
n_trials = 5

if not any_system_noise:
    n_trials = 1
```

```{python}
all_states_imp = jt.map(
    lambda task, models: jt.map(
        lambda models: vmap_eval_ensemble(models, task, n_trials, key_eval),
        models,
        is_leaf=is_module,
    ),
    tasks_imp, models_imp,
    is_leaf=is_module,
)
```

Make sure the subdirectories exist for the figures we'll generate:

```{python}
# for label in models_imp:
#     fig_subdir = FIGS_DIR / f"fb_impulse_{label}",
#     os.makedirs(fig_subdir, exist_ok=True)
```

## Choose a task variant for analysis

```{python}
from feedbax import tree_labels

analysis_variant: Literal['xy', 'rand'] = 'rand'

all_states = all_states_imp[analysis_variant]
directions = impulse_directions[analysis_variant]
tasks = tasks_imp[analysis_variant]
models = models_imp[analysis_variant]
```

## Plot some example trial sets

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

### A single trial set, for a single replicate

```{python}
i_trial = 0
i_replicate = 0

plot_states = tree_take_multi(all_states, [i_trial, i_replicate], [0, 1])

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        mode='markers+lines',
        ms=3,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
# TODO
# TODO: fix this for new nested dict structure of `figs`
# TODO

# for pert_condition in figs:
#     # TODO: now there's an extra layer here, but only for variant xy!
#     for disturbance_std_train, fig in figs[pert_condition].items():
#         add_context_annotation(
#             fig,
#             train_condition_strs=[
#                 f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{disturbance_std_train})"
#             ],
#             perturbations={
#                 f"{pert_condition} ({analysis_variant} direction) impulse": (
#                     impulse_amplitude, impulse_start_step, impulse_end_step
#                 )
#             },
#             i_trial=i_trial,
#             i_replicate=i_replicate,
#         )
#         # add_endpoint_traces(
#         #     fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
#         # )
#         savefig(
#             fig, 
#             join([
#                 f"disturbance-std-train-{disturbance_std_train}",
#                 f"imp-amp-{impulse_amplitude}",
#                 f"imp-steps-{impulse_duration}",
#                 f"rep-{i_replicate}",
#                 f"eval-{i_trial}",
#             ]),
#             subdir=f"fb_impulse_{pert_condition}/example_sets",
#         )
        
#     fig.show()
```

## Compare response trajectories

Toggle whether to plot x/y components or aligned components.

```{python}
components_plot: Literal['xy', 'aligned'] = 'aligned'
components_labels = dict(
    xy=('x', 'y'),
    aligned=(r'\parallel', r'\bot')
)
```

```{python}
impulse_responses = dict(xy=dict(), aligned=dict())
```

```{python}
# i_replicate = 1
n_preceding_steps = 0

# plot_ts = slice(impulse_start_step - n_preceding_steps, None)
plot_ts = slice(None)
```

### Control forces 

```{python}
fig_subdir = "control_profiles"
```

It is easy to obtain the x/y force components:

```{python}
impulse_responses['xy']['force'] = jt.map(
    lambda states: states.net.output[..., plot_ts, :],
    all_states,
    is_leaf=is_module,
)
```

The force components aligned to the perturbation direction are a little harder:

```{python}
from jaxtyping import Array, Float, PyTree
from feedbax.bodies import SimpleFeedbackState

def project_and_lump(
    all_states: PyTree[SimpleFeedbackState], 
    where_var: Callable[[SimpleFeedbackState], PyTree[Float[Array, "... conditions time xy=2"]]], 
    directions: PyTree[Float[Array, "... conditions xy=2"]],
):
    """For each state PyTree `states` in `all_states`, get `where_var(states)`
    and then project it onto `directions`.
    """
    all_var = jt.map(where_var, all_states, is_leaf=is_module)

    all_var_align = jt.map(
        lambda var, directions: project_onto_direction(var, directions),
        all_var, tree_prefix_expand(directions, all_var),
    )
    
    return {
        key: tree_stack(jt.leaves(var_tree, is_leaf=is_type(OrderedDict)))
        for key, var_tree in all_var_align.items()
    }
```

```{python}
impulse_responses['aligned']['force'] = project_and_lump(
    all_states, 
    lambda states: states.efferent.output,
    directions,
)
```

#### Generate figures

```{python}
all_controls_plot = impulse_responses[components_plot]['force']

axes_labels = [fr'$F_{sub}$' for sub in components_labels[components_plot]]

figs = {
    imp_condn: {
        label: fbp.profiles(
            tree_take_multi(
                controls, 
                [i], 
                [-1],
            ),
            timesteps=jnp.arange(-n_preceding_steps, n_steps),
            mode='std', 
            n_std_plot=1,
            varname=fr"${label}$",
            layout_kws=dict(
                legend=dict(
                    title="Train curl std.",
                    # xref="container",
                    # yref="container",
                    # y=0.60, 
                ),
                # legend2=dict( 
                #     title="",
                #     xref="container",
                #     yref="container",
                #     y=0.85, 
                # ),
            ),
            scatter_kws=dict(legend="legend"),
        )
        for i, label in enumerate(axes_labels)
    }
    for imp_condn, controls in all_controls_plot.items()
}
```

```{python}
n = np.prod(jt.leaves(all_controls_plot)[0].shape[:-2])

for pert_condition in figs:
    for label, fig in figs[pert_condition].items():
        add_context_annotation(
            fig,
            perturbations={
                f"{pert_condition} ({analysis_variant} direction) impulse": (
                    impulse_amplitude, impulse_start_step, impulse_end_step
                )
            },
            n=n,
        )
        fig.add_vrect(
            x0=impulse_start_step, 
            x1=impulse_end_step,
            fillcolor='grey', 
            opacity=0.1, 
            line_width=0,
            name='Perturbation',
            # showlegend=True,
            # legend="legend2",
        )
        
        savefig(
            fig, 
            join([
                f"{label}",
                f"imp-amp-{impulse_amplitude}",
                f"imp-steps-{impulse_duration}",
            ]),
            subdir=f"fb_impulse_{pert_condition}/force_response",
        )
        
        fig.show()
```

### Velocity profiles 

```{python}
fig_subdir = "vel_profiles"
```

The x/y components:

```{python}
impulse_responses['xy']['vel'] = jt.map(
    lambda states: states.mechanics.effector.vel,
    all_states,
    is_leaf=is_module,
)
```

And the velocity components parallel and orthogonal to the perturbation directions:

```{python}
impulse_responses['aligned']['vel'] = project_and_lump(
    all_states, 
    lambda states: states.mechanics.effector.vel,
    directions,
)
```

#### Generate figures

```{python}
all_vels_plot = impulse_responses[components_plot]['vel']

axes_labels = [fr'$v_{sub}$' for sub in components_labels[components_plot]]

figs = {
    imp_condn: {
        label: fbp.profiles(
            tree_take_multi(vels, [i], [-1]),
            timesteps=jnp.arange(-n_preceding_steps, n_steps),
            mode='std', 
            varname=fr"${label}$",
            layout_kws=dict(
                legend=dict(
                    title="Train curl std.",
                ),
            ),
            scatter_kws=dict(legend="legend"),
        )
        for i, label in enumerate(axes_labels)
    }
    for imp_condn, vels in all_vels_plot.items()
}
```

```{python}
n = np.prod(jt.leaves(all_vels_plot)[0].shape[:-2])

for pert_condition in figs:
    for label, fig in figs[pert_condition].items():
        fig.add_vrect(
            x0=impulse_start_step, 
            x1=impulse_end_step,
            fillcolor='grey', 
            opacity=0.1, 
            line_width=0,
            name='Perturbation',
        )
        
        add_context_annotation(
            fig,
            perturbations={
                f"{pert_condition} ({analysis_variant} direction) impulse": (
                    impulse_amplitude, impulse_start_step, impulse_end_step
                )
            },
            n=n,
        )
        
        savefig(
            fig, 
            join([
                f"{label}",
                f"imp-amp-{impulse_amplitude}",
                f"imp-steps-{impulse_duration}",
            ]),
            subdir=f"fb_impulse_{pert_condition}/vel_response",
        )
        
        fig.show()
```

## Summary comparison of performance measures

```{python}
fig_subdir = "performance_measures"
```

We'll base these measures on the impulse response states which have already been aligned with the impulse directions.

```{python}
# We'll copy the top level so we can add stuff without affecting what happened earlier
all_responses = dict(impulse_responses['aligned'])  
```

Thus note that in what follows, "forward", "backward", "lateral" are all relative to the perturbation direction.

### Add some other state variables to the impulse response tree

We didn't manipulate the position, but we will want to compute deviations with respect to the steady state (i.e. target) position.

```{python}
all_states = all_states_imp[analysis_variant]

target_pos = jt.map(
    lambda task: task.validation_trials.targets['mechanics.effector.pos'].value,
    tasks,
    is_leaf=is_module,
)

all_responses['pos'] = jt.map(
    lambda states, target_pos: (
        states.mechanics.effector.pos
        - target_pos[:, 0:1, :]  # only the first timestep, though the goal should be constant
    ),
    all_states, tree_prefix_expand(target_pos, all_states, is_leaf=is_module),
    is_leaf=is_module,
)
```

### Define all performance measures

**TODO: I don't think this will work as-is, since `responses[key]` is a dict of OrderedDicts. It may be necessary to transpose the PyTree so the outer keys are on the inside, so we can tree map over the perturbation/training conditions**

Max forward force

```{python}
def get_max_forward_force(responses):
    return jnp.max(responses['force'][..., 0], axis=-1)
```

Max backward force

```{python}
def get_max_backward_force(responses):
    return jnp.max(-responses['force'][..., 0], axis=-1)
```

Max net control force 

```{python}
def get_net_force(responses):
    return jnp.linalg.norm(responses['force'], axis=-1)
    
def get_max_net_force(responses):
    return jnp.max(get_net_force(responses), axis=-1)
```

Sum control forces

```{python}
def get_sum_net_forces(responses):
    return jnp.sum(get_net_force(responses), axis=-1)
```

Max forward velocity 

```{python}
def get_max_forward_vel(responses):
    return jnp.max(responses['vel'][..., 0], axis=-1)
```

Max backward velocity 

```{python}
def get_max_backward_vel(responses):
    return jnp.max(-responses['vel'][..., 0], axis=-1)
```

Max lateral (left) velocity

```{python}
def get_max_lateral_vel(responses, mul=1.):
    return jnp.max(mul * responses['vel'][..., 1], axis=-1)
```

Max deviation from steady state position (projection doesn't matter)

```{python}
def get_deviation(responses):
    return jnp.norm(responses['pos'], axis=-1)

def get_max_deviation(responses):
    return jnp.max(get_deviation(responses), axis=-1)
```

Sum deviations from steady state position

```{python}
def get_sum_deviations(responses):
    return jnp.sum(get_deviation(responses), axis=-1)
```

### Calculate all performance measures 

```{python}
all_measure_funcs, measure_handles = tree_unzip(OrderedDict({
    "Max forward force": (get_max_forward_force, "force-forward-max"),
    "Max backward force": (get_max_backward_force, "force-backward-max"),
    "Max net force": (get_max_net_force, "force-net-max"),
    "Sum net forces": (get_sum_net_forces, "force-net-sum"),
    "Max forward velocity": (get_max_forward_vel, "vel-forward-max"),
    "Max backward velocity": (get_max_backward_vel, "vel-backward-max"),
    "Max lateral velocity (left)": (
        partial(get_max_lateral_vel, mul=1.), 
        "vel-lateral-left-max",
    ),
    "Max lateral velocity (right)": (
        partial(get_max_lateral_vel, mul=-1.), 
        "vel-lateral-right-max",
    ),
    "Max deviation": (get_max_deviation, "deviation-max"),
    "Sum deviations": (get_sum_deviations, "deviation-sum")
}))
```

```{python}
# def get_all_measures(measure_funcs: PyTree[Callable], responses: PyTree[Array]):
#     return jt.map(
#         lambda func: jt.map(
#             lambda states: func(states),
#             states,
#             is_leaf=is_module,
#         ),
#         measure_funcs,
#     )
    
# all_measures = get_all_measures(all_measure_funcs, all_states)
```

### Comparison of all training conditions

### Comparison of low-high training conditions