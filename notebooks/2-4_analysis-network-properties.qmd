---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "2-4"

TRAIN_NB_ID = "2-1"
```

# Analysis of network properties

Here are some basic analyses of network weights and activities. 

1. Eigendecomposition of the Jacobian matrix at goal steady-states.
2. Correlation between top PCs and the network output.
3. Fixed point structures. 

In particular, we want to see how these vary 

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from functools import partial
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import numpy as np
import plotly
import plotly.graph_objects as go
from sklearn.decomposition import PCA

from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.task import SimpleReaches, TrialSpecDependency
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.constants import WORKSPACE
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
    get_model_record,
    add_evaluation,
    add_evaluation_figure,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies.fp_finder import (
    FixedPointFinder,
    FPFilteredResults,
    fp_adam_optimizer,
    take_top_fps,
)
from rnns_learn_robust_motor_policies.misc import log_version_info
from rnns_learn_robust_motor_policies.train_setup_part2 import setup_task_model_pairs
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.plot_utils import figleaves
from rnns_learn_robust_motor_policies.setup_utils import (
    get_base_task,
    set_model_noise,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
    take_single_replicate,
)
from rnns_learn_robust_motor_policies.types import (
    ContextInputDict,
    PertAmpDict, 
    TrainStdDict,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
log_version_info(jax, eqx, plotly)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

```{python}
#| tags: [parameters]

# Specify which trained models to load 
training_methods_load = ['amplitude', 'std']
disturbance_type_load: Literal['curl', 'constant'] = 'curl'
disturbance_stds_load = [0.4, 0.8, 1.2]
feedback_noise_std_load = 0.0
motor_noise_std_load = 0.0
feedback_delay_steps = 0
hidden_size = 100

# Specify model parameters to use for analysis (None -> use training value)
# disturbance_type: Optional[Literal['curl', 'constant']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps = int(feedback_delay_steps)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

```{python}
model_info = get_model_record(
    db_session,
    training_methods=training_methods_load,
    disturbance_type=disturbance_type_load,
    disturbance_stds=disturbance_stds_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps,
    hidden_size=hidden_size,
    has_replicate_info=True,
)
```

```{python}
models_load: TrainStdDict[float, eqx.Module] = load(
    model_info.path, partial(setup_models_only, setup_task_model_pairs),
)

replicate_info = load(
    model_info.replicate_info_path, partial(setup_replicate_info, models_load),
)
```

### Modify the system noise if needed

```{python}
models_base = jt.map(
    partial(
        set_model_noise, 
        noise_stds=dict(
            feedback=feedback_noise_std,
            motor=motor_noise_std,
        ),
        enable_noise=True,
    ),
    models_load,
    is_leaf=is_module,
)
```

### Optionally select a subset of training conditions

Depending on how training goes, we might want to leave out some of the training conditions (i.e. training disturbance stds) from the analysis.

```{python}
disturbance_train_stds = model_info.disturbance_stds

models_base = jt.map(lambda d: subdict(d, disturbance_train_stds), models_load, is_leaf=is_type(TrainStdDict))
```

### Optionally exclude replicates that perform much worse than average

When plotting single-replicate examples, we want to show the best replicate. Also, when lumping together replicates and plotting distributions, we want to include as many replicates as possible to show how performance may vary, but also exclude replicates whose performance is much worse than the best replicate.

:::{note}
The logic here is that systems like the brain will have much more efficient learning systems, and that we are approximating their efficiency by taking advantage of variance between model initializations. 

In other words: we are interested in the kind of performance that is feasible with these kinds of networked controllers, more than the kind of performance that we should expect on average (or in the worst case) given the technical details of network initialization etc.
:::

```{python}
exclude_underperformers = True
```

```{python}
measure_to_exclude_by = 'best_total_loss'

included_replicates = replicate_info['included_replicates'][measure_to_exclude_by]
best_replicate = replicate_info['best_replicates'][measure_to_exclude_by]

def take_replicate_or_best(tree: TrainStdDict, i_replicate=None, replicate_axis=1):
    if i_replicate is None:
        map_func = lambda tree: TrainStdDict({
            train_std: tree_take(subtree, best_replicate[train_std], replicate_axis)
            for train_std, subtree in tree.items()
        })
    else:
        map_func = lambda tree: tree_take_multi(tree, [i_replicate], [replicate_axis])
        
    return jt.map(map_func, tree, is_leaf=is_type(TrainStdDict))
```

Which replicates are included?

```{python}
print("\nReplicates included in analysis for each training condition:")
eqx.tree_pprint(jt.map(lambda x: jnp.where(x)[0], included_replicates), short_arrays=False)
```

Set the indices corresponding to the excluded replicates to NaN in each of the model arrays. This ensures that the shapes of the arrays remain consistent. NaN results will be ignored at plotting time. 

```{python}
models_all_replicates = models_base

if exclude_underperformers:
    models_base = jt.map(
        lambda models, included: tree_set_scalar(models, jnp.nan, jnp.where(~included)[0]),
        models_base, included_replicates,
        is_leaf=is_module,
    )
    n_replicates_included = jt.map(lambda x: jnp.sum(x).item(), included_replicates)
else:
    models_base = models_base
    n_replicates_included = dict.fromkeys(models.keys(), model_info.n_replicates)
```

**TODO**: Make an annotation that indicates "$n/N$ replicates are included" or something.

## Sort out the evaluation parameters

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
eval_parameters = use_record_params_where_none(dict(
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
), model_info)
```

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    # n_evals=n_evals['full'],
)
```

## Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    origin=NB_ID,
    model_hash=model_info.hash,
    eval_parameters=eval_parameters,
)
```

## Distributions of model weights

Input-hidden, hidden-hidden, and hidden-output

Violins.

### By training condition

### By replicate (sanity check)

## Get steady-state fixed points

These are the "goal-goal" FPs; i.e. the point mass is at the goal, so the network will stabilize on some hidden state that outputs a constant force that does not change the position of the point mass on average.

This contrasts with non-steady-state fixed points, which correspond to network outputs which should cause the point mass to move, and thus the network's feedback input (and thus fixed point) to change.

**TODO**
**TODO**
**TODO**: Do the analysis for all replicates and all model variants (training stds and methods).
It's not difficult to map over the PyTree of model variants, however the replicates are a bit more
complex since `fpfinder` can be vmapped easily enough over candidate `states` for an ensemble, 
but we can't vmap over an `rnn_func` that is a `Callable` wrapping the batched weights. So we need 
to have an intermediate function that takes a single model and its candidate states, and constructs 
the `rnn_func` and runs the FPFinder; then vmap over thise function with `models` and `states`.
(It could also compute the states itself for the single model, however this would mean using `task.eval`
inside the function. Can we vmap over `task.eval` when it is inside another function, this way? We
should be able to...)

```{python}
models = models_base['std'][0.8]
```

### Define a task where the point mass is already at the goal

This is the same set of tasks (varying by context input) we used for the feedback perturbation analysis, except a bit shorter in duration.

```{python}
context_inputs = [-2., -1., 0., 1., 2.]
```

```{python}
eval_grid_n = 5
EVAL_N_DIRECTIONS = 1
EVAL_REACH_LENGTH = 0.0  
```

```{python}
# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=WORKSPACE, 
    n_steps=model_info.n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=EVAL_N_DIRECTIONS,
    eval_reach_length=EVAL_REACH_LENGTH,  
)
```

```{python}
goals_pos = task.validation_trials.targets["mechanics.effector.pos"].value[:, -1]
```

```{python}
def get_context_input_func(x, n_steps, n_trials):
    return lambda trial_spec, key: (
        jnp.full((n_trials, n_steps - 1), x, dtype=float)
    )

all_tasks = ContextInputDict({
    context_input: eqx.tree_at(
        lambda task: task.input_dependencies,
        task, 
        {
            'context': TrialSpecDependency(get_context_input_func(
                context_input, model_info.n_steps, task.n_validation_trials
            ))
        },
    )
    for context_input in context_inputs
})
```

### Evaluate the model on the task, to get candidate hidden states

```{python}
def evaluate_all_states():
    return jt.map(
        lambda task: task.eval_ensemble(models, model_info.n_replicates, key=key_eval),
        # lambda task: task.eval(model, key=key_eval),
        all_tasks,
        is_leaf=is_module,
    )
```

```{python}
all_states_bytes = tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states))

print(f"\nEstimate {all_states_bytes / 1e9:.2f} GB of memory needed for all states.")
```

```{python}
all_states = evaluate_all_states()
```

### Instantiate the fixed point finder

```{python}
# Fixed point optimization hyperparameters
fp_tol = 1e-7  # Used for both fp_tol and opt_stop_tol
fp_num_batches = 10000         # Total number of batches to train on.
# fp_batch_size = 128          # How many examples in each batch
fp_step_size = 0.2          # initial learning rate
fp_decay_factor = 0.9999     # decay the learning rate this much
fp_decay_steps = 1           #
fp_adam_b1 = 0.9             # Adam parameters
fp_adam_b2 = 0.999
fp_adam_eps = 1e-5
fp_opt_print_every = 200   # Print training information during optimziation every so often

# Fixed point finding thresholds and other HPs
fp_noise_var = 0.0      # Gaussian noise added to fixed point candidates before optimization.
# fp_opt_stop_tol = 0.00001  # Stop optimizing when the average value of the batch is below this value.
# fp_tol = 0.00001        # Discard fps with squared speed larger than this value.
fp_unique_tol = 0.025   # tolerance for determination of identical fixed points
fp_outlier_tol = 1.0    # Anypoint whos closest fixed point is greater than tol is an outlier.
```

```{python}
fp_optimizer = fp_adam_optimizer(
    learning_rate=fp_step_size, 
    decay_steps=fp_decay_steps, 
    decay_rate=fp_decay_factor, 
    b1=fp_adam_b1, 
    b2=fp_adam_b2, 
    eps=fp_adam_eps, 
)

fpfinder = FixedPointFinder(fp_optimizer)

fpf_func = partial(
    fpfinder.find_and_filter,
    outlier_tol=fp_outlier_tol,
    unique_tol=fp_unique_tol,    
)
```

### Find the fixed points

```{python}
def get_ss_network_input_with_context(pos, context):
    input_star = jnp.zeros((models.step.net.hidden.input_size,))
    # Set target and feedback inputs to the same position
    input_star = input_star.at[1:3].set(pos) 
    input_star = input_star.at[5:7].set(pos)
    return input_star.at[0].set(context)
    

def get_rnn_func_at_context(rnn, pos, x):
    input_star = get_ss_network_input_with_context(pos, x)
    def rnn_func(h):
        return rnn(input_star, h, key=key_eval)
    return rnn_func

def get_rnn_fps(rnn, candidate_states, context):
    fps = fpf_func(
        get_rnn_func_at_context(rnn, 0.0, context), 
        candidate_states, 
        fp_tol,
    )
    return fps
```

```{python}
stride = 8

all_fps = ContextInputDict({
    context_input: eqx.filter_vmap(get_rnn_fps, in_axes=(0, None, None))(
        models.step.net.hidden,
        jnp.reshape(
            all_states[context_input].net.hidden, 
            (-1, model_info.hidden_size),
        )[::stride],
        context_input,
    )
    for context_input in context_inputs
})
```

How many fixed points meet all criteria? 

```{python}
n_fps_meeting_criteria = jt.map(
    lambda fps: fps.counts['meets_all_criteria'], 
    all_fps, 
    is_leaf=is_type(FPFilteredResults),
)

print("\nNumber of fixed points meeting all criteria:")
eqx.tree_pprint(
    n_fps_meeting_criteria,
    short_arrays=False,
)
```

Note that *no* points may be found for some replicates, but that's OK.

:::{note}
More interestingly, sometimes more than one "fixed point" meets all criteria. But typically these are actually the same fixed point, except that due to differences in optimization, some context inputs converged better than others. 

We could try to vary `fp_tol`, or use some kind of adaptive convergence criteria, but instead it is simple enough for now to test if the "different" points are very far from each other, and if not, just average them and keep going. 

First, get the top 5 points for each replicate and context input.

```{python}
all_top_fps = take_top_fps(all_fps, n_keep=6)
```

Now get the standard deviation over the top 5, averaged over units.

```{python}
top_fps_std_mean = jt.map(
    lambda fps: jnp.nanmean(jnp.nanstd(fps, axis=1), axis=-1),
    all_top_fps,
)

print("\nStandard deviation of top 5 fixed points, mean over dimensions (network units):")
eqx.tree_pprint(top_fps_std_mean, short_arrays=False)
```

At this point we could exclude replicates based on a threshold for these standard deviation means. However, I've generally found that 

If the above printout of standard deviations has a value larger than about 0.01, this may need to be revisited. Thus, raise an error if that's the case, but otherwise do not worry about it.

```{python}
threshold = 0.01

all_top_fps_std_mean = jnp.stack(jt.leaves(top_fps_std_mean))
below_threshold = all_top_fps_std_mean < threshold

assert jnp.all(below_threshold | jnp.isnan(all_top_fps_std_mean)), (
    "\nStandard deviation between multiple candidate fixed points is high. " 
    "Consider excluding replicates for which this is true. "
)
```
:::

Find replicates for which a FP was found for every context input.

```{python}
satisfactory_replicates = jnp.all(jnp.stack(jt.leaves(n_fps_meeting_criteria), axis=0), axis=0)

print("\nReplicates for which a fixed point was found for every context input:")
eqx.tree_pprint(satisfactory_replicates, short_arrays=False)
```

Average over the top fixed points, to get a single one for each included replicate and control input.

```{python}
fps_final = jt.map(
    lambda fps: jnp.nanmean(fps[satisfactory_replicates], axis=1), 
    all_top_fps, 
    is_leaf=is_type(FPFilteredResults),
)
```

## Eigendecomposition of goal steady-state Jacobians

```{python}
jacobian_funcs = jt.map(
    lambda model: jax.jacobian(get_rnn_func(model.step.net)),
    trained_models,
    is_leaf=is_module,
)

jacobians = jt.map(
    lambda jac_func: eqx.filter_vmap(
        eqx.filter_vmap(
            jac_func, 
            # Map over context inputs
            in_axes=(None, None, 0, None),
        ),
        # Map over goal positions
        in_axes=(None, 0, None, None),
    ),
    jacobian_funcs,
)
```