---
jupyter: python3
---

```{python}
NB_ID = "2-1"
```

# Training models for Part 2

In this notebook, we train models similarly to [Part 1](1-1_train.qmd). Most of the details of the models and the training process are identical, and they will not be explained again here.

However, instead of training separate models on different amplitudes of perturbation, we instead train a single model, and give it contextual information about the presence/amplitude of the current disturbance. Since the direction of the disturbance remains unknown to the model, it must still learn a robust policy (rather than an adaptation), but we expect it to learn to modulate the robustness (vs. efficiency) of its policy based on the contextual information. 

If that is the case, then following training, we should be able to control the robustness of the model's policy by adjusting the value of the contextual input. 

:::{note}
Because different types of perturbations (e.g. curl force fields vs. random constant force fields) may induce different robust policies, it may also make sense to provide a second contextual input which indicates the type of the current disturbance. In principle this input should be categorical, however we could indicate it probabilistically (e.g. 0 -> curl, 1 -> random, 0.5 -> 50% chance of curl, vs. random) and this might allow us to interpolate between the two policies as well. 

However, in this notebook we will only train on the first contextual input.
::: 

## Approach to training

We will try at least two different approaches:

1. The contextual input is provided as 0 (no perturbation) or 1 (perturbation), regardless of the exact amplitude of the perturbation.
2. The contextual input is provided as the amplitude (though not the direction) of the perturbation for the same trial.

Note:

- We must decide on a standard deviation of the perturbation amplitude, for perturbed trials. This will be held constant for each trained model; i.e. we may still have multiple trained models/conditions, one for each different stds of perturbation amplitude.
- In 1, clearly some trials should not have a perturbation at all.
- In 2, we may also decide to include some simply unperturbed trials, in which case the contextual input will certainly be 0. However this may be unnecessary, as even if all of the trials are perturbed, many of the perturbations will have small amplitude (given that the mean amplitude is 0).

There is a third approach, which may be worth trying, and which might avoid the need to train the network on multiple conditions:

3. The contextual input is provided as the standard deviation of the distribution from which the perturbation amplitude is drawn, rather than the exact amplitude; and this std is itself drawn from another distribution, either for each trial or for each batch of trials.

In that case, the contextual input is more of an indirect measure of uncertainty (i.e. the probability that the network will experience a large perturbation on this trial), whereas in 2 it is a more direct measure of degree of robustness needed, and in 1 it is an indirect measure but anchored to a given perturbation distribution.

## General predictions

The contextual input will be continuous, so that even in the case of 1, after training we could still set it to 0.5 and reasonably expect to observe a "half-robust" policy.

Assuming that sufficient robustness to larger perturbations versus smaller perturbations means scaling the *same kind of policy*, rather than that a structural change to the policy is necessary to deal with a larger versus a smaller perturbation, then we may be able to train a network on a single condition using approach 1 or 2, i.e. with a single std of perturbation amplitude, and obtain a similar result (modulo the scaling of the contextual input) as if we had trained on a different std of perturbation amplitude. For example, if using approach 1 we train network A on std 1.0 and network B on std 2.0, then after training we may find that network B with contextual input 0.5 behaves similarly to network A with input 1.0 (assuming linearity for the sake of illustration).

Likewise, we may be able to achieve generalization of robustness to perturbations which are rare or absent in the training set, due to their large amplitudes, by extrapolating the value of the contextual input to larger values than seen in the training set.

I suspect approaches 2 and 3 will differ somewhat because of less precise information being available to the network in 3; however it is possible that this will only matter very early in the trials, when the network has not had time to receive feedback to indicate the magnitude of the disturbance.


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```


```{python}
from functools import partial
from typing import Any, Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import numpy as np
import optax 

import feedbax
from feedbax import (
    is_module,
    is_type,
    tree_unzip,
    tree_map_tqdm,
)
from feedbax.misc import where_func_to_labels
from feedbax.train import TaskTrainer
from feedbax.xabdeef.losses import simple_reach_loss

import rnns_learn_robust_motor_policies 
from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
    save_model_and_add_record,
)
from rnns_learn_robust_motor_policies.misc import log_version_info
from rnns_learn_robust_motor_policies.setup_utils import (
    get_readout_norm_loss,
)
from rnns_learn_robust_motor_policies.train_setup_part2 import (
    setup_task_model_pairs, 
)
from rnns_learn_robust_motor_policies.tree_utils import pp, subdict
from rnns_learn_robust_motor_policies.types import TaskModelPair
from rnns_learn_robust_motor_policies.train_setup import (
    concat_save_iterations,
    iterations_to_save_model_parameters,
    make_delayed_cosine_schedule,
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
version_info = log_version_info(
    jax, eqx, optax, git_modules=(feedbax, rnns_learn_robust_motor_policies)
)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

These are parameters that can be [varied](https://quarto.org/docs/computations/parameters.html) through the command line interface to Quarto. 

```{python}
#| tags: [parameters]

disturbance_type: Literal['curl', 'constant'] = 'curl'  
feedback_delay_steps = 0
feedback_noise_std = 0.1
motor_noise_std = 0.1
hidden_size = 100
n_replicates = 5
n_steps = 100
dt = 0.05

n_batches_baseline = 0
n_batches_condition = 10000
batch_size = 250
learning_rate_0 = 0.001
constant_lr_iterations = 2000 # Number of initial training iterations to hold lr constant
cosine_annealing_alpha = 1.0  # Max learning rate factor decrease during cosine annealing 
weight_decay = 0

# Force the Frobenius norm of the readout weight matrix to be close (squared error) to this value
readout_norm_value = 2.0
readout_norm_loss_weight = 0.0

# TODO: Implement this for part 2!
n_scaleup_batches = 1000
intervention_scaleup_batches = (n_batches_baseline, n_batches_baseline + n_scaleup_batches)

# reset the optimizer state at these iterations
state_reset_iterations = jnp.array([])

# change which parameters are trained, after a given number of iterations
where_train = {
    0: lambda model: (
        model.step.net.hidden,
        model.step.net.readout, 
    ),
    # stop training the readout 
    # 1000: lambda model: model.step.net.hidden,
}

training_methods: list[Literal["active", "amplitude", "std"]] = ["amplitude", "std"]

p_perturbed = {
    'active': 0.25,
    # The rest don't do anything atm, even if they're <1
    'amplitude': 1.0,  
    'std': 1.0,  
}

# Define the disturbance amplitudes to train, depending on disturbance type
# NOTE: Only one of these disturbance types is trained per notebook run; see the parameters cell above
disturbance_stds = {
    'curl': [0.4, 0.8, 1.2, 1.6, 2.0],
    'constant': [0.4, 0.8, 1.2, 1.6, 2.0],
}
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Set up models and tasks for the different training variants

We need to define:

1. The type of disturbance, which is chosen as a hyperparameter earlier in this notebook;
2. The disturbance amplitudes to train -- each resulting in a distinct trained model;
3. For each training variant, the probability that each trial will be perturbed;
4. The form of the context input to the neural network; 
5. Finally, the task-model pairs that will be trained on each disturbance amplitude, for each training variant.

Some of these (e.g. the form of the context input, `CONTEXT_INPUT_FUNCS`) are defined once and for all in the project module `train_setup_part2.py`.

```{python}
task_model_pairs = setup_task_model_pairs(
    n_replicates=n_replicates,
    dt=dt,
    hidden_size=hidden_size,
    n_steps=n_steps,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
    intervention_scaleup_batches=intervention_scaleup_batches,
    training_methods=training_methods,
    p_perturbed=p_perturbed,
    key=key_init,
) 
```

## Training setup

```{python}
optimizer_class = partial(
    optax.adamw,
    weight_decay=weight_decay,
)

where_train = lambda model: (
    model.step.net.hidden,
    model.step.net.readout, 
)

n_batches = n_batches_baseline + n_batches_condition
save_model_parameters = iterations_to_save_model_parameters(n_batches)
```

```{python}
schedule = make_delayed_cosine_schedule(
    learning_rate_0, 
    constant_lr_iterations, 
    n_batches, 
    cosine_annealing_alpha,
) 

trainer = TaskTrainer(
    optimizer=optax.inject_hyperparams(optimizer_class)(
        learning_rate=schedule
    ),
    checkpointing=True,
)
```

```{python}
readout_norm_loss = readout_norm_loss_weight * get_readout_norm_loss(readout_norm_value)
loss_func = simple_reach_loss() + readout_norm_loss
```

## Train the task-model pairs

```{python}
train_params = dict(
    ensembled=True,
    loss_func=loss_func,
    where_train=where_train,
    batch_size=batch_size, 
    log_step=500,
    save_model_parameters=save_model_parameters,
    state_reset_iterations=state_reset_iterations,
    # disable_tqdm=True,
)


key0, key1 = jr.split(key_train, 2)


def train_pair(pair):   
    if n_batches_baseline > 0:
        pretrained, pretrain_history, opt_state = trainer(
            task_baseline,
            pair.model,
            n_batches=n_batches_baseline, 
            run_label="Baseline training",
            key=key0,
            **train_params,
        )
    else: 
        pretrained = pair.model
        pretrain_history = None
        opt_state = None
    
    trained, train_history, _ = trainer(
        pair.task, 
        pretrained,
        opt_state=opt_state,
        n_batches=n_batches_condition, 
        idx_start=n_batches_baseline,
        run_label="Condition training",
        key=key1,
        **train_params,
    )
    
    if pretrain_history is None:
        train_history_all = train_history
    else:
        train_history_all = tree_concatenate([pretrain_history, train_history])
    
    return trained, train_history_all


trained_models, train_histories = tree_unzip(tree_map_tqdm(
    train_pair,
    task_model_pairs,
    label="Training all pairs",
    is_leaf=is_type(TaskModelPair),
))
```

## Save the models with their parameters on the final iteration

```{python}
save_model_parameters_all = concat_save_iterations(
    save_model_parameters, 
    (n_batches_baseline, n_batches_condition),
)

where_train_strs = jt.map(where_func_to_labels, where_train)

training_hyperparameters = dict(
    learning_rate_0=learning_rate_0,
    constant_lr_iterations=constant_lr_iterations,
    cosine_annealing_alpha=cosine_annealing_alpha,
    weight_decay=weight_decay,
    n_batches=n_batches,
    n_batches_condition=n_batches_condition,
    n_batches_baseline=n_batches_baseline,
    batch_size=batch_size,
    save_model_parameters=save_model_parameters.tolist(),
    where_train_strs=where_func_to_labels(where_train),
    training_methods=training_methods,
    state_reset_iterations=state_reset_iterations.tolist(),
    p_perturbed=p_perturbed,
)
```

```{python}
model_hyperparameters = dict(
    n_replicates=n_replicates,
    hidden_size=hidden_size,
    feedback_delay_steps=feedback_delay_steps,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
    dt=dt,
    n_steps=n_steps,
    disturbance_type=disturbance_type,
    disturbance_stds=disturbance_stds[disturbance_type],
    readout_norm_loss_weight=readout_norm_loss_weight,
    readout_norm_value=readout_norm_value,
    intervention_scaleup_batches=intervention_scaleup_batches,
    training_methods=training_methods,
    p_perturbed=p_perturbed,
)
```

```{python}
train_histories_hyperparameters = dict(
    disturbance_stds=disturbance_stds[disturbance_type],
    n_batches=n_batches,
    batch_size=batch_size,
    n_replicates=n_replicates,
    where_train_strs=where_func_to_labels(where_train),
    save_model_parameters=save_model_parameters.tolist(),
    readout_norm_loss_weight=readout_norm_loss_weight,
    readout_norm_value=readout_norm_value,
)
```

```{python}
model_record = save_model_and_add_record(
    db_session,
    origin=NB_ID,
    model=trained_models,
    model_hyperparameters=model_hyperparameters,
    other_hyperparameters=training_hyperparameters,
    train_history=train_histories,
    train_history_hyperparameters=train_histories_hyperparameters,
    version_info=version_info,
)
```
