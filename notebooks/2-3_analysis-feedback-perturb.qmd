---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "2-3"

# TODO: This is clear in the eval_rules file; probably don't need to specify here again
TRAIN_NB_ID = "2-1"  # Notebook the models were trained in
```

# Analysis of feedback perturbations



## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import namedtuple
from functools import partial
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import numpy as np
import plotly
import plotly.graph_objects as go
from tqdm.auto import tqdm

import feedbax
from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_prefix_expand,
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.intervene import (
    CurlField, 
    FixedField, 
    schedule_intervenor,
)
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches, TrialSpecDependency
from feedbax.xabdeef.losses import simple_reach_loss

import rnns_learn_robust_motor_policies
from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
    get_colors_dicts_from_discrete,
)
from rnns_learn_robust_motor_policies.constants import (
    INTERVENOR_LABEL,
    POS_ENDPOINTS_ALIGNED,
    WORKSPACE,
)
from rnns_learn_robust_motor_policies.database import (
    get_db_session,
    get_model_record,
    add_evaluation,
    add_evaluation_figure,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies import measures
from rnns_learn_robust_motor_policies.measures import (
    MEASURES, 
    MEASURE_LABELS,
    RESPONSE_VAR_LABELS,
    Direction,
    Measure,
    Responses,
    ResponseVar,
    compute_all_measures,
    output_corr,
    signed_max,
)
from rnns_learn_robust_motor_policies.misc import log_version_info, lohi
from rnns_learn_robust_motor_policies.perturbations import (
    feedback_impulse,
)
from rnns_learn_robust_motor_policies.plot import (
    add_endpoint_traces,
    get_violins,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    figleaves,
    figs_flatten_with_paths,
)
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.setup_utils import (
    get_base_task,
    convert_tasks_to_small,
    set_model_noise,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_aligned_vars,
    get_pos_endpoints,
    orthogonal_field,
    project_onto_direction,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.train_setup_part2 import (
    setup_task_model_pairs, 
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
)
from rnns_learn_robust_motor_policies.types import (
    ContextInputDict,
    ImpulseAmpTuple,
    PertAmpDict,
    PertVarDict,
    TrainingMethodDict,
    TrainStdDict, 
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
version_info = log_version_info(
    jax, eqx, plotly, git_modules=(feedbax, rnns_learn_robust_motor_policies)
)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'constant'] = 'constant'
feedback_noise_std_load = 0.01
motor_noise_std_load = 0.01
feedback_delay_steps_load = 0
hidden_size = 100

disturbance_stds_load = [0.0, 0.01, 0.02, 0.03, 0.04, 0.08, 0.16, 0.32]
training_methods_load = ['std']

# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'constant']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

```{python}
# context_inputs = [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
context_inputs = [-2., -1., 0., 1., 2.]
```

```{python}
max_impulse_amplitude = dict(
    pos=1.8,
    vel=1.2,
)

n_impulse_amplitudes = 3

impulse_start_step = 30  
impulse_duration = 5  # steps

eval_grid_n = 5
```

The analysis will be performed for only one training method, and for a subset of the `disturbance_stds_load`:

```{python}
training_method = 'std'  
disturbance_stds_train = [0.0, 0.04, 0.08]
```

#### 

```{python}
pert_var_names = ('pos', 'vel')
feedback_var_idxs = dict(zip(pert_var_names, range(len(pert_var_names))))
```

#### Casting

Parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

### Colors setup

```{python}
# when coloring by training condition
disturbance_stds_train_colors, disturbance_stds_train_colors_dark = get_colors_dicts(
    disturbance_stds_train, COLORSCALES['disturbance_train_stds'],
)

# when coloring by perturbed feedback variable
pert_vars_colors, pert_vars_colors_dark = get_colors_dicts_from_discrete(
    pert_var_names, COLORSCALES['fb_pert_vars']
)

# by context input 
context_input_colors, context_input_colors_dark = get_colors_dicts(
    context_inputs, COLORSCALES['context_inputs'],
)
```

## Load and adjust trained models

```{python}
model_info = get_model_record(
    db_session,
    origin=TRAIN_NB_ID,
    has_replicate_info=True,
    disturbance_type=disturbance_type_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps_load,
    hidden_size=hidden_size,
    n_batches=10000,
    disturbance_stds=disturbance_stds_load,
    learning_rate_0=0.001,
    # intervention_scaleup_batches=[0, 1000],
    training_methods=training_methods_load,
)
```

```{python}
models_load: TrainStdDict[float, eqx.Module] = load(
    model_info.path, partial(setup_models_only, setup_task_model_pairs),
)

replicate_info = load(
    model_info.replicate_info_path, partial(setup_replicate_info, models_load),
)
```

### Take a subset of training methods & conditions for this analysis

```{python}
models_load, replicate_info = jt.map(
    lambda d: subdict(d[training_method], disturbance_stds_train),
    (models_load, replicate_info),
    is_leaf=is_type(TrainingMethodDict),    
)
```

### Modify the system noise if needed

```{python}
models_base = jt.map(
    partial(
        set_model_noise, 
        noise_stds=dict(
            feedback=feedback_noise_std,
            motor=motor_noise_std,
        ),
        enable_noise=True,
    ),
    models_load,
    is_leaf=is_module,
)
```

### Optionally exclude replicates that perform much worse than average

When plotting single-replicate examples, we want to show the best replicate. Also, when lumping together replicates and plotting distributions, we want to include as many replicates as possible to show how performance may vary, but also exclude replicates whose performance is much worse than the best replicate.

:::{note}
The logic here is that systems like the brain will have much more efficient learning systems, and that we are approximating their efficiency by taking advantage of variance between model initializations. 

In other words: we are interested in the kind of performance that is feasible with these kinds of networked controllers, more than the kind of performance that we should expect on average (or in the worst case) given the technical details of network initialization etc.
:::

```{python}
exclude_underperformers = True
```

```{python}
measure_to_exclude_by = 'best_total_loss'

included_replicates = replicate_info['included_replicates'][measure_to_exclude_by]
best_replicate = replicate_info['best_replicates'][measure_to_exclude_by]

def take_replicate_or_best(tree: TrainStdDict, i_replicate=None, replicate_axis=1):
    if i_replicate is None:
        map_func = lambda tree: TrainStdDict({
            train_std: tree_take(subtree, best_replicate[train_std], replicate_axis)
            for train_std, subtree in tree.items()
        })
    else:
        map_func = lambda tree: tree_take_multi(tree, [i_replicate], [replicate_axis])
        
    return jt.map(map_func, tree, is_leaf=is_type(TrainStdDict))
```

Set the indices corresponding to the excluded replicates to NaN in each of the model arrays. This ensures that the shapes of the arrays remain consistent. NaN results will be ignored at plotting time. 

```{python}
models_all_replicates = models_base

if exclude_underperformers:
    print("\nReplicates included in analysis for each training condition:")
    eqx.tree_pprint(jt.map(lambda x: jnp.where(x)[0], included_replicates), short_arrays=False)
    models_base = jt.map(
        lambda models, included: tree_set_scalar(models, jnp.nan, jnp.where(~included)[0]),
        models_base, included_replicates,
        is_leaf=is_module,
    )
    n_replicates_included = jt.map(lambda x: jnp.sum(x).item(), included_replicates)
else:
    n_replicates_included = dict.fromkeys(models_base.keys(), model_info.n_replicates)
```

## Sort out the evaluation parameters

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
eval_parameters = use_record_params_where_none(dict(
    disturbance_type=disturbance_type,
    feedback_noise_std=feedback_noise_std,
    motor_noise_std=motor_noise_std,
), model_info)

disturbance_type = eval_parameters["disturbance_type"]
```

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Number of evaluations per model and condition

```{python}
n_evals = 5

if not any_system_noise:
     n_evals = 1
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    n_evals=n_evals,
    eval_grid_n=eval_grid_n,
    impulse_start_step=impulse_start_step,
    impulse_duration=impulse_duration,
    max_impulse_amplitude=max_impulse_amplitude,
    n_impulse_amplitudes=n_impulse_amplitudes,
)
```

## Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    model_hash=model_info.hash,
    eval_parameters=eval_parameters,
    origin=NB_ID,
    version_info=version_info,
)
```

## Setup tasks with impulse perturbations to different feedback channels

### Setup the base task

```{python}
EVAL_N_DIRECTIONS = 1
EVAL_REACH_LENGTH = 0.0  
```

```{python}
# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=WORKSPACE, 
    n_steps=model_info.n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=EVAL_N_DIRECTIONS,
    eval_reach_length=EVAL_REACH_LENGTH,  
)
```

### Tasks with impulse feedback perturbations in random directions

```{python}
impulse_amplitudes = jt.map(
    lambda max_amp: jnp.linspace(0, max_amp, n_impulse_amplitudes + 1)[1:],
    max_impulse_amplitude,
)

impulse_end_step = impulse_start_step + impulse_duration
impulse_time_idxs = slice(impulse_start_step, impulse_end_step)
```

```{python}
all_tasks_imp, all_models_imp = tree_unzip(jt.map(
    lambda feedback_var_idx: schedule_intervenor(
        task, models_base,
        lambda model: model.step.feedback_channels[0],
        feedback_impulse(  
            model_info.n_steps,
            1.0, #impulse_amplitude[pert_var_names[feedback_var_idx]],
            impulse_duration,
            feedback_var_idx,   
            impulse_start_step,
        ),
        default_active=False,
        stage_name="update_queue",
    ),
    PertVarDict(pos=0, vel=1),
    is_leaf=is_type(tuple),
))
```

```{python}
impulse_directions = jt.map(
    lambda task: task.validation_trials.intervene['ConstantInput'].arrays[:, impulse_start_step],
    all_tasks_imp,
    is_leaf=is_module,
)
```

### Task variants with different context inputs

**TODO**: Ideally we'd just `tree_at` or `vmap` a single instance, instead of constructing a whole PyTree of them

```{python}
def get_context_input_func(x, n_steps, n_trials):
    return lambda trial_spec, key: (
        jnp.full((n_trials, n_steps - 1), x, dtype=float)
    )

all_tasks = jt.map(
    lambda task: ContextInputDict({
        context_input: eqx.tree_at(
            lambda task: task.input_dependencies,
            task, 
            {
                'context': TrialSpecDependency(get_context_input_func(
                    context_input, model_info.n_steps, task.n_validation_trials
                ))
            },
        )
        for context_input in context_inputs
    }),
    all_tasks_imp,
    is_leaf=is_module,
)
```


## Evaluate the trained models on the perturbed tasks 

:::{note}
This is a bit different than 1-2b:

1. Only consider perturbations in random directions.
2. Evaluate perturbations over a range of context inputs.
3. Evaluate over a more limited range of training stds.

2 means that we need an additional vmap, over `context_inputs`.

3 isn't absolutely necessary, however we were already close to using all available memory in 1-2b when evaluating multiple times per condition (i.e. when noise wasn't zero) so adding in an extra dimension of variation would likely lead to an overflow.

This is OK since we probably only want to perform the analysis for train std 0 (to make sure nothing is amiss at baseline) and for the train std that gives the best spread of unrobust-to-hyperrobust behaviour, across context inputs.
:::


```{python}
def eval_with_imp_amplitude(impulse_amplitude, models, task, n_evals, key_eval):
    task = eqx.tree_at(
        lambda task: task.intervention_specs.validation['ConstantInput'].intervenor.params.scale,
        task,
        impulse_amplitude,
    ) 
    return vmap_eval_ensemble(models, task, n_evals, key_eval)


def evaluate_all_impulse_responses():
    # Wrap as a function for the convenience of estimating the amount of memory needed for the result.
    return PertVarDict({
        pert_var: jt.map(  # Necessary to map over possibly an extra tree level in different analysis variants
            lambda models: jt.map(  # Maps over the models in the OrderedDict (train conditions)
                lambda task: eqx.filter_vmap(
                    eval_with_imp_amplitude,
                    in_axes=(0, None, None, None, None)
                )(
                    impulse_amplitudes[pert_var], 
                    models, 
                    task, 
                    n_evals, 
                    key_eval,
                ),
                all_tasks[pert_var],
                is_leaf=is_module,
            ),
            all_models_imp[pert_var],
            is_leaf=is_module,
        )
        for pert_var in pert_var_names
    })
```

```{python}
all_states_bytes = tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_impulse_responses))

print(f"\nEstimate {all_states_bytes / 1e9:.2f} GB of memory needed for all responses.")
```

```{python}
all_states = evaluate_all_impulse_responses()
```

## Get responses, aligned to impulse direction

Take a subset of the states and project them.

```{python}
steady_pos = task.validation_trials.inits["mechanics.effector"].pos

profile_vars_where = Responses(
    pos=lambda states: states.mechanics.effector.pos - steady_pos[..., None, :],
    vel=lambda states: states.mechanics.effector.vel,
    force=lambda states: states.efferent.output,
)

# In plots we'll label them this way
profile_vars_labels = Responses(pos='p', vel='v', force='F')
```

```{python}
impulse_responses = jt.map(
    lambda where: jt.map(
        lambda states, directions: project_onto_direction(
            where(states),
            directions,
        ),
        all_states, 
        tree_prefix_expand(impulse_directions, all_states, is_leaf=is_module),
        is_leaf=is_module,
    ),
    profile_vars_where,
)
```

## Compare response trajectories

```{python}
n_preceding_steps = 0
```

```{python}
components_labels = (r'\parallel', r'\bot')
components_names = ('parallel', 'orthogonal')
```

```{python}
def get_all_profiles(all_plot, var_label, colors, dict_type, legend_title):   
    y_axes_labels = [fr'${var_label}_{sub}$' for sub in components_labels]
    
    figs = {
        coord_label: {
            pert_condn: jt.map(
                lambda var_by_train_condn: fbp.profiles(
                    tree_take(var_by_train_condn, i, -1),
                    timesteps=jnp.arange(-n_preceding_steps, model_info.n_steps),
                    mode='std', 
                    varname=fr"${y_axes_labels[i]}$",
                    colors=list(colors.values()),
                    layout_kws=dict(
                        legend_title=legend_title,
                        width=600,
                        height=400,
                        legend_tracegroupgap=1,
                    ),
                ),
                var_by_pert_amp,
                is_leaf=is_type(dict_type),
            )
            for pert_condn, var_by_pert_amp in all_plot.items()
        }
        for i, coord_label in enumerate(components_names)
    }

    for path, fig in figs_flatten_with_paths(figs):
        component_name = path[0].key
        pert_var = path[1].key
        pert_condition = '-'.join(str(dict_key.key) for dict_key in path[1:-1])
        pert_amp = path[-1].key
        
        fig.add_vrect(
            x0=impulse_start_step, 
            x1=impulse_end_step,
            fillcolor='grey', 
            opacity=0.1, 
            line_width=0,
            name='Perturbation',
        )
    
    return figs
```

### Compare context inputs, for a particular training condition and impulse amplitude

```{python}
plot_id = "response_profiles/compare_context_inputs"
```

```{python}
impulse_amplitude_idx = -2  

train_std = 0.04
```

```{python}
responses_plot = jt.map(
    lambda d: d[train_std],
    impulse_responses,
    is_leaf=is_type(TrainStdDict),
) 

responses_plot = jt.map(
    lambda arr: arr[impulse_amplitude_idx],
    responses_plot, 
)
```

```{python}
figs = jt.map(
    lambda responses, label: get_all_profiles(
        responses, label, context_input_colors_dark, ContextInputDict, "Context input"
    ),
    responses_plot, 
    profile_vars_labels,
    is_leaf=is_type(PertVarDict),
)
```

```{python}
for path, fig in figs_flatten_with_paths(figs):
    var_name = path[0].name
    component_name = path[1].key
    pert_var = path[2].key
    
    n = int(np.prod(jt.leaves(getattr(impulse_responses, var_name))[0].shape[:-2]))
    
    fig_parameters = dict(
        disturbance_train_std=train_std,
        disturbance_type=f"impulse/feedback/{pert_var}",
        disturbance_amplitude=impulse_amplitudes[pert_var][impulse_amplitude_idx],
        component_name=component_name,
        variable_name=getattr(profile_vars_labels, var_name),
        n=n,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
    
    print(var_name, component_name, pert_var)
    fig.show()
```

## Measure distributions

```{python}
shortly_after_impulse = slice(impulse_end_step, impulse_end_step + impulse_duration)
after_impulse = slice(impulse_end_step, None)

# Use these as-is from the `measures` module
measure_keys = [
    "max_net_force",
    "max_parallel_force_reverse",
    "sum_net_force",
    "max_parallel_vel_forward",
    "max_parallel_vel_reverse",
    "max_orthogonal_vel_left",
    "max_orthogonal_vel_right",
    "max_deviation",
    "sum_deviation",
]

# Add a couple more measures over specific intervals of the trials
custom_measures, custom_measure_labels = tree_unzip({
    "max_parallel_force_forward_shortly_after_impulse": (
        measures.set_timesteps(
            measures.max_parallel_force, shortly_after_impulse,
        ),
        f"Max forward force within {impulse_duration} steps of pert. end",
    ),
    "max_net_force_during_impulse": (
        measures.set_timesteps(
            measures.max_net_force, impulse_time_idxs,
        ),
        "Max net force during pert.",
    ),
    "max_net_force_after_impulse": (
        measures.set_timesteps(
            measures.max_net_force, after_impulse,
        ),
        "Max net force after pert.",
    ),
})

all_measures = subdict(MEASURES, measure_keys) | custom_measures
measure_labels = MEASURE_LABELS | custom_measure_labels
```

### Calculate all performance measures 

Rearrange response PyTree so `Responses` tuples are on the inside, and `ImpulseAmpTuple` is on the outside.

```{python}
from feedbax import move_level_to_outside

impulse_responses_tuples = jt.transpose(
    jt.structure(impulse_responses, is_leaf=is_type(PertVarDict)), 
    jt.structure(jt.leaves(impulse_responses, is_leaf=is_type(PertVarDict))[0]),
    impulse_responses,
)
```

```{python}   
all_measure_values = compute_all_measures(all_measures, impulse_responses_tuples)

# all_measure_values_lohi = jt.map(lohi, all_measure_values, is_leaf=is_type(TrainStdDict))
```

### Comparison of context inputs across impulse amplitudes

```{python}
plot_id = "performance_measures/compare_impulse_amplitudes"
```

```{python}
train_std = 0.04

measures_plot = jt.map(
    lambda d: d[train_std],
    all_measure_values,
    is_leaf=is_type(TrainStdDict),
)

measures_plot = jt.map(
    lambda d: PertVarDict({
        pert_var: jt.map(
            lambda arr: PertAmpDict(zip(impulse_amplitudes[pert_var].tolist(), arr)),
            d[pert_var],
        )
        for pert_var in d
    }),
    measures_plot,
    is_leaf=is_type(PertVarDict),
)
```

```{python}
figs = {
    key: {
        pert_var: get_violins(
            measure[pert_var], 
            yaxis_title=measure_labels[key],
            xaxis_title="Impulse amplitude",
            legend_title="Context input",
            colors=context_input_colors_dark,
            arr_axis_labels=["Evaluation", "Replicate", "Condition"],
            layout_kws=dict(
                width=800,
                height=600,
                # yaxis_range=[0, measure_ranges[key][1]],
                title=f"{pert_var} feedback impulse",
                violinmode="group",
            ),
        )
        for pert_var in pert_var_names
    }
    for key, measure in measures_plot.items()
}
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    pert_var = path[1].key
    n_dist = int(np.prod(jt.leaves(all_measure_values[path[0].key])[0].shape))
    
    fig_parameters = dict(
        # analysis_variant=analysis_variant,
        measure_name=path[0].key,
        disturbance_type=f"impulse/feedback/{pert_var}",
        n=n_dist,
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```

```{python}
for key in all_measures:
    figs[key]['pos'].show()
    figs[key]['vel'].show()
```