---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

# Analysis of plant perturbations

In this notebook we begin our analysis of the models trained in the presence of different levels of curl force fields:
- examine differences in baseline performance and state profiles of the models; e.g. differences in velocity profiles
- compare robustness to mechanical disturbances; e.g. differences in endpoint error when subject to large curl fields; 


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict, namedtuple
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import numpy as np
import plotly.colors as plc
import plotly.graph_objects as go
from tqdm.auto import tqdm

from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import (
    CurlField, 
    FixedField, 
    add_intervenors, 
    schedule_intervenor,
)
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.misc import lohi, load_from_json
from rnns_learn_robust_motor_policies.part1_setup import (
    setup_models, 
    setup_model_parameter_histories,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    add_context_annotation,
    add_endpoint_traces,
    figleaves,
    get_savefig_func,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    find_unique_filepath,
    set_model_noise,
    setup_train_histories,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_forward_lateral_vel, 
    get_lateral_distance,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
    swap_model_trainables, 
)
from rnns_learn_robust_motor_policies.types import PertAmpDict, TrainStdDict
```

```{python}
# jax.config.update("jax_compilation_cache_dir", "/tmp/jax_cache")
# jax.config.update("jax_persistent_cache_min_entry_size_bytes", -1)
# jax.config.update("jax_persistent_cache_min_compile_time_secs", 0)
# jax.config.update("jax_explain_cache_misses", True)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"feedbax commit: {git_commit_id()}\n")
```

Unique ID for notebook, for naming outputs.

```{python}
NB_ID = "1-2a"
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'curl'
feedback_noise_std_load = 0.04
motor_noise_std_load = 0.04
feedback_delay_steps_load = 0

# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
noise_stds = dict(
    feedback=feedback_noise_std,
    motor=motor_noise_std,
)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### Directories and filenames

```{python}
MODELS_DIR = Path('../models')
# FIGS_BASE_DIR = Path('/mnt/storage/tmp/figures')
FIGS_BASE_DIR = Path('../figures')

if not MODELS_DIR.exists():
    raise FileNotFoundError(f"Models directory not found: {MODELS_DIR.absolute()}")
```

Strings with which the saved model and hyperparameter files were labeled.

:::{note}
Note that "best_params" refers to the output of `post_training.py` run over the models trained by notebook 1-1.
:::

```{python}
MODEL_FILE_LABEL = "trained_models_best_params"
HYPERPARAMS_FILE_LABEL = "hyperparameters"
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Define base task

We will generally evaluate on a 2*2 grid of center-out reach sets (i.e. 4 sets total), with 24 reach directions per set. This is to ensure good coverage and a larger set of conditions/trials on which to perform statistics. 

In the case of visualization of center-out sets, we'll use a smaller version of the task with only a single set of 7 reaches (using an odd number helps with visualization).

For 4 sets of 24 center-out reaches (i.e. 96 reach conditions), with 10 replicates and 5 evaluations (i.e. 50 trials) per reach condition, and 100 timesteps, evaluating each task variant leads to approximately 1.5 GB of states. If we run out memory, it may be necessary to:

- reduce the number of evaluation reaches (e.g. `n_evals` or `eval_n_directions`);
- wait to evaluate until we have decided on a subset of trials to plot; i.e. define a function to evaluate subsets of data as needed;
- evaluate on CPU (assuming we have more RAM available).

```{python}
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

# TODO: Define this centrally

eval_grid_n = 2
eval_n_directions = 24
eval_reach_length = 0.5
n_conditions = eval_grid_n ** 2 * eval_n_directions

eval_grid_n_small = 1
eval_n_directions_small = 7
eval_reach_length_small = 0.5
n_conditions_small = eval_grid_n_small ** 2 * eval_n_directions_small

# Define the base task
task_base = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=eval_n_directions,
    eval_reach_length=eval_reach_length,  
)
```

We need to define the base task now so that we can load the training histories (particularly, the losses) in a moment. The actual task variants we'll evaluate on -- large and small, and across a range of disturbance amplitudes -- will be defined after the trained models are loaded.

**TODO**: We can probably move this back down to where the rest of the task setup is done, if we no longer need to load the entire training histories but only the stuff in the "extras" file.

## Load and adjust trained models

### Specify file to load 

We provide a couple of options, here: either we load trained models based on their hyperparameters as specified [above](#Hyperparameters), or we allow the user to select a file. We control which method to use by toggling the following variable:

```{python}
load_from_parameters = True
```

Note that when we load from parameters, this assumes that models are available, having been trained with those parameters. Otherwise, an error will be raised as the respectively-named file will not be found.

#### By noise and delay hyperparameters

```{python}
suffix_load = '_'.join([
    f"alpha-{alpha}",
    f"{disturbance_type_load}",
    f"noise-{feedback_noise_std_load}-{motor_noise_std_load}",
    f"delay-{feedback_delay_steps_load}",
])

if load_from_parameters and 'None' in suffix_load:
    raise ValueError("If loading from parameters, all parameters must be specified.")
```

#### Specify file to load by user selection

Maybe the user wants to browse and select a trained model file. In that case we can display a file chooser.

```{python}
if not load_from_parameters:
    fc = display_model_filechooser(MODELS_DIR, filter_pattern='1-1_*trained_models*')
```

The default filename is the one that sorts last. If the user does not select a file or if the following cell is run before they do, then the default file will be loaded.

### Load the specified model and associated parameters and metadata

```{python}
if not load_from_parameters:
    if fc.selected is None:
        models_filepath = f"{fc.default_path}/{fc.default_filename}"
    else:
        models_filepath = fc.selected
else: 
    models_filepath = str(find_unique_filepath(
        MODELS_DIR, f"1-1__{suffix_load}__{MODEL_FILE_LABEL}"
    ))
    if models_filepath is None:
        raise FileNotFoundError(f"No models found with file label: {suffix_load}")

trained_models_load: dict[float, eqx.Module] = load(
    MODELS_DIR / models_filepath, setup_models,
)
```

```{python}
hyperparameters_filepath = models_filepath.replace(
    f'{MODEL_FILE_LABEL}.eqx', 
    f'{HYPERPARAMS_FILE_LABEL}.json',
)
hyperparameters = load_from_json(hyperparameters_filepath)
extras_filepath = models_filepath.replace(f'{MODEL_FILE_LABEL}.eqx', 'extras.json')
extras = load_from_json(extras_filepath)

disturbance_train_stds_load = hyperparameters['disturbance_stds']
n_replicates = hyperparameters['n_replicates']

# We'll use this for creating figure subdirectories according to the training conditions
suffix_train = models_filepath.split('__')[1]
```

### Modify the system noise if needed

```{python}
trained_models_load = jt.map(
    partial(
        set_model_noise, 
        noise_stds=noise_stds,
        enable_noise=True,
    ),
    trained_models_load,
    is_leaf=is_module,
)
```

### Optionally exclude replicates that perform much worse than average

When plotting single-replicate examples, we want to show the best replicate. Also, when lumping together replicates and plotting distributions, we want to include as many replicates as possible to show how performance may vary, but also exclude replicates whose performance is much worse than the best replicate.

:::{note}
The logic here is that systems like the brain will have much more efficient learning systems, and that we are approximating their efficiency by taking advantage of variance between model initializations. 

In other words: we are interested in the kind of performance that is feasible with these kinds of networked controllers, more than the kind of performance that we should expect on average (or in the worst case) given the technical details of network initialization etc.
:::

```{python}
exclude_underperformers = True
```

```{python}
included_replicates = TrainStdDict({
    float(k): jnp.array(v) for k,v in extras['included_replicates'].items()
})
best_replicate = TrainStdDict({
    # JSON stores keys as strings, so cast them back to floats
    float(k): v for k,v in extras['best_replicate_by_loss'].items()
})

def take_replicate_or_best(tree: TrainStdDict, i_replicate=None, replicate_axis=1):
    if i_replicate is None:
        map_func = lambda tree: TrainStdDict({
            train_std: tree_take(subtree, best_replicate[train_std], replicate_axis)
            for train_std, subtree in tree.items()
        })
    else:
        map_func = lambda tree: tree_take_multi(tree, [i_replicate], [replicate_axis])
        
    return jt.map(map_func, tree, is_leaf=is_type(TrainStdDict))
```

Which replicates are included?

```{python}
eqx.tree_pprint(included_replicates, short_arrays=False)
```

Set the indices corresponding to the excluded replicates to NaN in each of the model arrays. This ensures that the shapes of the arrays remain consistent. NaN results will be ignored at plotting time. 

```{python}
if exclude_underperformers:
    trained_models = jt.map(
        lambda models, included: tree_set_scalar(models, jnp.nan, jnp.where(~included)[0]),
        trained_models_load, included_replicates,
        is_leaf=is_module,
    )
    n_replicates_included = jt.map(lambda x: jnp.sum(x).item(), included_replicates)
else:
    trained_models = trained_models_load
    n_replicates_included = dict.fromkeys(trained_models.keys(), n_replicates)
```

**TODO**: Make an annotation that indicates "$n/N$ replicates are included" or something.

### Define subsets of models/training conditions

Depending on how training goes, we might want to leave out some of the training conditions (i.e. training disturbance stds) from the analysis.

```{python}
disturbance_train_stds = disturbance_train_stds_load

trained_models = subdict(trained_models, disturbance_train_stds)
```

### Sort out the training and testing hyperparameters

The following will never vary between training and analysis.

```{python}
feedback_delay_steps = hyperparameters['feedback_delay_steps']
```

The following may change for the analysis, but we may want to refer to the training values.

```{python}
disturbance_type_train = hyperparameters['disturbance_type']
noise_stds_train = dict(
    feedback=hyperparameters['feedback_noise_std'],
    motor=hyperparameters['motor_noise_std'],
)
```

If we didn't alter the disturbance type or noise levels for the analysis, we can infer they'll be the same as the ones used during training.

```{python}
if disturbance_type is None:
    disturbance_type = disturbance_type_train

noise_stds = {
    k: v if v is not None else noise_stds_train[k]
    for k, v in noise_stds.items()
} 

any_system_noise = any(jt.leaves(noise_stds))
```

### Setup figure directories

The hierarchy of figure directories depends on the hyperparameters of the trained model. Now that we've sorted that out, we can set up the base subdirectory for figures generated in the analyses that follow in this notebook.

```{python}
suffix = f"{disturbance_type}_noise-{noise_stds['feedback']}-{noise_stds['motor']}_delay-{feedback_delay_steps}"

figs_dir = FIGS_BASE_DIR / f"{NB_ID}/train__{suffix_train}/{suffix}"

for d in (figs_dir,):
    d.mkdir(parents=True, exist_ok=True)
```

```{python}
savefig = get_savefig_func(figs_dir)
```

## Set up evaluation tasks with different disturbance amplitudes

We'll evaluate on a range of constant curl amplitudes, starting from 0 (no curl/control).

```{python}
if disturbance_type == 'curl':
    disturbance_amplitudes = [0.0, 0.5, 1.0, 2.0, 4.0]
    
    def disturbance(amplitude):
        return CurlField.with_params(amplitude=amplitude)    
        
elif disturbance_type == 'random':
    disturbance_amplitudes = [0.0, 0.05, 0.1, 0.2, 0.4]
    
    def disturbance(field_std):
        
        def orthogonal_field(trial_spec, *, key):
            init_pos = trial_spec.inits['mechanics.effector'].pos
            goal_pos = jnp.take(trial_spec.targets['mechanics.effector.pos'].value, -1, axis=-2)
            direction_vec = goal_pos - init_pos
            direction_vec = direction_vec / jnp.linalg.norm(direction_vec)
            return jnp.array([-direction_vec[1], direction_vec[0]])
            
        return FixedField.with_params(
            scale=field_std,
            field=orthogonal_field,  
        ) 
          
else:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")
```

```{python}
# TODO: import this from `setup_utils`
intervenor_label = "DisturbanceField"

# Insert the disturbance field component into each model
models = jt.map(
    lambda models: add_intervenors(
        models,
        lambda model: model.step.mechanics,
        # The first key is the model stage where to insert the disturbance field;
        # `None` means prior to the first stage.
        # The field parameters will come from the task, so use an amplitude 0.0 placeholder.
        {None: {intervenor_label: disturbance(0.0)}},
    ),
    TrainStdDict(trained_models),
    is_leaf=is_module,
)

# Generate tasks with different amplitudes of disturbance field
tasks, _ = tree_unzip(jt.map(
    lambda disturbance_amplitude: schedule_intervenor(
        task_base, trained_models[0],
        lambda model: model.step.mechanics,
        disturbance(disturbance_amplitude),
        label=intervenor_label,  
        default_active=False,
    ),
    PertAmpDict(zip(disturbance_amplitudes, disturbance_amplitudes)),
))

# Make smaller versions of the tasks for visualization.
tasks_small = jt.map(
    lambda task: eqx.tree_at(
        lambda task: (
            task.eval_grid_n,
            task.eval_n_directions,
            task.eval_reach_length,
        ),
        task, 
        (
            eval_grid_n_small, 
            eval_n_directions_small, 
            eval_reach_length_small,
        ),
    ),
    tasks,
    is_leaf=is_module,
)
``` 

And for convenience:

```{python}
trial_specs = task_base.validation_trials
trial_specs_small = jt.leaves(tasks_small, is_leaf=is_module)[0].validation_trials

def get_pos_endpoints(trial_specs):
    return jnp.stack([
        trial_specs.inits['mechanics.effector'].pos, 
        jnp.take(trial_specs.targets['mechanics.effector.pos'].value, -1, axis=-2),
    ], axis=0)

pos_endpoints = get_pos_endpoints(trial_specs)
pos_endpoints_small = get_pos_endpoints(trial_specs_small)

# Once the positions are center-subtracted and aligned to reach direction,
# all the endpoints will be the same.
pos_endpoints_aligned = jnp.array([
    [0., 0.], [eval_reach_length, 0.]
])
pos_endpoints_small_aligned = jnp.array([
    [0., 0.], [eval_reach_length_small, 0.]
])
```

### Number of evaluations per model and condition

We'll evaluate each condition (reach direction) several times, to see how performance varies with noise.

```{python}
n_evals = 5
n_evals_small = 5

if not any_system_noise:
    n_evals = n_evals_small = 1
```

## Setup colors for plots

Now that we know how many training and evaluation conditions we'll be working with, we can define the color scales for plots once and for all.

```{python}
trials_colors, trials_colors_dark = get_colors_dicts(
    range(n_evals), COLORSCALES['trials'],
)

# by training condition
disturbance_train_stds_colors, disturbance_train_stds_colors_dark = get_colors_dicts(
    disturbance_train_stds, COLORSCALES['disturbance_train_stds'],
)

# by evaluation condition
disturbance_amplitudes_colors, disturbance_amplitudes_colors_dark = get_colors_dicts(
    disturbance_amplitudes, COLORSCALES['disturbance_amplitudes'], 
)
```

## Evaluate the trained models on each evaluation task

In particular, for each trained ensemble of models (i.e. each training condition), evaluate each model in the ensemble on `n_evals` repetitions of each of the reach conditions, for each of the task variants. 

First, define the full evaluation as a function so that we can estimate the memory needed for the result. 

```{python}
def evaluate_all_states(tasks, n_evals):
    return jt.map( # Map over task variants
        lambda task: jt.map(  # Map over training conditions (`models` entries)
            lambda models: vmap_eval_ensemble(models, task, n_evals, key_eval),
            models,
            is_leaf=is_module,
        ),
        tasks,
        is_leaf=is_module,
    )
```

```{python}
all_states_bytes = (
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, tasks, n_evals)),
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, tasks_small, n_evals_small)),
)

print(f"{sum(all_states_bytes) / 1e9:.2f} GB of memory estimated to store all states.")
```

Now actually evaluate the states:

```{python}
all_states = evaluate_all_states(tasks, n_evals)
all_states_small = evaluate_all_states(tasks_small, n_evals_small)
```

The objects `all_states` and `all_states_small` are dicts whose keys are the `disturbance_amplitudes`, and whose values are dicts whose keys are the `disturbance_train_stds` used in training, and whose values in turn are the state PyTrees.

Each state array has the following batch dimensions: `(n_evals, n_replicates, n_conditions, n_steps)`. For a single center-out set in `all_states_small`, `n_conditions = eval_n_directions`. But since `eval_grid_n != 1` for the full tasks evaluated in `all_states`, then there will be `eval_grid_n ** 2` center-out sets and the third dimension will have size `eval_n_directions * eval_grid_n ** 2`.

```{python}
pp(all_states)
```

TODO: Instead of a two-layer `jt.map`, we could instead define a function (to schedule the intervenor given a `disturbance_amplitude` and then `jt.map` the vmapped `eval_ensemble`) and then vmap it over `disturbance_amplitudes` to get a single-level dict with a bit less overhead.

### Project positions, velocities, and forces into reach direction

In other words, change from x/y components to parallel/orthogonal components, relative to a straight reach.

```{python}
from rnns_learn_robust_motor_policies.state_utils import project_onto_direction
from typing import Any

Responses = namedtuple('Responses', ('pos', 'vel', 'force'))

# class Responses(eqx.Module):
#     pos: Any
#     vel: Any
#     force: Any

aligned_var_labels = Responses('Position', 'Velocity', 'Control force')
where_vars_to_align = lambda states, pos_endpoints: Responses(
    # Positions with respect to the origin
    states.mechanics.effector.pos - pos_endpoints[0][..., None, :],
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

```{python}
def get_aligned_vars(all_states, where, endpoints): 
    directions = endpoints[1] - endpoints[0]
    
    return jt.map(
        lambda states: jt.map(
            lambda var: project_onto_direction(var, directions),
            where(states, endpoints),
        ),
        all_states,
        is_leaf=is_module,
    )
    
aligned_vars = get_aligned_vars(all_states, where_vars_to_align, pos_endpoints)
aligned_vars_small = get_aligned_vars(all_states_small, where_vars_to_align, pos_endpoints_small)
```

## Plot example center-out sets

i.e. show trials for multiple reach directions, to show performance across conditions at a glance, for a single model

Note that we use `states_small` here, and only plot a single center-out set of 7 reach directions.

```{python}
fig_subdir = "center_out_sets"
```

```{python}
if not any_system_noise:
    var_labels = ('Position', 'Velocity', 'Control force')
    where_plot = lambda states: (
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
        states.efferent.output,
    )
else:
    var_labels = ('Position', 'Velocity')
    # Forces are very messy when there's noise,
    # and we'll visualize the aligned forces anyway
    where_plot = lambda states: (
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
    )
```

```{python}
plot_trajectories = lambda states, *args, **kwargs: fbp.trajectories_2D(
    where_plot(states),
    var_labels=var_labels,
    axes_labels=('x', 'y'),
    colorscale=COLORSCALES['reach_condition'],
    legend_title='Reach direction',
    # scatter_kws=dict(line_width=0.5),
    layout_kws=dict(
        width=100 + len(var_labels) * 300,
        height=400,
        legend_tracegroupgap=1,
    ),
    *args, 
    **kwargs,
)
```

### All trials for a replicate

```{python}
fig_subdir = "center_out_sets/all_evals_single_replicate"
```

```{python}
# If None, plot the replicate with the lowest total training loss
i_replicate = None
```

```{python}
if not any_system_noise:
    print("Skipping center-out sets that compare different evaluations, for this zero-noise condition")
else:
    plot_states = take_replicate_or_best(all_states_small, i_replicate)
    
    figs = jt.map(
        partial(
            plot_trajectories, 
            curves_mode='lines', 
            colorscale_axis=1, 
            mean_trajectory_line_width=2.5,
            darken_mean=MEAN_LIGHTEN_FACTOR,
            scatter_kws=dict(line_width=0.5),
        ),
        plot_states,
        is_leaf=is_module,
    )
    
    for disturbance_amplitude in figs:
        for disturbance_train_std, fig in figs[disturbance_amplitude].items():
            if i_replicate is None:
                i_rep = best_replicate[disturbance_train_std]
            else:
                i_rep = i_replicate
            
            # Add an annotation that provides some context if the figure is shared.
            # This will be removed before publication
            add_context_annotation(
                fig,
                train_condition_strs=[
                    f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{disturbance_train_std ** 2:.2f})"
                ],
                perturbations={
                    f"{disturbance_type}": (disturbance_amplitude, None, None)
                },
                n=n_evals_small,
                i_replicate=i_rep,
            )
            # Plot the reach endpoints on the effector position subplot
            add_endpoint_traces(
                fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
            )
            # Save to disk.
            # savefig(
            #     fig, 
            #     join([
            #         f"{disturbance_type}-field-{disturbance_amplitude}",
            #         f"{disturbance_type_train}-std-{disturbance_train_std}",
            #         f"replicate-{i_rep}",
            #     ]),
            #     subdir=fig_subdir,
            # )
    
    # Only display figures for the low-high train and eval conditions.        
    for disturbance_amplitude in lohi(disturbance_amplitudes):
        for disturbance_train_std in lohi(disturbance_train_stds):
            figs[disturbance_amplitude][disturbance_train_std].show()
```

### A single trial set, for a single replicate

```{python}
fig_subdir = "center_out_sets/single_eval_single_replicate"
```

```{python}
i_trial = 0

plot_states = take_replicate_or_best(all_states_small, i_replicate)
plot_states = tree_take(plot_states, i_trial, 0)

figs = jt.map(
    partial(
        plot_trajectories, 
        mode='markers+lines', 
        ms=3,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_train_std, fig in figs[disturbance_amplitude].items():
        if i_replicate is None:
            i_rep = best_replicate[disturbance_train_std]
        else:
            i_rep = i_replicate
            
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{disturbance_train_std ** 2:.2f})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            i_trial=i_trial,
            i_replicate=i_rep,
        )
        add_endpoint_traces(
            fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
        )
        filename = join([
                f"{disturbance_type}-amp-{disturbance_amplitude}",
                f"{disturbance_type_train}-train-std-{disturbance_train_std}",
                f"rep-{i_rep}",
                f"eval-{i_trial}",
        ])
        savefig(fig, filename, subdir=fig_subdir)
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    for disturbance_train_std in lohi(disturbance_train_stds):
        figs[disturbance_amplitude][disturbance_train_std].show()
```

### A single trial, for all replicates

```{python}
fig_subdir = "center_out_sets/single_eval_all_replicates"
```

```{python}
plot_states = tree_take_multi(all_states_small, [i_trial], [0])

figs = jt.map(
    partial(
        plot_trajectories, 
        curves_mode='lines', 
        colorscale_axis=1, 
        mean_trajectory_line_width=2.5,
        darken_mean=MEAN_LIGHTEN_FACTOR,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_train_std, fig in figs[disturbance_amplitude].items():
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{disturbance_train_std ** 2:.2f})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            i_trial=i_trial,
            n=n_replicates_included[disturbance_train_std],
        )
        add_endpoint_traces(
            fig, pos_endpoints_small, visible=[False, True], xaxis='x1', yaxis='y1', colorscale='phase'
        )
```
```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_train_std, fig in figs[disturbance_amplitude].items():        
        savefig(
            fig, 
            join([
                f"{disturbance_type}-field-{disturbance_amplitude}",
                f"{disturbance_type_load}-train-std-{disturbance_train_std}",
                f"eval-{i_trial}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    for disturbance_train_std in lohi(disturbance_train_stds):
        figs[disturbance_amplitude][disturbance_train_std].show()
```

## Plot aligned trajectories

i.e. for a single reach direction, compare multiple trials/replicates different training conditions; visualize how training on different disturbance strengths affects response.

```{python}
fig_subdir = "aligned_to_reach_condition"
```

```{python}
n_curves_max = 20

plot_condition_trajectories = partial(
    fbp.trajectories_2D,
    var_labels=aligned_var_labels,
    axes_labels=('x', 'y'),
    # mode='std',
    mean_trajectory_line_width=3,
    n_curves_max=n_curves_max,
    darken_mean=MEAN_LIGHTEN_FACTOR,
    layout_kws=dict(
        width=900,
        height=400,
        legend_tracegroupgap=1,
        margin_t=75,
    ),
    scatter_kws=dict(
        line_width=1, 
        opacity=0.6,
    ),
)
```

### All trials and replicates for a given train-test condition, indexing by trial

```{python}
fig_subdir = "aligned_to_reach_condition/per_train_test_pair"
```

```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        legend_title="Trial",
        colorscale=COLORSCALES['trials'],
        colorscale_axis=0, 
        curves_mode='lines', 
    ),
    aligned_vars_small,
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_train_std, fig in figs[disturbance_amplitude].items():
        n_dist = n_replicates_included[disturbance_train_std] * n_conditions_small
        
        add_endpoint_traces(fig, pos_endpoints_small_aligned, xaxis='x1', yaxis='y1')
        
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{disturbance_train_std ** 2:.2f})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            n=min(n_dist, n_curves_max),  # color/legend is by trial, so this is N per trial
            # i_condition=i_condition,
        )
        
        # savefig(
        #     fig, 
        #     join([
        #         f"{disturbance_type}-field-{disturbance_amplitude}",
        #         f"{disturbance_type_load}-train-std-{disturbance_train_std}",
        #     ]),
        #     subdir=fig_subdir,
        # )
```

Just show a single example, so we'll know if something is really amiss. But these plots are not very interesting except to show the overall variation for a given noise condition.

```{python}
figleaves(figs)[-1].show()
# for disturbance_amplitude in lohi(disturbance_amplitudes):
#     for disturbance_train_std in lohi(disturbance_train_stds):
#         figs[disturbance_amplitude][disturbance_train_std].show()
```

### Compare disturbance amplitudes, for each trained disturbance std

```{python}
fig_subdir = "aligned_to_reach_condition/compare_test_conditions"
```

```{python}
# Stack the states for different disturbance amplitudes into single arrays
plot_vars_stacked = tree_stack(aligned_vars_small.values())

figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['disturbance_amplitudes'],
        colorscale_axis=0,
        legend_title="Field<br>amplitude",
        legend_labels=disturbance_amplitudes,
        curves_mode='lines',
    ),
    plot_vars_stacked,
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_train_std, fig in tqdm(figs.items()):
    n_trials = n_evals_small * n_conditions_small * n_replicates_included[disturbance_train_std]
    
    add_context_annotation(
        fig,
        train_condition_strs=[
            f"{disturbance_type_train} fields with amplitude ~ \U0001d4dd(0,{disturbance_train_std ** 2:.2f})"
        ],
        n=min(n_dist, n_curves_max),
    )
    
    add_endpoint_traces(fig, pos_endpoints_small_aligned, xaxis='x1', yaxis='y1')
    
    savefig(
        fig, 
        join([
            f"{disturbance_type_load}-train-std-{disturbance_train_std}",
        ]),
        subdir=fig_subdir,
    )    
```

```{python}
for disturbance_train_std in lohi(disturbance_train_stds):
    figs[disturbance_train_std].show()
```

### Compare the trained disturbance stds, for each test disturbance amplitude

```{python}
fig_subdir = "aligned_to_reach_condition/compare_train_conditions"
```

```{python}
# Only plot a subset of the training conditions
# disturbance_train_stds_plot = [
#     disturbance_train_stds[0],
#     disturbance_train_stds[len(disturbance_train_stds) // 2],
#     disturbance_train_stds[-1],
# ]
disturbance_train_stds_plot = disturbance_train_stds
stride = 1

# Stack the states for different training disturbance stds into single arrays
# (also taking a subset of the training conditions with `subdict`)
plot_vars_stacked = {
    # concatenate along the replicate axis, which has variable length
    disturbance_amplitude: tree_stack(subdict(vars_, disturbance_train_stds_plot).values())
    for disturbance_amplitude, vars_ in aligned_vars_small.items()
}
```
```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['disturbance_train_stds'],
        colorscale_axis=0,
        stride=stride,
        legend_title="Train<br>field std.",
        legend_labels=disturbance_train_stds_plot,
        curves_mode='lines',
        var_endpoint_ms=0,
        scatter_kws=dict(line_width=0.5, opacity=0.3),
        # ref_endpoints=(pos_endpoints, None),
    ),
    plot_vars_stacked,
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_amplitude, fig in tqdm(figs.items()):
    add_context_annotation(
        fig,
        perturbations={
            f"{disturbance_type}": (disturbance_amplitude, None, None)
        },
        train_condition_strs=[
            f"{disturbance_type_train} fields",
        ],        
        # TODO: The number of replicates (`n_replicates_included`) may vary with the disturbance train std!
        # n=min(n_evals_small * n_replicates, n_curves_max),
    )
    
    add_endpoint_traces(fig, pos_endpoints_small_aligned, xaxis='x1', yaxis='y1')

    savefig(
        fig, 
        join([
            f"{disturbance_type}-field-{disturbance_amplitude}",
        ]),
        subdir=fig_subdir,
    )   
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    figs[disturbance_amplitude].show()
```

## Compare velocity profiles

```{python}
fig_subdir = "velocity_profiles"
```

Average over trials, directions, and replicates to get an average velocity profile + error bands for each training condition. 

Note that it only makes sense to compare across reach directions if we consider the velocity profiles along the respective directions. For example, the y-profile in one direction should be comparable to the x-profile in an orthogonal direction.

There are two ways we might get profiles for comparison.

1. Calculate speed as the amplitude of the velocity vector. This is simpler.
2. Project the velocity profiles onto the reach direction.

These are not identical, and the second method is more flexible because we still retain the components and their signs. 

### Speed profiles

:::{note}
I am omitting speed profiles from analysis since they are not very interpretable.
:::

```{python}
fig_subdir = "velocity_profiles/speed"
```

```{python}
def get_speed(responses):
    return jnp.linalg.norm(responses.vel, axis=-1)

# all_speeds = jt.map(get_speed, aligned_vars, is_leaf=is_module)

# n_dist = np.prod(jt.leaves(all_speeds)[0].shape[:-1])

# figs = {
#     disturbance_amplitude: fbp.profiles(
#         speeds,
#         mode='std', 
#         varname="Speed",
#     )
#     for disturbance_amplitude, speeds in all_speeds.items()
# }

# for disturbance_amplitude in figs:
#     add_context_annotation(
#         figs[disturbance_amplitude],
#         perturbations={
#             f"{disturbance_type}": (disturbance_amplitude, None, None)
#         },
#         n=n_dist,
#         y=1.1,
#     )

# for disturbance_amplitude in lohi(disturbance_amplitudes):
#     figs[disturbance_amplitude].show()
```

### Forward and lateral velocity profiles

```{python}
fig_subdir = "velocity_profiles/forward_lateral"
```

**TODO: Plot on shared time axis**

```{python}
aligned_vel = jt.map(
    lambda responses: responses.vel,
    aligned_vars,
    is_leaf=is_type(Responses),
)
```

```{python}
n_dist = np.prod(jt.leaves(aligned_vel)[0].shape[:-2])

figs = {
    disturbance_amplitude: {
        label: fbp.profiles(
            tree_take(aligned_vel, i, -1)[disturbance_amplitude],
            varname=f"{label} velocity",
            legend_title="Train<br>field std.",
            mode='std', # or 'curves'
            n_std_plot=1,
            colors=list(disturbance_train_stds_colors_dark.values()),
            # stride_curves=500,
            # curves_kws=dict(opacity=0.7),
            layout_kws=dict(
                width=600,
                height=400,
                legend_tracegroupgap=1,
            ),
        )
        for i, label in enumerate(("Forward", "Lateral"))
    }
    for disturbance_amplitude in disturbance_amplitudes
}

for disturbance_amplitude in tqdm(disturbance_amplitudes):
    for i, label in enumerate(("Forward", "Lateral")):
        fig = figs[disturbance_amplitude][label]
        fig.add_hline(y=0, line_color="grey", layer="below")
        add_context_annotation(
            fig,
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            train_condition_strs=[
                f"{disturbance_type_train} fields",
            ],  
            n=n_dist,
            y=0.95,
        )
        filename = f"{disturbance_type}-field-{disturbance_amplitude}_{label.lower()}-vels"
        savefig(fig, filename, subdir=fig_subdir)
        
for disturbance_amplitude in lohi(disturbance_amplitudes):
    figs[disturbance_amplitude]["Forward"].show()
    figs[disturbance_amplitude]["Lateral"].show()

```

## Summary comparison of performance measures 

```{python}
fig_subdir = "performance_measures"
```

### Calculate all measures

Maximum speed

```{python}
def get_max_speed(responses):
    return jnp.max(get_speed(responses), axis=-1)
```

Maximum lateral (left and right relative to reach direction) and forward velocities

```{python}
def get_max_forward_vel(aligned_vars, mul=1.):
    return jnp.max(mul * aligned_vars.vel[..., 0], axis=-1)
```

```{python}
def get_max_lateral_vel(aligned_vars, mul=1.):
    return jnp.max(mul * aligned_vars.vel[..., 1], axis=-1)
```

Time to peak forward velocity

```{python}
def get_argmax_forward_vel(aligned_vars, mul=1.):
    return jnp.argmax(mul * aligned_vars.vel[..., 0], axis=-1)
```

Maximum and summed lateral distance of the effector from a straight trajectory

```{python}
# lambda states: get_lateral_distance(states.mechanics.effector.pos, pos_endpoints)
```

```{python}
def get_max_lateral_distance(aligned_vars, eval_reach_length=None):
    distances = jnp.max(aligned_vars.pos[..., 1], axis=-1)
    if eval_reach_length is not None:
        distances = 100 * distances / eval_reach_length
    return distances

def get_sum_lateral_distance(aligned_vars):
    return jnp.sum(jnp.abs(aligned_vars.pos[..., 1]), axis=-1)
```

(Note that we use `jnp.abs`, so perturbations that cause oscillation about the straight reach line will lead to accumulation of summed deviation, not cancellation.)

Position and velocity errors relative to the goal, averaged over the last `last_n_steps` time steps

```{python}
last_n_steps = 10

def end_position_error(aligned_vars, eval_reach_length=1, last_n_steps=1):
    final_pos = aligned_vars.pos[..., -last_n_steps:, :]
    # Since the data is aligned, the goal is always at the same position
    goal_pos = jnp.array([eval_reach_length, 0])
    error = jnp.mean(jnp.linalg.norm(final_pos - goal_pos, axis=-1), axis=-1)
    if eval_reach_length is not None:
        error = 100 * error / eval_reach_length
    return error
    
# TODO: This could be normalized by the max. velocity
def end_velocity_error(aligned_vars, last_n_steps=1):
    final_vel = aligned_vars.vel[..., -last_n_steps:, :]
    goal_vel = 0
    return jnp.mean(jnp.linalg.norm(final_vel - goal_vel, axis=-1), axis=-1)
```

Max and sum of the net control forces

```{python}
def get_net_control_force(aligned_vars):
    return jnp.linalg.norm(aligned_vars.force, axis=-1)

def get_max_net_control_force(aligned_vars):
    return jnp.max(get_net_control_force(aligned_vars), axis=-1)
    
def get_sum_net_control_force(aligned_vars):
    return jnp.sum(get_net_control_force(aligned_vars), axis=-1)
    
def get_max_parallel_force(aligned_vars, mul=1., timesteps=slice(None)):
    return jnp.max(mul * aligned_vars.force[..., timesteps, 0], axis=-1)
    
def get_sum_parallel_force(aligned_vars, timesteps=slice(None)):
    return jnp.sum(jnp.abs(aligned_vars.force[..., timesteps, 0]), axis=-1)
    
def get_max_lateral_force(aligned_vars, mul=1., timesteps=slice(None)):
    return jnp.max(mul * aligned_vars.force[..., timesteps, 1], axis=-1)
    
def get_sum_lateral_force(aligned_vars, timesteps=slice(None)):
    return jnp.sum(jnp.abs(aligned_vars.force[..., timesteps, 1]), axis=-1)
```

Collect measures into PyTree

```{python}
all_measure_funcs, measure_handles = tree_unzip(OrderedDict({
    # "Max speed": (get_max_speed, "speed-max"),
    "Max forward velocity": (get_max_forward_vel, "vel-forward-max"),
    "Max lateral velocity (left)": (
        partial(get_max_lateral_vel, mul=1.), 
        "vel-lateral-left-max",
    ),
    "Max lateral velocity (right)": (
        partial(get_max_lateral_vel, mul=-1.), 
        "vel-lateral-right-max",
    ),
    "Max lateral distance (% reach length)": (
        partial(get_max_lateral_distance, eval_reach_length=eval_reach_length), 
        "dist-lateral-max",
    ),
    "Sum lateral distance": (get_sum_lateral_distance, "dist-lateral-sum"),
    "End position error (% reach length)": (
        partial(end_position_error, last_n_steps=last_n_steps, eval_reach_length=eval_reach_length), 
        "error-end-pos",
    ),
    "End velocity error": (
        partial(end_velocity_error, last_n_steps=last_n_steps), 
        "error-end-vel",
    ),
    "Max net control force": (get_max_net_control_force, "force-net-max"),
    "Sum net control force": (get_sum_net_control_force, "force-net-sum"),
    "Max forward force": (
        partial(get_max_parallel_force, mul=1.), 
        "force-forward-max",
    ),
    "Sum parallel force": (
        partial(get_sum_parallel_force), 
        "force-forward-sum",
    ),
    "Max lateral (counter pert.) force": (
        partial(get_max_lateral_force, mul=-1.), 
        "force-counterfield-lateral-max",
    ),
    "Sum lateral force": (
        partial(get_sum_lateral_force), 
        "force-lateral-sum",
    ),
}))

box_measure_funcs, box_measure_handles = tree_unzip(OrderedDict({
    "Time to peak forward velocity": (get_argmax_forward_vel, "vel-forward-max-timestep"),
}))
```

```{python}
from jaxtyping import PyTree
from collections.abc import Callable
from feedbax.bodies import SimpleFeedbackState

def get_all_measures(measure_funcs: PyTree[Callable], all_aligned_vars: PyTree[SimpleFeedbackState]):
    return jt.map(
        lambda func: jt.map(
            lambda aligned_vars: func(aligned_vars),
            all_aligned_vars,
            is_leaf=is_type(Responses),
        ),
        measure_funcs,
    )
    
all_measures = get_all_measures(all_measure_funcs, aligned_vars)

box_measures = get_all_measures(box_measure_funcs, aligned_vars)

# TODO: fix the `pos_endpoints` hardcoding, above
# all_measures_small = get_all_measures(all_measure_funcs, all_states_small)
```

### For each evaluation condition, plot measure distributions by training condition

:::{note}
Skip this, since better summary plots immediately follow
:::

```{python}
fig_subdir = "performance_measures/compare_train_conditions"
```

One plot per evaluation condition (disturbance amplitude); one violin per training condition (disturbance std).

Distributions are aggregated over all replicates and trials. 

```{python}
def get_violins_across_train_conditions(data, measure_name, disturbance_amplitude, annotation_kws=None):
    n_dist = np.prod(jt.leaves(data)[0].shape)
    
    fig = go.Figure(
        data=[
            go.Violin(
                y=data[disturbance_train_std].flatten(),
                name=disturbance_train_std,
                legendgroup=disturbance_train_std,
                box_visible=False,
                meanline_visible=True,
                line_color=disturbance_train_stds_colors_dark[disturbance_train_std],
                showlegend=False,
                opacity=1,
                spanmode='hard',
            )
            for i, disturbance_train_std in enumerate(data)
        ],
        
        layout=dict(
            # title=(f"Response to amplitude {disturbance_amplitude} field <br>N = {n_dist}"),
            width=900,
            height=400,
            xaxis_title="Train field std.",
            yaxis_title=measure_name,
            xaxis_range=[-0.5, len(data) - 0.5],
            xaxis_tickvals=list(data.keys()),
            yaxis_range=[0, None],
            # violinmode='overlay',
            violingap=0,
            margin_t=75,
            legend_tracegroupgap=1,
        )
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(
        fig, 
        perturbations={
            f"{disturbance_type} field": (disturbance_amplitude, None, None)
        },
        train_condition_strs=[
            f"{disturbance_type_train} fields",
        ],
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig


def get_one_measure_plot_per_eval_condition(plot_func, all_measures, **kwargs):
    return {
        measure_name: {
            disturbance_amplitude: plot_func(
                measure[disturbance_amplitude], measure_name, disturbance_amplitude, **kwargs
            )
            for disturbance_amplitude in measure
        }
        for measure_name, measure in all_measures.items()
    }
    

# figs = get_one_measure_plot_per_eval_condition(
#     get_violins_across_train_conditions, 
#     all_measures,
# )
```

```{python}
# for measure_name, measure_handle in tqdm(measure_handles.items()):
#     for disturbance_amplitude, fig in figs[measure_name].items():
#         savefig(
#             fig, 
#             join([
#                 f"{measure_handle}",
#                 f"{disturbance_type}-field-{disturbance_amplitude}",
#             ]),
#             subdir=fig_subdir,
#         )
```

```{python}
# for measure_name in all_measures:
#     figs[measure_name][disturbance_amplitudes[0]].show()
#     figs[measure_name][disturbance_amplitudes[-1]].show()
```

### Plot measure distributions by training condition, across evaluation conditions

```{python}
fig_subdir = "performance_measures/compare_train_conditions"
```

One plot per evaluation condition (disturbance amplitude); one violin per training condition (disturbance std).

Distributions are aggregated over all replicates and trials. 

```{python}
def get_violins_across_train_conditions(measure_data, measure_name, annotation_kws=None):
    example_traindict = jt.leaves(measure_data, is_leaf=is_type(TrainStdDict))[0]
    n_train_std = len(example_traindict)
    n_dist = np.prod(jt.leaves(measure_data)[0].shape)

    fig = go.Figure(
        layout=dict(
            # title=(f"Response to amplitude {disturbance_amplitude} field <br>N = {n_dist}"),
            width=900,
            height=400,
            legend_title="Field<br>amplitude",
            yaxis_title=measure_name,
            xaxis_title="Train field std.",
            xaxis_type='category',
            xaxis_range=[-0.75, n_train_std - 0.25],
            xaxis_tickvals=np.arange(n_train_std),
            xaxis_ticktext=[f'{std:.1f}' for std in example_traindict.keys()],
            xaxis_tickformat='.1',
            yaxis_range=[0, None],
            violinmode='overlay',
            violingap=0,
            violingroupgap=0,
            margin_t=75,
        )
    )
    
    for i, disturbance_amplitude in enumerate(measure_data):
        measure_data_i = measure_data[disturbance_amplitude]
        
        xs = jnp.stack([
            jnp.full_like(data, disturbance_train_std)
            for disturbance_train_std, data in measure_data_i.items()
        ]).flatten()
        
        fig.add_trace(
            go.Violin(
                x=xs,
                y=jnp.stack(tuple(measure_data_i.values())).flatten(),
                name=disturbance_amplitude,
                legendgroup=disturbance_amplitude,
                box_visible=False,
                meanline_visible=True,
                line_color=disturbance_amplitudes_colors_dark[disturbance_amplitude],
                # showlegend=False,
                opacity=1,
                spanmode='hard',
            )
        )
        
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(
        fig, 
        perturbations={
            f"{disturbance_type} field": (None, None, None)
        },
        train_condition_strs=[
            f"{disturbance_type_train} fields",
        ],
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig


def get_violins_per_measure(plot_func, all_measures, **kwargs):
    return {
        measure_name: plot_func(
            measure, measure_name, **kwargs
        )
        for measure_name, measure in all_measures.items()
    }
    

figs = get_violins_per_measure(
    get_violins_across_train_conditions, 
    all_measures,
)
```

```{python}
for measure_name, measure_handle in tqdm(measure_handles.items()):
    savefig(
        figs[measure_name], 
        join([
            f"{measure_handle}",
        ]),
        subdir=fig_subdir,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name].show()
```

### Repeat for just a single reach condition

:::{note}
Omitting this from the output since it's sufficient to see the variation in (e.g.) the velocity profiles to be convinced that there is not huge variation between reach directions/conditions.
:::

```{python}
fig_subdir = "performance_measures/compare_train_conditions/single_reach_condition"
```

The distributions should be similar, since the model+task is isotropic.

TODO: could use `all_states_small` and increase the number of trials...

```{python}
# i_condition = 0
# all_measures_one_dirxn = tree_take(all_measures, i_condition, -1)

# figs0 = get_one_measure_plot_per_eval_condition(
#     get_violins_across_train_conditions, 
#     all_measures_one_dirxn,
#     annotation_kws=dict(i_condition=i_condition),
# )

# for measure_name, measure_handle in measure_handles.items():
#     for disturbance_amplitude, fig in figs0[measure_name].items():
#         savefig(
#             fig, 
#             join([
#                 f"{measure_handle}",
#                 f"{disturbance_type}-field-{disturbance_amplitude}",
#                 f"condition-{i_condition}",
#             ]),
#             subdir=fig_subdir,
#         )

# for measure_name in all_measures:
#     figs0[measure_name][disturbance_amplitudes[-1]].show()
```

### Repeat for just the zero vs. highest-std training condition

```{python}
fig_subdir = "performance_measures/compare_train_conditions/lowhigh_only"
```

```{python}
def subset_dict_tree_level(tree, subset_keys, dict_type=dict):
    return jt.map(
        lambda measure: subdict(measure, subset_keys),
        tree,
        is_leaf=is_type(dict_type),
    )
    
subset_by_train_stds = partial(subset_dict_tree_level, dict_type=TrainStdDict)

all_measures_subset_disturbance_train_stds = subset_by_train_stds(
    all_measures,
    lohi(disturbance_train_stds),
)

box_measures_subset_disturbance_train_stds = subset_by_train_stds(
    box_measures,
    lohi(disturbance_train_stds),
)
```

:::{note}
Omitting these figures since the low-high summary figures below will include this comparison.
:::

```{python}
# figs = get_one_measure_plot_per_eval_condition(
#     get_violins_across_train_conditions, 
#     all_measures_subset_disturbance_train_stds ,
# )

# figs = jt.map(
#     lambda fig: fig.update_layout(
#         height=400,
#         width=500,
#     ),
#     figs,
#     is_leaf=is_module,
# )

# for measure_name, measure_handle in measure_handles.items():
#     for disturbance_amplitude, fig in figs[measure_name].items():
#         savefig(
#             fig, 
#             join([
#                 f"{measure_handle}",
#                 f"{disturbance_type}-field-{disturbance_amplitude}",
#                 "lowest-vs-highest",
#             ]),
#             subdir=fig_subdir,
#         )

# for measure_name in all_measures:
#     figs[measure_name][disturbance_amplitudes[-1]].show()
```

### Comparison of zero vs. high train disturbance std, for different replicates

Compare the smallest (zero) and largest training disturbance stds. For the plots above, this would mean keeping just the leftmost and the rightmost violins. However, those plots were aggregated over replicates and trials. Now we would like to examine the variance across model replicates. Thus, for each measure, we will generate one plot for each evaluation condition (disturbance amplitude) as before, but now each containing `n_replicates` *split* violins, where the left half corresponds to the zero training disturbance std, and the right half to the largest training disturbance std.

```{python}
fig_subdir = "performance_measures/compare_replicates_lowhigh_train_conditions"
```

```{python}
labels = lohi(disturbance_train_stds)
colors = lohi(disturbance_train_stds_colors_dark)

def get_measure_replicate_comparisons(data, measure_name, disturbance_amplitude, annotation_kws=None):
    n_dist = np.prod(jt.leaves(data)[0].shape)
    
    data = jnp.stack(list(data.values()))
    
    # TODO: Exclude replicates which were excluded from analysis for either training condition
    # replicates_any_nan = jnp.any(jnp.isnan(data), axis=-2)
    # data = jnp.take(data, jnp.where(replicates_any_nan)[0], axis=-2)

    fig = go.Figure()
    # x axis: replicates
    for i in range(data.shape[-2]):        
        # split violin: smallest vs. largest train disturbance std
        for j, train_std in enumerate(labels):
        
            data_j = data[j, :, i].flatten()
            
            fig.add_trace(
                go.Violin(
                    x=np.full_like(data_j, i),
                    y=data_j.flatten(),
                    name=train_std,
                    legendgroup=train_std,
                    box_visible=False,
                    meanline_visible=True,
                    line_color=colors[train_std],
                    side='positive' if j == 1 else 'negative',
                    showlegend=(i == 0),
                    spanmode='hard',
                )
            )
    fig.update_layout(
        xaxis_title="Model replicate",
        yaxis_title=measure_name,
        xaxis_range=[-0.5, data.shape[-2] - 0.5],
        xaxis_tickvals=list(range(data.shape[-2])),
        yaxis_range=[0, None],
        violinmode='overlay',
        violingap=0,
        # title=(f"Response to amplitude {disturbance_amplitude} {disturbance_type} field <br>N = {n_dist}"),
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(
        fig, 
        perturbations={
            f"{disturbance_type}": (disturbance_amplitude, None, None)
        },
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig
    
figs = get_one_measure_plot_per_eval_condition(
    get_measure_replicate_comparisons, 
    all_measures_subset_disturbance_train_stds,
)
```

```{python}
for measure_name, measure_handle in tqdm(measure_handles.items()):
    for disturbance_amplitude, fig in figs[measure_name].items():
        savefig(
            fig, 
            join([
                f"{measure_handle}",
                f"{disturbance_type}-field-{disturbance_amplitude}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for measure_name in all_measures:
    figs[measure_name][disturbance_amplitudes[-1]].show()
```

### Summary comparison of no-disturbance vs. high-disturbance evaluation, for no-disturbance vs. high-disturbance training

```{python}
fig_subdir = "performance_measures/lowhigh_summaries"
```

```{python}
all_measures_lohi = {
    measure_name: subdict(measure, lohi(disturbance_amplitudes))
    for measure_name, measure in all_measures_subset_disturbance_train_stds.items()
}

box_measures_lohi = {
    measure_name: subdict(measure, lohi(disturbance_amplitudes))
    for measure_name, measure in box_measures_subset_disturbance_train_stds.items()
}
```

```{python}
def get_lohi_summary_violins(measure_data, measure_name, annotation_kws=None):
    n_dist = np.prod(jt.leaves(measure_data)[0].shape)

    fig = go.Figure(
        data=jt.leaves([
            [
                go.Violin(
                    x=jnp.full((n_dist,), disturbance_train_std),
                    y=data.flatten(),
                    name=disturbance_amplitude,
                    legendgroup=disturbance_amplitude,
                    box_visible=False,
                    meanline_visible=True,
                    line_color=disturbance_amplitudes_colors_dark[disturbance_amplitude],
                    showlegend=(j == 0),
                    spanmode='hard',
                )
                for j, (disturbance_train_std, data) 
                in enumerate(measure_data[disturbance_amplitude].items())
            ]
            for i, disturbance_amplitude in enumerate(measure_data)
        ]),
        layout=dict(
            width=500,
            height=400,
            xaxis_title="Train disturbance std.",
            yaxis_title=measure_name,
            # xaxis_range=[-0.5, len(measure_data) - 0.5],
            # xaxis_ticktext=list(measure_data.keys()),
            yaxis_range=[0, None],
            legend_title="Disturbance amplitude",
            # violinmode='overlay',
            violingap=0,
        )
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(fig, n=n_dist, **annotation_kws)
    
    return fig


figs = {
    measure_name: get_lohi_summary_violins(measure, measure_name)
    for measure_name, measure in all_measures_lohi.items()
}
```

```{python}
for measure_name, measure_handle in tqdm(measure_handles.items()):
    savefig(
        figs[measure_name], 
        join([
            f"{measure_handle}",
            "lohi-summary",
        ]),
        subdir=fig_subdir,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name].show()
```

i.e. not one plot for each evaluation condition for each measure, but a single plot for each measure, comparing the distributions for zero vs. high training disturbance std, on zero vs. high evaluation disturbance amplitude.

### Time to peak velocity

Since this is an integer (# timesteps), a KDE is not great for visualization (lumpy). Instead we mostly want to visualize the difference in mean and spread, thus we should use a box plot.

```{python}
# def get_lohi_summary_boxes(measure_data, measure_name, annotation_kws=None):
#     n_dist = np.prod(jt.leaves(measure_data)[0].shape)
    
#     fig = go.Figure(
#         data=jt.leaves([
#             [
#                 go.Box(
#                     x=jnp.full((n_dist,), disturbance_train_std),
#                     y=data.flatten(),
#                     name=disturbance_amplitude,
#                     # box_visible=False,
#                     # meanline_visible=True,
#                     line_color=disturbance_amplitudes_colors_dark[disturbance_amplitude],
#                     showlegend=(j == 0),
#                     # spanmode='hard',
#                 )
#                 for j, (disturbance_train_std, data) 
#                 in enumerate(measure_data[disturbance_amplitude].items())
#             ]
#             for i, disturbance_amplitude in enumerate(measure_data)
#         ]),
#         layout=dict(
#             width=500,
#             height=400,
#             xaxis_title="Train disturbance std.",
#             yaxis_title=measure_name,
#             xaxis_range=[-0.5, len(measure_data) - 0.5],
#             xaxis_tickvals=list(measure_data.keys()),
#             yaxis_range=[0, None],
#             legend_title="Disturbance amplitude",
#             # violinmode='overlay',
#             # violingap=0,
#         )
#     )
    
#     if annotation_kws is None:
#         annotation_kws = dict()
    
#     add_context_annotation(fig, n=n_dist, **annotation_kws)
    
#     return fig
    
# figs = {
#     measure_name: get_lohi_summary_boxes(measure, measure_name)
#     for measure_name, measure in box_measures_lohi.items()
# }
```

```{python}
# for measure_name in box_measures:
#     figs[measure_name].show()
```


