---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

# Analysis of plant perturbations

In this notebook we begin our analysis of the models trained in the presence of different levels of curl force fields:
- examine differences in baseline performance and state profiles of the models; e.g. differences in velocity profiles
- compare robustness to mechanical disturbances; e.g. differences in endpoint error when subject to large curl fields; 


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import numpy as np
import plotly.colors as plc
import plotly.graph_objects as go

from feedbax import (
    load_with_hyperparameters, 
    is_module, 
    is_type,
    tree_stack,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.channel import toggle_channel_noise
from feedbax.intervene import CurlField, schedule_intervenor
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies.part1_setup import (
    setup_models, 
    setup_model_parameter_histories,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    get_savefig_func,
    add_context_annotation,
    add_endpoint_traces,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_forward_lateral_vel, 
    get_lateral_distance,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    swap_model_trainables, 
    subdict, 
    pp,
)
from rnns_learn_robust_motor_policies.setup_utils import (
    display_model_filechooser,
    filename_join as join,
    set_model_noise,
    find_unique_filepath,
)

```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"\nFeedbax commit hash: {git_commit_id()}")
```

Unique ID for notebook, for naming outputs.

```{python}
NB_ID = "1-2a"
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_type_load: Literal['curl', 'random'] = 'curl'
feedback_noise_std_load = 0.0
motor_noise_std_load = 0.0
feedback_delay_steps_load = 0  

# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'random']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
noise_stds = dict(
    feedback=feedback_noise_std,
    motor=motor_noise_std,
)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### Directories

```{python}
MODELS_DIR = Path('../models')

if not MODELS_DIR.exists():
    raise FileNotFoundError(f"Models directory not found: {MODELS_DIR.absolute()}")
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

We'll provide a couple of options, here. 

### Specify file by noise and delay hyperparameters

First, we can specify all the hyperparameters to load the corresponding model, assuming it has been trained.

```{python}
load_from_parameters = True
```

```{python}
suffix_load = '_'.join([
    f"{disturbance_type_load}",
    f"noise-{feedback_noise_std_load}-{motor_noise_std_load}",
    f"delay-{feedback_delay_steps_load}",
])

if load_from_parameters and 'None' in suffix_load:
    raise ValueError("If loading from parameters, all parameters must be specified.")
```

### Specify file by user selection

On the other hand, maybe the user wants to browse and select a trained model file. In that case we can display a file chooser.

```{python}
if not load_from_parameters:
    fc = display_model_filechooser(MODELS_DIR, filter_pattern='1-1_*trained_models.eqx')
```

The default filename is the one that sorts last. If the user does not select a file or if the following cell is run before they do, then the default file will be loaded.

### Load the specified model

```{python}
if not load_from_parameters:
    if fc.selected is None:
        models_filepath = f"{fc.default_path}/{fc.default_filename}"
    else:
        models_filepath = fc.selected
else: 
    models_filepath = str(find_unique_filepath(MODELS_DIR, f"1-1__{suffix_load}__trained_models"))
    if models_filepath is None:
        raise FileNotFoundError(f"No models found with file label: {suffix_load}")

trained_models, hyperparameters = load_with_hyperparameters(
    # MODELS_DIR / models_filepath, setup_models,
    MODELS_DIR / models_filepath, setup_models,
)

# We'll use this for creating figure subdirectories according to the training conditions
suffix_train = models_filepath.split('__')[1]
```

### Modify the system noise if needed

```{python}
trained_models = jt.map(
    partial(
        set_model_noise, 
        noise_stds=noise_stds,
        enable_noise=True,
    ),
    trained_models,
    is_leaf=is_module,
)

curl_stds_train = hyperparameters['disturbance_stds']

trained_models = dict(zip(curl_stds_train, trained_models))
```

### Load parameters from earlier training iterations, where necessary

Also depending on how training goes, we might want to use model parameters from an earlier training iteration. 

```{python}
model_parameter_histories, train_hyperparameters = load_with_hyperparameters(
    MODELS_DIR / models_filepath.replace('trained_models', 'model_parameter_histories'),
    partial(setup_model_parameter_histories, list(trained_models.values())),
)

where_train = attr_str_tree_to_where_func(train_hyperparameters['where_train_strs'])

model_parameter_histories = dict(zip(curl_stds_train, model_parameter_histories))

# load_spec = {0.4: -1, 0.5: -1}

# for curl_std, i in load_spec.items():
#     trained_models[curl_std] = swap_model_trainables(
#         trained_models[curl_std], tree_take(model_parameter_histories[curl_std], i), where_train
#     )
```

### Assign some variables to indicate which parameters we used/will be using

The following will not vary between training and analysis.

```{python}
feedback_delay_steps = hyperparameters['feedback_delay_steps']
n_replicates = hyperparameters['n_replicates']
```

The following may vary in analysis but we may want to refer to the training values.

```{python}
disturbance_type_train = hyperparameters['disturbance_type']
noise_stds_train = dict(
    feedback=hyperparameters['feedback_noise_std'],
    motor=hyperparameters['motor_noise_std'],
)
```

If we didn't alter the disturbance type or noise levels for the analysis, we can infer they'll be the same as the ones used during training.

```{python}
if disturbance_type is None:
    disturbance_type = disturbance_type_train

noise_stds = {
    k: v if v is not None else noise_stds_train[k]
    for k, v in noise_stds.items()
} 

any_system_noise = any(jt.leaves(noise_stds))
```

### Define subsets of models/training conditions

Depending on how training goes, we might want to leave out some of the extreme curl strength training conditions from the analysis.

```{python}
include_curl_stds = curl_stds_train  # Change nothing.

trained_models = {key: trained_models[key] for key in include_curl_stds}
```

Some summary plots will only include the lowest and highest training curl conditions. 

```{python}
curl_stds_train_lohi = [0, curl_stds_train[-1]]
```

### Additional setup that depends on having loaded the models

The hierarchy of figure directories depends on the hyperparameters of the trained model. Now that we've sorted that out, we can set up the base subdirectory for figures generated in the analyses that follow in this notebook.

```{python}
suffix = f"{disturbance_type}_noise-{noise_stds['feedback']}-{noise_stds['motor']}_delay-{feedback_delay_steps}"

FIGS_DIR = Path(f'../figures/{NB_ID}/train__{suffix_train}/{suffix}')

for d in (FIGS_DIR,):
    d.mkdir(parents=True, exist_ok=True)
```

```{python}
savefig = get_savefig_func(FIGS_DIR)
```

## Set up evaluation tasks

We will generally evaluate on a 2*2 grid of center-out reach sets, with 32 reach directions per set. 

This is to ensure good coverage and a larger set of conditions/trials on which to perform statistics. 

In the case of visualization of center-out sets, we'll use a smaller version of the task with only a single set of 7 reaches (an odd number helps with visualization).

We'll evaluate on a range of constant curl amplitudes, starting from 0 (no curl/control).

For a 2*2  grid of 32-direction center-out reach sets, 10 replicates, and 5 trials per reach, evaluating each task variant leads to approximately 1.5 GB of states. If we run out memory, it may be necessary to:

- reduce the number of evaluation reaches (e.g. `n_trials` or `eval_n_directions`);
- wait to evaluate until we have decided on a subset of trials to plot; i.e. define a function to evaluate subsets of data as needed;
- do this on CPU (assuming we have more RAM available)

```{python}
if disturbance_type == 'curl':
    disturbance_amplitudes = [0.0, 0.5, 1.0, 2.0, 4.0]
    
    def disturbance(amplitude):
        return CurlField.with_params(amplitude=amplitude)    

elif disturbance_type == 'random':
    disturbance_amplitudes = [0.0, 0.05, 0.1, 0.2, 0.4]
    
    # TODO: sample Gaussian magnitude and uniform direction
    def disturbance(amplitude):
        return FixedField.with_params(
            scale=field_std,
            field=lambda trial_spec, *, key: jr.normal(key, (2,)),
            # active=True,
        )
        
else:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")

disturbance_amplitudes_lohi = [0.0, disturbance_amplitudes[-1]]
```

```{python}
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

eval_grid_n = 2
eval_n_directions = 24
eval_reach_length = 0.5

eval_grid_n_small = 1
eval_n_directions_small = 7
eval_reach_length_small = 0.5

# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=eval_n_directions,
    eval_reach_length=eval_reach_length,  
)
```

```{python}
# Make task variants with different levels of disturbance.
# The trained models already have a `CurlField` intervenor, so we can just 
# schedule an intervenor (of the same label) with the task, and discard the modified 
# model returned by `schedule_intervenor`. 
tasks, _ = tree_unzip(jt.map(
    lambda disturbance_amplitude: schedule_intervenor(
        task, trained_models[0],
        lambda model: model.step.mechanics,
        CurlField.with_params(amplitude=disturbance_amplitude),
        # Ensures the schedule will be applied to the labelled intervenors already present in the trained models
        label="CurlField",  
        default_active=False,
    ),
    dict(zip(disturbance_amplitudes, disturbance_amplitudes)),
))

# Make smaller versions of the tasks for visualization.
tasks_small = jt.map(
    lambda task: eqx.tree_at(
        lambda task: (
            task.eval_grid_n,
            task.eval_n_directions,
            task.eval_reach_length,
        ),
        task, 
        (
            eval_grid_n_small, 
            eval_n_directions_small, 
            eval_reach_length_small,
        ),
    ),
    tasks,
    is_leaf=is_module,
)
``` 

And for convenience:

```{python}
trial_specs = task.validation_trials
trial_specs_small = jt.leaves(tasks_small, is_leaf=is_module)[0].validation_trials

pos_endpoints = jnp.stack([
    trial_specs.inits['mechanics.effector'].pos, 
    trial_specs.targets['mechanics.effector.pos'].value[:, -1],
], axis=0)

pos_endpoints_small = jnp.stack([
    trial_specs_small.inits['mechanics.effector'].pos, 
    trial_specs_small.targets['mechanics.effector.pos'].value[:, -1],
], axis=0)
```

### Number of evaluations per model and condition

We'll evaluate each condition (reach direction) several times, to see how performance varies with noise.

```{python}
n_trials = 5
n_trials_small = 5

if not any_system_noise:
    n_trials = n_trials_small = 1
```

## Setup colorscales for plots

Now that we know how many training and evaluation conditions we'll be working with, we can define the color scales for plots once and for all.

```{python}
# some colors will be darkened by a factor (e.g. mean curves)
darken_factor = 0.7

# when coloring by reach condition
reach_condition_colorscale = 'phase'

# when coloring by evaluations ("trials")
# NOTE: for now we are not coloring by trials, so make these constant
trials_colorscale = 'Tealgrn'  # for trials
trials_colors = plc.sample_colorscale(trials_colorscale, n_trials + 1)[:-1]
trials_colors_small = plc.sample_colorscale(trials_colorscale, n_trials_small + 1)[:-1]

# when coloring by training condition
curl_stds_train_colorscale = 'viridis'
curl_stds_train_colors = fbp.sample_colorscale_unique(curl_stds_train_colorscale, len(curl_stds_train))
curl_stds_train_colors_lohi = [curl_stds_train_colors[0], curl_stds_train_colors[-1]]
curl_stds_train_colors_dark = fbp.adjust_color_brightness(curl_stds_train_colors, darken_factor)
curl_stds_train_colors_dark_lohi = [curl_stds_train_colors_dark[0], curl_stds_train_colors_dark[-1]]

# when coloring by evaluation condition
disturbance_amplitudes_colorscale = 'plotly3'
disturbance_amplitudes_colors = fbp.sample_colorscale_unique(disturbance_amplitudes_colorscale, len(disturbance_amplitudes))
disturbance_amplitudes_colors_lohi = [disturbance_amplitudes_colors[0], disturbance_amplitudes_colors[-1]]
disturbance_amplitudes_colors_dark = fbp.adjust_color_brightness(disturbance_amplitudes_colors, darken_factor)
disturbance_amplitudes_colors_dark_lohi = [disturbance_amplitudes_colors_dark[0], disturbance_amplitudes_colors_dark[-1]]
```

## Evaluate the trained models on each evaluation task

In particular, for each trained ensemble of models (i.e. each training condition), evaluate each model in the ensemble on `n_trials` repetitions of each of the reach conditions. 

```{python}
def get_eval_ensemble(models, task):
    def eval_ensemble(key):
        return task.eval_ensemble(
            models,
            n_replicates=n_replicates,
            ensemble_random_trials=False,
            key=key,
        )
    return eval_ensemble

all_states = jt.map(
    lambda task: jt.map(
        lambda models: eqx.filter_vmap(get_eval_ensemble(models, task))(
            jr.split(key_eval, n_trials),
        ),
        trained_models,
        is_leaf=is_module,
    ),
    tasks,
    is_leaf=is_module,
)

all_states_small = jt.map(
    lambda task: jt.map(
        lambda models: eqx.filter_vmap(get_eval_ensemble(models, task))(
            jr.split(key_eval, n_trials_small),
        ),
        trained_models,
        is_leaf=is_module,
    ),
    tasks_small,
    is_leaf=is_module,
)
```

Note that we:

1. Set `ensemble_random_trials=False` so that each model in the ensemble will be evaluated on the same set of trials. 
2. Vmap over `keys_eval`, to obtain `n_trials` different evaluations of the center-out set. 
3. Use `jt.map` to repeat for each training condition (entry in `trained_models`).

The objects `all_states` and `all_states_small` are dicts whose keys are the `disturbance_amplitudes`, and whose values are dicts whose keys are the `curl_stds` used in training, and whose values are the PyTrees of state arrays.

Each state array has the following batch dimensions: `(n_trials, n_replicates, n_conditions, n_steps)`. For a single center-out set in `all_states_small`, `n_conditions = eval_n_directions`. But since `eval_grid_n != 1` for the full tasks evaluated in `all_states`, then there will be `eval_grid_n ** 2` center-out sets and the third dimension will have size `eval_n_directions * eval_grid_n ** 2`.

```{python}
pp(all_states)
```

TODO: Instead of a two-layer `jt.map`, we could instead define a function (to schedule the intervenor given a `curl_std` and then `jt.map` the vmapped `eval_ensemble`) and then vmap it over `curl_stds_test` to get a single-level dict with a bit less overhead.

## Plot example center-out sets

i.e. show trials for multiple reach directions, to show performance across conditions at a glance, for a single model

Note that we use `states_small` here, and only plot a single center-out set of 7 reach directions.

```{python}
fig_subdir = "center_out_sets"
```

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

### All trials for a replicate

```{python}
fig_subdir = "center_out_sets/all_evals_single_replicate"
```

```{python}
i_replicate = 0
```

```{python}
plot_states = tree_take_multi(all_states_small, [i_replicate], [1])

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        colorscale=reach_condition_colorscale,
        colorscale_axis=1,  # Color/group by reach condition
        mode='lines',
        legend_title='Reach direction',
        scatter_kws=dict(line_width=0.5),
    ),
    plot_states,
    is_leaf=is_module,
)
```

Add an annotation to provide context, and save figures. Display just the (2x2) zero versus high curl, training versus evaluation conditions.


```{python}
if not any_system_noise:
    print("Skipping center-out sets that compare different evaluations, for this zero-noise condition")
else:
    for disturbance_amplitude in figs:
        for train_curl_std, fig in figs[disturbance_amplitude].items():
            add_context_annotation(
                fig,
                train_condition_strs=[
                    f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{train_curl_std})"
                ],
                perturbations={
                    f"{disturbance_type}": (disturbance_amplitude, None, None)
                },
                n=n_trials_small,
                i_replicate=i_replicate,
            )
            add_endpoint_traces(
                fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
            )
            savefig(
                fig, 
                join([
                    f"eval-curl-{disturbance_amplitude}",
                    f"train-curl-std-{train_curl_std}",
                    f"replicate-{i_replicate}",
                ]),
                subdir=fig_subdir,
            )
            
    for disturbance_amplitude in disturbance_amplitudes_lohi:
        for train_curl_std in curl_stds_train_lohi:
            figs[disturbance_amplitude][train_curl_std].show()
```

### A single trial set, for a single replicate

```{python}
fig_subdir = "center_out_sets/single_eval_single_replicate"
```

```{python}
i_trial = 0

plot_states = tree_take_multi(all_states_small, [i_trial, i_replicate], [0, 1])

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        mode='markers+lines',
        colorscale=reach_condition_colorscale,
        legend_title='Reach direction',
        ms=3,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in figs:
    for train_curl_std, fig in figs[disturbance_amplitude].items():
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{train_curl_std})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            i_trial=i_trial,
            i_replicate=i_replicate,
        )
        add_endpoint_traces(
            fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
        )
        filename = join([
                f"eval-curl-{disturbance_amplitude}",
                f"train-curl-std-{train_curl_std}",
                f"rep-{i_replicate}",
                f"eval-{i_trial}",
        ])
        savefig(fig, filename, subdir=fig_subdir)
```

```{python}
for disturbance_amplitude in disturbance_amplitudes_lohi:
    for train_curl_std in curl_stds_train_lohi:
        figs[disturbance_amplitude][train_curl_std].show()
```

### A single trial, for all replicates

```{python}
fig_subdir = "center_out_sets/single_eval_all_replicates"
```

```{python}
plot_states = tree_take_multi(all_states_small, [i_trial], [0])

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        colorscale=reach_condition_colorscale,
        colorscale_axis=1,  # Color/group by reach condition
        mode='lines',
        legend_title='Reach direction',
        scatter_kws=dict(line_width=0.5),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in figs:
    for train_curl_std, fig in figs[disturbance_amplitude].items():
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{train_curl_std})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            i_trial=i_trial,
            n=n_replicates,
        )
        add_endpoint_traces(
            fig, pos_endpoints_small, xaxis='x1', yaxis='y1', colorscale='phase'
        )
        fig.update_layout(legend_tracegroupgap=5)
        savefig(
            fig, 
            join([
                f"eval-curl-{disturbance_amplitude}",
                f"train-curl-std-{train_curl_std}",
                f"eval-{i_trial}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for disturbance_amplitude in disturbance_amplitudes_lohi:
    for train_curl_std in curl_stds_train_lohi:
        figs[disturbance_amplitude][train_curl_std].show()
```

### Side-by-side comparison of position trajectories across training conditions

i.e. show center-out sets for model trained without curl vs. with strongest curl

TODO: one comparison require a single call to `trajectories_2D`

## Plot example trajectories for a single reach condition

i.e. for a single reach direction, compare multiple trials/replicates different training conditions; visualize how training on different curl strengths affects response.

```{python}
fig_subdir = "single_reach_condition"
```

```{python}
i_condition = 0
```

```{python}
pos_endpoints_i = pos_endpoints_small[:, i_condition]
```

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

### All trials and replicates for a given train-test condition, indexing by trial

```{python}
fig_subdir = "single_reach_condition/per_train_test_pair"
```

```{python}
plot_states = tree_take_multi(all_states_small, [i_condition], [2])
```

```{python}
figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        mode='lines',
        legend_title="Trial",
        colorscale=trials_colorscale,
        colorscale_axis=0,
        scatter_kws=dict(line_width=0.5),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in figs:
    for train_curl_std, fig in figs[disturbance_amplitude].items():
        add_endpoint_traces(fig, pos_endpoints_i, xaxis='x1', yaxis='y1')
        
        add_context_annotation(
            fig,
            train_condition_strs=[
                f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{train_curl_std})"
            ],
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            n=n_replicates,  # color/legend is by trial, so this is N per trial
            # i_condition=i_condition,
        )
        
        savefig(
            fig, 
            join([
                f"single-condition-{i_condition}",
                f"eval-curl-{disturbance_amplitude}",
                f"train-curl-std-{train_curl_std}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for disturbance_amplitude in disturbance_amplitudes_lohi:
    for train_curl_std in curl_stds_train_lohi:
        figs[disturbance_amplitude][train_curl_std].show()
```

### Compare the test curl amplitude, for each training curl std

```{python}
fig_subdir = "single_reach_condition/compare_test_conditions"
```

```{python}
# Stack the states for different test curls into single arrays
plot_states_stacked = tree_stack(plot_states.values())

n_trials_plot = n_trials_small * n_replicates

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        mean_trajectory_line_width=3,
        axes_labels=('x', 'y'),
        mode='lines',
        colorscale=disturbance_amplitudes_colorscale,
        colorscale_axis=0,
        legend_title="Curl amplitude",
        legend_labels=disturbance_amplitudes,
        scatter_kws=dict(line_width=1, opacity=0.75),
    ),
    plot_states_stacked,
    is_leaf=is_module,
)
```

```{python}
for train_curl_std, fig in figs.items():
    add_context_annotation(
        fig,
        train_condition_strs=[
            f"{disturbance_type_train} with amplitude ~ \U0001d4dd(0,{train_curl_std})"
        ],
        n=n_trials_plot,
    )
    
    add_endpoint_traces(fig, pos_endpoints_i, xaxis='x1', yaxis='y1')
    
    savefig(
        fig, 
        join([
            f"single-condition-{i_condition}",
            f"train-curl-std-{train_curl_std}",
        ]),
        subdir=fig_subdir,
    )    
```

```{python}
for train_curl_std in curl_stds_train_lohi:
    figs[train_curl_std].show()
```

### Compare the training curl std, for each test curl amplitude

```{python}
fig_subdir = "single_reach_condition/compare_train_conditions"
```

```{python}
# Only plot a subset of the training conditions
# curl_stds_train_plot = [
#     curl_stds_train[0],
#     curl_stds_train[len(curl_stds_train) // 2],
#     curl_stds_train[-1],
# ]
curl_stds_train_plot = curl_stds_train 
stride = 2

# Stack the states for different training curl stds into single arrays
# (also taking a subset of the training conditions with `subdict`)
plot_states_stacked = {
    disturbance_amplitude: tree_stack(subdict(states, curl_stds_train_plot).values())
    for disturbance_amplitude, states in plot_states.items()
}

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        # ref_endpoints=(pos_endpoints, None),
        mean_trajectory_line_width=3,
        darken_mean=darken_factor,
        var_endpoint_ms=0,
        axes_labels=('x', 'y'),
        mode='lines',
        stride=stride,
        colorscale=curl_stds_train_colorscale,
        colorscale_axis=0,
        legend_title='Train curl std.',
        legend_labels=curl_stds_train,
        scatter_kws=dict(line_width=1, opacity=0.3),
    ),
    plot_states_stacked,
    is_leaf=is_module,
)

n_trials_plot = n_trials_small * n_replicates

for disturbance_amplitude, fig in figs.items():
    add_context_annotation(
        fig,
        perturbations={
            f"{disturbance_type}": (disturbance_amplitude, None, None)
        },
        n=n_trials_plot,
    )
    
    add_endpoint_traces(fig, pos_endpoints_i, xaxis='x1', yaxis='y1')

    savefig(
        fig, 
        join([
            f"single-condition-{i_condition}",
            f"disturbance_amplitude-{disturbance_amplitude}",
        ]),
        subdir=fig_subdir,
    )   
```

```{python}
for disturbance_amplitude in disturbance_amplitudes_lohi:
    figs[disturbance_amplitude].show()
```

## Compare velocity profiles

```{python}
fig_subdir = "velocity_profiles"
```

Average over trials, directions, and replicates to get an average velocity profile + error bands for each training condition. 

Note that it only makes sense to compare across reach directions if we consider the velocity profiles along the respective directions. For example, the y-profile in one direction should be comparable to the x-profile in an orthogonal direction.

There are two ways we might get profiles for comparison.

1. Calculate speed as the amplitude of the velocity vector. This is simpler.
2. Project the velocity profiles onto the reach direction.

These are not identical, and the second method is more flexible because we still retain the components and their signs. 

### Speed profiles

:::{note}
I am omitting speed profiles from analysis since they are not very interpretable.
:::

```{python}
fig_subdir = "velocity_profiles/speed"
```

```{python}
# def get_speed(states):
#     return jnp.linalg.norm(states.mechanics.effector.vel, axis=-1)

# all_speeds = jt.map(get_speed, all_states, is_leaf=is_module)

# n_dist = np.prod(jt.leaves(all_speeds)[0].shape[:-1])

# figs = {
#     disturbance_amplitude: fbp.profiles(
#         speeds,
#         mode='std', 
#         varname="Speed",
#     )
#     for disturbance_amplitude, speeds in all_speeds.items()
# }

# for disturbance_amplitude in figs:
#     add_context_annotation(
#         figs[disturbance_amplitude],
#         perturbations={
#             f"{disturbance_type}": (disturbance_amplitude, None, None)
#         },
#         n=n_dist,
#         y=1.1,
#     )

# for disturbance_amplitude in disturbance_amplitudes_lohi:
#     figs[disturbance_amplitude].show()
```

### Forward and lateral velocity profiles

```{python}
fig_subdir = "velocity_profiles/forward_lateral"
```

**TODO: Plot on shared time axis**

```{python}
forward_lateral_vel = jt.map(
    lambda state: get_forward_lateral_vel(state.mechanics.effector.vel, pos_endpoints),
    all_states,
    is_leaf=is_module,
)
```

```{python}
n_dist = np.prod(jt.leaves(forward_lateral_vel)[0].shape[:-2])

figs = {
    disturbance_amplitude: {
        label: fbp.profiles(
            tree_take(forward_lateral_vel, i, -1)[disturbance_amplitude],
            varname=f"{label} velocity",
            legend_title="Train curl std.",
            mode='std', # or 'curves'
            n_std_plot=1,
            colors=curl_stds_train_colors_dark,
            # stride_curves=500,
            # curves_kws=dict(opacity=0.7),
        )
        for i, label in enumerate(("Forward", "Lateral"))
    }
    for disturbance_amplitude in disturbance_amplitudes
}

for disturbance_amplitude in disturbance_amplitudes:
    for i, label in enumerate(("Forward", "Lateral")):
        fig = figs[disturbance_amplitude][label]
        fig.add_hline(y=0, line_color="grey", layer="below")
        add_context_annotation(
            fig,
            perturbations={
                f"{disturbance_type}": (disturbance_amplitude, None, None)
            },
            n=n_dist,
        )
        filename = f"eval-curl-{disturbance_amplitude}_{label.lower()}-vels"
        savefig(fig, filename, subdir=fig_subdir)
        
for disturbance_amplitude in disturbance_amplitudes_lohi:
    figs[disturbance_amplitude]["Forward"].show()
    figs[disturbance_amplitude]["Lateral"].show()

```

## Summary comparison of performance measures 

```{python}
fig_subdir = "performance_measures"
```

### Calculate all measures

Maximum speed

```{python}
def get_max_speed(states):
    return jnp.max(get_speed(states), axis=-1)
```

Maximum lateral (left and right relative to reach direction) and forward velocities

**TODO: avoid repeat calculation of forward_lateral_vel; perhaps by inserting subtrees in `all_measure_funcs` and then flattening**
**TODO: don't hard-code `pos_endpoints` otherwise we can't use the resulting functions on other task states (e.g. all_states_small)**

```{python}
def get_max_forward_vel(states):
    vel = states.mechanics.effector.vel
    forward_lateral_vel = get_forward_lateral_vel(vel, pos_endpoints)
    return jnp.max(forward_lateral_vel[..., 0], axis=-1)
```

```{python}
def get_max_lateral_vel(states, mul=1.):
    vel = states.mechanics.effector.vel
    return jnp.max(mul * get_forward_lateral_vel(vel, pos_endpoints)[..., 1], axis=-1)
```

Time to peak forward velocity

```{python}
def get_argmax_forward_vel(states):
    vel = states.mechanics.effector.vel
    return jnp.argmax(get_forward_lateral_vel(vel, pos_endpoints)[..., 0], axis=-1)
```

Maximum and summed lateral distance of the effector from a straight trajectory

```{python}
# lambda states: get_lateral_distance(states.mechanics.effector.pos, pos_endpoints)
```

```{python}
def get_max_lateral_distance(states, eval_reach_length=None):
    pos = states.mechanics.effector.pos
    distances = jnp.max(get_lateral_distance(pos, pos_endpoints), axis=-1)
    if eval_reach_length is not None:
        distances = 100 * distances / eval_reach_length
    return distances
    
def get_sum_lateral_distance(states):
    pos = states.mechanics.effector.pos
    return jnp.sum(get_lateral_distance(pos, pos_endpoints), axis=-1)
```

Position and velocity errors relative to the goal, averaged over the last `last_n_steps` time steps

```{python}
last_n_steps = 10

def end_position_error(states, last_n_steps=1, eval_reach_length=None):
    final_pos = states.mechanics.effector.pos[..., -last_n_steps:, :]
    goal_pos = trial_specs.targets['mechanics.effector.pos'].value[..., -last_n_steps:, :]
    error = jnp.mean(jnp.linalg.norm(final_pos - goal_pos, axis=-1), axis=-1)
    if eval_reach_length is not None:
        error = 100 * error / eval_reach_length
    return error
    
# TODO: This could be normalized by the max. velocity
def end_velocity_error(states, last_n_steps=1):
    final_vel = states.mechanics.effector.vel[..., -last_n_steps:, :]
    goal_vel = 0
    return jnp.mean(jnp.linalg.norm(final_vel - goal_vel, axis=-1), axis=-1)
```

Max and sum of the net control forces

```{python}
def get_net_control_force(states):
    return jnp.linalg.norm(states.efferent.output, axis=-1)

def get_max_net_control_force(states):
    return jnp.max(get_net_control_force(states), axis=-1)
    
def get_sum_net_control_force(states):
    return jnp.sum(get_net_control_force(states), axis=-1)
```

Collect measures into PyTree

```{python}
all_measure_funcs, measure_handles = tree_unzip(OrderedDict({
    # "Max speed": (get_max_speed, "speed-max"),
    "Max forward velocity": (get_max_forward_vel, "vel-forward-max"),
    "Max lateral velocity (left)": (
        partial(get_max_lateral_vel, mul=1.), 
        "vel-lateral-left-max",
    ),
    "Max lateral velocity (right)": (
        partial(get_max_lateral_vel, mul=-1.), 
        "vel-lateral-right-max",
    ),
    "Max lateral distance (% reach length)": (
        partial(get_max_lateral_distance, eval_reach_length=eval_reach_length), 
        "dist-lateral-max",
    ),
    "Sum lateral distance": (get_sum_lateral_distance, "dist-lateral-sum"),
    "End position error (% reach length)": (
        partial(end_position_error, last_n_steps=last_n_steps, eval_reach_length=eval_reach_length), 
        "error-end-pos",
    ),
    "End velocity error": (
        partial(end_velocity_error, last_n_steps=last_n_steps), 
        "error-end-vel",
    ),
    "Max net control force": (get_max_net_control_force, "force-net-max"),
    "Sum net control force": (get_sum_net_control_force, "force-net-sum"),
}))

box_measure_funcs, box_measure_handles = tree_unzip(OrderedDict({
    "Time to peak forward velocity": (get_argmax_forward_vel, "vel-forward-max-timestep"),
}))
```

```{python}
from jaxtyping import PyTree
from collections.abc import Callable
from feedbax.bodies import SimpleFeedbackState

def get_all_measures(measure_funcs: PyTree[Callable], states: PyTree[SimpleFeedbackState]):
    return jt.map(
        lambda func: jt.map(
            lambda states: func(states),
            states,
            is_leaf=is_module,
        ),
        measure_funcs,
    )
    
all_measures = get_all_measures(all_measure_funcs, all_states)

box_measures = get_all_measures(box_measure_funcs, all_states)

# TODO: fix the `pos_endpoints` hardcoding, above
# all_measures_small = get_all_measures(all_measure_funcs, all_states_small)
```

### For each evaluation condition, plot measure distributions by training condition

```{python}
fig_subdir = "performance_measures/compare_train_conditions"
```

One plot per evaluation condition (curl amplitude); one violin per training condition (curl std).

Distributions are aggregated over all replicates and trials. 

```{python}
def get_violins_across_train_conditions(data, measure_name, disturbance_amplitude, annotation_kws=None):
    n_dist = np.prod(jt.leaves(data)[0].shape)
    
    fig = go.Figure(
        data=[
            go.Violin(
                y=data[train_curl_std].flatten(),
                name=train_curl_std,
                box_visible=False,
                meanline_visible=True,
                line_color=curl_stds_train_colors_dark[i],
                showlegend=False,
                opacity=1,
                spanmode='hard',
                
            )
            for i, train_curl_std in enumerate(data)
        ],
        layout=dict(
            # title=(f"Response to amplitude {disturbance_amplitude} curl field <br>N = {n_dist}"),
            xaxis_title="Training curl std.",
            yaxis_title=measure_name,
            xaxis_range=[-0.5, len(data) - 0.5],
            xaxis_tickvals=list(data.keys()),
            yaxis_range=[0, None],
            # violinmode='overlay',
            violingap=0,
        )
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(
        fig, 
        perturbations={
            f"{disturbance_type}": (disturbance_amplitude, None, None)
        },
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig


def get_one_measure_plot_per_eval_condition(plot_func, all_measures, **kwargs):
    return {
        measure_name: {
            disturbance_amplitude: plot_func(
                measure[disturbance_amplitude], measure_name, disturbance_amplitude, **kwargs
            )
            for disturbance_amplitude in measure
        }
        for measure_name, measure in all_measures.items()
    }
    

figs = get_one_measure_plot_per_eval_condition(
    get_violins_across_train_conditions, 
    all_measures,
)
```

```{python}
for measure_name, measure_handle in measure_handles.items():
    for disturbance_amplitude, fig in figs[measure_name].items():
        savefig(
            fig, 
            join([
                f"{measure_handle}",
                f"eval-curl-{disturbance_amplitude}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for measure_name in all_measures:
    figs[measure_name][4].show()
```

### Repeat for just a single reach condition

:::{note}
Omitting this from the output since it's sufficient to see the variation in (e.g.) the velocity profiles to be convinced that there is not huge variation between reach directions/conditions.
:::

```{python}
fig_subdir = "performance_measures/compare_train_conditions/single_reach_condition"
```

The distributions should be similar, since the model+task is isotropic.

TODO: could use `all_states_small` and increase the number of trials...

```{python}
# i_condition = 0
# all_measures_one_dirxn = tree_take(all_measures, i_condition, -1)

# figs0 = get_one_measure_plot_per_eval_condition(
#     get_violins_across_train_conditions, 
#     all_measures_one_dirxn,
#     annotation_kws=dict(i_condition=i_condition),
# )

# for measure_name, measure_handle in measure_handles.items():
#     for disturbance_amplitude, fig in figs0[measure_name].items():
#         savefig(
#             fig, 
#             join([
#                 f"{measure_handle}",
#                 f"eval-curl-{disturbance_amplitude}",
#                 f"condition-{i_condition}",
#             ]),
#             subdir=fig_subdir,
#         )

# for measure_name in all_measures:
#     figs0[measure_name][4].show()
```

### Repeat for just the zero vs. highest-std training condition

:::{note}
Omitting this from the output since the low-high summary figures below will include this comparison.
:::

```{python}
fig_subdir = "performance_measures/compare_train_conditions/lowhigh_only"
```

```{python}
all_measures_subset_curl_stds_train = {
    k1: {k2: subdict(v, curl_stds_train_lohi) for k2, v in all_measures[k1].items()}
    for k1 in all_measures
}

box_measures_subset_curl_stds_train = {
    k1: {k2: subdict(v, curl_stds_train_lohi) for k2, v in box_measures[k1].items()}
    for k1 in box_measures
}
```

```{python}
# figs = get_one_measure_plot_per_eval_condition(
#     get_violins_across_train_conditions, 
#     all_measures_subset_curl_stds_train,
# )

# figs = jt.map(
#     lambda fig: fig.update_layout(
#         height=400,
#         width=500,
#     ),
#     figs,
#     is_leaf=is_module,
# )

# for measure_name, measure_handle in measure_handles.items():
#     for disturbance_amplitude, fig in figs[measure_name].items():
#         savefig(
#             fig, 
#             join([
#                 f"{measure_handle}",
#                 f"eval-curl-{disturbance_amplitude}",
#                 "lowest-vs-highest",
#             ]),
#             subdir=fig_subdir,
#         )

# for measure_name in all_measures:
#     figs[measure_name][4].show()
```

### Plot comparison of distributions for zero vs. high train curl std, for different replicates

Compare the smallest (zero) and largest training curl stds. For the plots above, this would mean keeping just the leftmost and the rightmost violins. However, those plots were aggregated over replicates and trials. Now we would like to examine the variance across model replicates. Thus, for each measure, we will generate one plot for each evaluation condition (curl amplitude) as before, but now each containing `n_replicates` *split* violins, where the left half corresponds to the zero training curl std, and the right half to the largest training curl std.

```{python}
fig_subdir = "performance_measures/compare_replicates_lowhigh_train_conditions"
```

```{python}
labels = curl_stds_train_lohi
colors = curl_stds_train_colors_dark_lohi

def get_measure_replicate_comparisons(data, measure_name, disturbance_amplitude, annotation_kws=None):
    n_dist = np.prod(jt.leaves(data)[0].shape)
    
    data = jnp.stack(list(data.values()))
    
    fig = go.Figure()
    # x axis: replicates
    for i in range(data.shape[-2]):
        # split violin: smallest vs. largest train curl std
        for j in range(data.shape[0]):
        
            fig.add_trace(
                go.Violin(
                    x=np.full_like(data[j, :, i].flatten(), i),
                    y=data[j, :, i].flatten(),
                    name=labels[j],
                    box_visible=False,
                    meanline_visible=True,
                    line_color=colors[j],
                    side='positive' if j == 1 else 'negative',
                    showlegend=(i == 0),
                    spanmode='hard',
                )
            )
    fig.update_layout(
        xaxis_title="Model replicate",
        yaxis_title=measure_name,
        xaxis_range=[-0.5, data.shape[-2] - 0.5],
        xaxis_tickvals=list(range(data.shape[-2])),
        yaxis_range=[0, None],
        violinmode='overlay',
        violingap=0,
        # title=(f"Response to amplitude {disturbance_amplitude} curl field <br>N = {n_dist}"),
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(
        fig, 
        perturbations={
            f"{disturbance_type}": (disturbance_amplitude, None, None)
        },
        n=n_dist, 
        **annotation_kws,
    )
    
    return fig
    
figs = get_one_measure_plot_per_eval_condition(
    get_measure_replicate_comparisons, 
    all_measures_subset_curl_stds_train,
)
```

```{python}
for measure_name, measure_handle in measure_handles.items():
    for disturbance_amplitude, fig in figs[measure_name].items():
        savefig(
            fig, 
            join([
                f"{measure_handle}",
                f"eval-curl-{disturbance_amplitude}",
            ]),
            subdir=fig_subdir,
        )
```

```{python}
for measure_name in all_measures:
    figs[measure_name][4].show()
```

### Summary comparison of no-curl vs. high-curl evaluation, for no-curl vs. high-curl training

```{python}
fig_subdir = "performance_measures/lowhigh_summaries"
```

```{python}
all_measures_lohi = {
    measure_name: subdict(measure, disturbance_amplitudes_lohi)
    for measure_name, measure in all_measures_subset_curl_stds_train.items()
}

box_measures_lohi = {
    measure_name: subdict(measure, disturbance_amplitudes_lohi)
    for measure_name, measure in box_measures_subset_curl_stds_train.items()
}
```

```{python}
def get_lohi_summary_violins(measure_data, measure_name, annotation_kws=None):
    n_dist = np.prod(jt.leaves(measure_data)[0].shape)
    
    fig = go.Figure(
        data=jt.leaves([
            [
                go.Violin(
                    x=jnp.full((n_dist,), train_curl_std),
                    y=data.flatten(),
                    name=disturbance_amplitude,
                    box_visible=False,
                    meanline_visible=True,
                    line_color=disturbance_amplitudes_colors_dark_lohi[i],
                    showlegend=(j == 0),
                    spanmode='hard',
                )
                for j, (train_curl_std, data) in enumerate(measure_data[disturbance_amplitude].items())
            ]
            for i, disturbance_amplitude in enumerate(measure_data)
        ]),
        layout=dict(
            width=500,
            height=400,
            xaxis_title="Training curl std.",
            yaxis_title=measure_name,
            xaxis_range=[-0.5, len(measure_data) - 0.5],
            xaxis_tickvals=list(measure_data.keys()),
            yaxis_range=[0, None],
            legend_title="Curl amplitude",
            # violinmode='overlay',
            violingap=0,
        )
    )
    
    if annotation_kws is None:
        annotation_kws = dict()
    
    add_context_annotation(fig, n=n_dist, **annotation_kws)
    
    return fig


figs = {
    measure_name: get_lohi_summary_violins(measure, measure_name)
    for measure_name, measure in all_measures_lohi.items()
}
```

```{python}
for measure_name, measure_handle in measure_handles.items():
    savefig(
        figs[measure_name], 
        join([
            f"{measure_handle}",
            "lohi-summary",
        ]),
        subdir=fig_subdir,
    )
```

```{python}
for measure_name in all_measures:
    figs[measure_name].show()
```

i.e. not one plot for each evaluation condition for each measure, but a single plot for each measure, comparing the distributions for zero vs. high training curl std, on zero vs. high evaluation curl amplitude.

### Time to peak velocity

Since this is an integer (# timesteps), a KDE is not great for visualization (lumpy). Instead we mostly want to visualize the difference in mean and spread, thus we should use a box plot.

```{python}
# def get_lohi_summary_boxes(measure_data, measure_name, annotation_kws=None):
#     n_dist = np.prod(jt.leaves(measure_data)[0].shape)
    
#     fig = go.Figure(
#         data=jt.leaves([
#             [
#                 go.Box(
#                     x=jnp.full((n_dist,), train_curl_std),
#                     y=data.flatten(),
#                     name=disturbance_amplitude,
#                     # box_visible=False,
#                     # meanline_visible=True,
#                     line_color=disturbance_amplitudes_colors_dark_lohi[i],
#                     showlegend=(j == 0),
#                     # spanmode='hard',
#                 )
#                 for j, (train_curl_std, data) in enumerate(measure_data[disturbance_amplitude].items())
#             ]
#             for i, disturbance_amplitude in enumerate(measure_data)
#         ]),
#         layout=dict(
#             width=500,
#             height=400,
#             xaxis_title="Training curl std.",
#             yaxis_title=measure_name,
#             xaxis_range=[-0.5, len(measure_data) - 0.5],
#             xaxis_tickvals=list(measure_data.keys()),
#             yaxis_range=[0, None],
#             legend_title="Curl amplitude",
#             # violinmode='overlay',
#             # violingap=0,
#         )
#     )
    
#     if annotation_kws is None:
#         annotation_kws = dict()
    
#     add_context_annotation(fig, n=n_dist, **annotation_kws)
    
#     return fig
    
# figs = {
#     measure_name: get_lohi_summary_boxes(measure, measure_name)
#     for measure_name, measure in box_measures_lohi.items()
# }
```

```{python}
# for measure_name in box_measures:
#     figs[measure_name].show()
```

### Others

#### Trend lines? 

Not sure the relationship is linear...

