---
jupyter: python3
---

# Analysis of plant perturbations

In this notebook we begin our analysis of the models trained in the presence of different levels of curl force fields:

- examine differences in baseline performance and state profiles of the models; e.g. differences in velocity profiles
- compare robustness to mechanical disturbances; e.g. differences in endpoint error when subject to large curl fields; 


## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from functools import partial
from itertools import zip_longest
from pathlib import Path
from operator import itemgetter

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import matplotlib.pyplot as plt
import plotly.graph_objects as go

from feedbax import (
    load_with_hyperparameters, 
    is_module, 
    tree_stack,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.intervene import CurlField, schedule_intervenor
from feedbax.misc import git_commit_id, attr_str_tree_to_where_func
from feedbax.noise import replace_noise
import feedbax.plotly as fbp
from feedbax.task import SimpleReaches
from feedbax._tree import eitherf, istype
from feedbax.xabdeef.losses import simple_reach_loss

from rnns_learn_robust_motor_policies.part1_setup import setup_models, setup_model_parameter_histories
from rnns_learn_robust_motor_policies.plot_utils import get_savefig_func
from rnns_learn_robust_motor_policies.state_utils import forward_lateral_vels
from rnns_learn_robust_motor_policies.tree_utils import swap_model_trainables, subdict

```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
for mod in (jax, eqx): 
    print(f"{mod.__name__} version: {mod.__version__}")
    
print(f"\nFeedbax commit hash: {git_commit_id()}")
```


### Global conditions

```{python}
NO_SYSTEM_NOISE = False

if NO_SYSTEM_NOISE:
    fig_suffix = "_no-noise"
else:
    fig_suffix = ""
```

### Directories setup

```{python}
FIGS_DIR = Path('../figures/1-2a/')
MODELS_DIR = Path('../models')

for d in (FIGS_DIR,):
    d.mkdir(parents=True, exist_ok=True)
    
if not MODELS_DIR.exists():
    raise FileNotFoundError(f"Models directory not found: {MODELS_DIR.absolute()}")
```

### Plotting setup 

```{python}
trials_cmap = 'viridis'  # for trials
trials_cmap_func = plt.get_cmap(trials_cmap)

savefig = get_savefig_func(FIGS_DIR)
```

### Helper functions

```{python}
def pp(tree):
    eqx.tree_pprint(tree, truncate_leaf=eitherf(is_module, istype(go.Figure)))
```

### RNG setup

```{python}
SEED = 5566
key = jr.PRNGKey(SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load trained models

```{python}
models_save_filename = "1-1_trained_models.eqx"
models_parameter_history_filename = "1-1_model_parameter_histories.eqx"

trained_models, hyperparameters = load_with_hyperparameters(
    MODELS_DIR / models_save_filename, setup_models,
)

if NO_SYSTEM_NOISE:
    trained_models = replace_noise(trained_models)

train_curl_stds = hyperparameters['disturbance_levels']
trained_models = dict(zip(train_curl_stds, trained_models))
```

Depending on how training goes, we might want to leave out some of the extreme curl strength training conditions.

```{python}
# curl_stds = curl_stds[:-2]

trained_models = {key: trained_models[key] for key in train_curl_stds}
```

### Load parameters from earlier training iterations, where necessary

Also depending on how training goes, we might want to keep the model parameters from an earlier training iteration.

```{python}
model_parameter_histories, train_hyperparameters = load_with_hyperparameters(
    MODELS_DIR / models_parameter_history_filename,
    partial(setup_model_parameter_histories, list(trained_models.values())),
)

model_parameter_histories = dict(zip(train_curl_stds, model_parameter_histories))
where_train = attr_str_tree_to_where_func(train_hyperparameters['where_train_strs'])

load_spec = {0.4: -1, 0.5: -1}

for curl_std, i in load_spec.items():
    trained_models[curl_std] = swap_model_trainables(
        trained_models[curl_std], tree_take(model_parameter_histories[curl_std], i), where_train
    )
```

## Set up evaluation tasks

We will generally evaluate on a 2*2 grid of center-out reach sets, with 32 reach directions per set. 

This is to ensure good coverage and a larger set of conditions/trials on which to perform statistics. 

In the case of visualization of center-out sets, we'll use a smaller version of the task with only a single set of 7 reaches (an odd number helps with visualization).

We'll evaluate on a range of constant curl amplitudes, starting from 0 (no curl/control).

For a 2*2  grid of 32-direction center-out reach sets, 10 replicates, and 5 trials per reach, evaluating each task variant leads to approximately 1.5 GB of states. If we run out memory, it may be necessary to:

- reduce the number of evaluation reaches (e.g. `n_trials` or `eval_n_directions`);
- wait to evaluate until we have decided on a subset of trials to plot; i.e. define a function to evaluate subsets of data as needed;
- do this on CPU (assuming we have more RAM available)

```{python}
curl_amplitudes_test = [0, 0.1, 0.5, 1.0]
n_steps = 100
workspace = ((-1., -1.),
             (1., 1.))

eval_grid_n = 2
eval_n_directions = 32
eval_reach_length = 0.3

eval_grid_n_small = 1
eval_n_directions_small = 7
eval_reach_length_small = 0.5

# Define the base task
task = SimpleReaches(
    loss_func=simple_reach_loss(),
    workspace=workspace, 
    n_steps=n_steps,
    eval_grid_n=eval_grid_n,
    eval_n_directions=eval_n_directions,
    eval_reach_length=eval_reach_length,  
)

# Make task variants with different levels of curl field.
# The trained models already have a `CurlField` intervenor, so we can just 
# schedule an intervenor (of the same label) with the task, and discard the modified 
# model returned by `schedule_intervenor`. 
tasks, _ = tree_unzip(jt.map(
    lambda curl_amplitude: schedule_intervenor(
        task, trained_models[0],
        lambda model: model.step.mechanics,
        CurlField.with_params(amplitude=curl_amplitude),
        # Ensures the schedule will be applied to the labelled intervenors already present in the trained models
        label="CurlField",  
        default_active=False,
    ),
    dict(zip(curl_amplitudes_test, curl_amplitudes_test)),
))

# Make smaller versions of the tasks for visualization.
tasks_small = jt.map(
    lambda task: eqx.tree_at(
        lambda task: (
            task.eval_grid_n,
            task.eval_n_directions,
            task.eval_reach_length,
        ),
        task, 
        (
            eval_grid_n_small, 
            eval_n_directions_small, 
            eval_reach_length_small,
        ),
    ),
    tasks,
    is_leaf=is_module,
)
``` 

And for convenience:

```{python}
trial_specs = task.validation_trials
trial_specs_small = jt.leaves(tasks_small, is_leaf=is_module)[0].validation_trials

pos_endpoints = jnp.stack([
    trial_specs.inits['mechanics.effector'].pos, 
    trial_specs.targets['mechanics.effector.pos'].value[:, -1],
], axis=0)

pos_endpoints_small = jnp.stack([
    trial_specs_small.inits['mechanics.effector'].pos, 
    trial_specs_small.targets['mechanics.effector.pos'].value[:, -1],
], axis=0)
```

## Evaluate the trained models on each evaluation task

Evaluate each condition (reach direction) several times, to see how performance varies with noise.

```{python}
n_trials = 5

keys_eval = jr.split(key_eval, n_trials)
```

In particular, for each trained ensemble of models (i.e. each training condition), evaluate each model in the ensemble on `n_trials` repetitions of each of the reach conditions. 

```{python}
def get_eval_ensemble(models, task):
    def eval_ensemble(key):
        return task.eval_ensemble(
            models,
            n_replicates=hyperparameters['n_replicates'],
            ensemble_random_trials=False,
            key=key,
        )
    return eval_ensemble

all_states = jt.map(
    lambda task: jt.map(
        lambda models: eqx.filter_vmap(get_eval_ensemble(models, task))(keys_eval),
        trained_models,
        is_leaf=is_module,
    ),
    tasks,
    is_leaf=is_module,
)

all_states_small = jt.map(
    lambda task: jt.map(
        lambda models: eqx.filter_vmap(get_eval_ensemble(models, task))(keys_eval),
        trained_models,
        is_leaf=is_module,
    ),
    tasks_small,
    is_leaf=is_module,
)
```

Note that we:

1. Set `ensemble_random_trials=False` so that each model in the ensemble will be evaluated on the same set of trials. 
2. Vmap over `keys_eval`, to obtain `n_trials` different evaluations of the center-out set. 
3. Use `jt.map` to repeat for each training condition (entry in `trained_models`).

The objects `all_states` and `all_states_small` are dicts whose keys are the `curl_amplitudes_test`, and whose values are dicts whose keys are the `curl_stds` used in training, and whose values are the PyTrees of state arrays.

Each state array has the following batch dimensions: `(n_trials, n_replicates, n_conditions, n_steps)`. For a single center-out set in `all_states_small`, `n_conditions = eval_n_directions`. But since `eval_grid_n != 1` for the full tasks evaluated in `all_states`, then there will be `eval_grid_n ** 2` center-out sets and the third dimension will have size `eval_n_directions * eval_grid_n ** 2`.

```{python}
pp(all_states)
```

TODO: Instead of a two-layer `jt.map`, we could instead define a function (to schedule the intervenor given a `curl_std` and then `jt.map` the vmapped `eval_ensemble`) and then vmap it over `curl_stds_test` to get a single-level dict with a bit less overhead.

## Plot example center-out sets

i.e. show trials for multiple reach directions, to show performance across conditions at a glance, for a single model

Note that we use `states_small` here, and only plot a single center-out set of 7 reach directions.

```{python}
fig_subdir = "center_out_set_examples"
```

### TODO: use `trajectories_2D` and plot all replicates + trials at once, with mean trajectories

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

```{python}
# plot_states = tree_take_multi(all_states_small, [i_replicate], [1])

# figs = jt.map(
#     lambda states: fbp.trajectories_2D(
#         where_plot(states),
#         var_labels=var_labels,
#         axes_labels=('x', 'y'),
#         mode='lines',
#         # colorscale_axis=0,
#         trace_kws=dict(line_width=1),
#     ),
#     plot_states,
#     is_leaf=is_module,
# )
```

### A single trial set, for a single replicate

```{python}
i_replicate = 0
```

```{python}
i_trial = 0

figs = jt.map(
    lambda states: fbp.effector_trajectories(
        tree_take_multi(states, [i_trial, i_replicate], [0, 1]),
        trial_specs=trial_specs_small,
    ),
    all_states_small,
    is_leaf=is_module,
)

for eval_curl_amplitude, figs_ in figs.items():
    for train_curl_std, fig in figs_.items():
        filename = '_'.join([
                f"eval-curl-{eval_curl_amplitude}",
                f"train-curl-std-{train_curl_std}",
                f"single-replicate-{i_replicate}",
                "single-trial",
        ])
        savefig(fig, filename, subdir=fig_subdir)
        
figs[0.5][0.5].show()
```

### All trials for a replicate

```{python}
figs = jt.map(
    lambda states: fbp.effector_trajectories(
        tree_take_multi(states, [i_replicate], [1]),
        trial_specs=trial_specs_small,
        mode='lines',
        trace_kwargs=dict(line_width=0.5),
    ),
    all_states_small,
    is_leaf=is_module,
)

for eval_curl_amplitude, figs_ in figs.items():
    for train_curl_std, fig in figs_.items():
        savefig(
            fig, 
            '_'.join([
                f"eval-curl-{eval_curl_amplitude}",
                f"train-curl-std-{train_curl_std}",
                f"single-replicate-{i_replicate}",
            ]),
            subdir=fig_subdir,
        )

figs[0.5][0.5].show()
```

### All trials for all replicates

TODO: once the previous two headings are converted to use `trajectories_2D`

### Side-by-side comparison of position trajectories across training conditions

i.e. show center-out sets for model trained without curl vs. with strongest curl

TODO: one comparison require a single call to `trajectories_2D`

## Plot example trajectories for a single condition

i.e. for a single reach direction, compare multiple trials/replicates different training conditions; show how training on different curl strengths affects response.

```{python}
fig_subdir = "single_condition_examples"
```

```{python}
i_condition = 0

plot_states = tree_take_multi(all_states_small, [i_condition], [2])

pos_endpoints_i = pos_endpoints_small[:, i_condition]
```

```{python}
var_labels = ('Position', 'Velocity', 'Control force')

where_plot = lambda states: (
    states.mechanics.effector.pos,
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

All trials and replicates for a given condition, indexing by trial:

```{python}
figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        axes_labels=('x', 'y'),
        mode='lines',
        legend_title="Trial",
        colorscale_axis=0,
        scatter_kws=dict(line_width=0.5),
    ),
    plot_states,
    is_leaf=is_module,
)

# Train without disturbances; evaluate with 0.5 curl
figs[0.5][0].show()
```

Now compare the test curl magnitudes, for each training condition:

```{python}
# Stack the states for different test curls into single arrays
plot_states_stacked = tree_stack(plot_states.values())

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        mean_trajectory_line_width=3,
        axes_labels=('x', 'y'),
        mode='lines',
        colorscale='Viridis',
        colorscale_axis=0,
        legend_title="Curl magnitude",
        legend_labels=curl_amplitudes_test,
        scatter_kws=dict(line_width=0.5, opacity=0.3),
    ),
    plot_states_stacked,
    is_leaf=is_module,
)

# Trained on curl std 0.5
figs[0.5].show()
```

And compare the training conditions, for each test curl magnitude:

```{python}
# Only plot a subset of the training conditions
train_curl_stds_plot = [0, 0.5]

# Stack the states for different training curl stds into single arrays
# (also taking a subset of the training conditions with `subdict`)
plot_states_stacked = {
    test_curl_mag: tree_stack(subdict(s, train_curl_stds_plot).values())
    for test_curl_mag, s in plot_states.items()
}

figs = jt.map(
    lambda states: fbp.trajectories_2D(
        where_plot(states),
        var_labels=var_labels,
        # ref_endpoints=(pos_endpoints, None),
        mean_trajectory_line_width=3,
        darken_mean=0.7,
        var_endpoint_ms=0,
        axes_labels=('x', 'y'),
        mode='lines',
        colorscale_axis=0,
        legend_title='Train curl std.',
        legend_labels=train_curl_stds_plot,
        scatter_kws=dict(line_width=1, opacity=0.3),
    ),
    plot_states_stacked,
    is_leaf=is_module,
)

ms_init = ms_goal = 10

for k, fig in figs.items():
    fig.update_layout(title=f"Responses to magnitude {k} curl field")
    
    # TODO: make this into a function that works with subplots
    for j, (label, (ms, symbol)) in enumerate(
        {
            "Start": (ms_init, 'square'),
            "Goal": (ms_goal, 'circle'),
        }.items()
    ):
        fig.add_traces(
            [
                go.Scatter(
                    name=f"{label}",
                    meta=dict(label=label),
                    legendgroup=label,
                    hovertemplate=f"{label}<extra></extra>",
                    x=pos_endpoints_i[j, 0][None],
                    y=pos_endpoints_i[j, 1][None],
                    mode="markers",
                    marker=dict(
                        size=ms/2,
                        symbol=symbol,
                        color='rgb(25, 25, 25)',
                        # color='rgba(255, 255, 255, 0)',
                        line=dict(
                            color='rgb(25, 25, 25)',
                            width=2,
                        ),
                    ),
                    xaxis="x1",
                    yaxis="y1",
                    # TODO: Show once in legend, for all markers of type j
                    showlegend=True,
                )
            ]
        )
        
    fig.show()
```

## Compare velocity profiles

```{python}
fig_subdir = "velocity_profiles"
```

Average over trials, directions, and replicates to get an average velocity profile + error bands for each training condition. 

Note that it only makes sense to compare across reach directions if we consider the velocity profiles along the respective directions. For example, the y-profile in one direction should be comparable to the x-profile in an orthogonal direction.

There are two ways we might get profiles for comparison.

1. Calculate speed as the magnitude of the velocity vector. This is simpler.
2. Project the velocity profiles onto the reach direction.

These are not identical, and the second method is more flexible because we still retain the components and their signs. 

### Speed profiles

```{python}
all_speeds = jt.map(
    lambda states: jnp.linalg.norm(states.mechanics.effector.vel, axis=-1),
    all_states,
    is_leaf=is_module,
)

figs = {
    k: fbp.profiles(speeds, mode='std', varname="Speed")
    for k, speeds in all_speeds.items()
}

figs[0.5].show()
```

### Forward and lateral velocity profiles

TODO: Plot on shared time axis

```{python}
forward_vels, lateral_vels = tree_unzip(jt.map(
    lambda state: forward_lateral_vels(state.mechanics.effector.vel, task.validation_trials),
    all_states,
    is_leaf=is_module,
))
```

```{python}
for eval_curl_amplitude in curl_amplitudes_test:
    for vel_type, vels in zip(
        ("Forward", "Lateral"), 
        (forward_vels, lateral_vels),
    ):
        fig = fbp.profiles(
            vels[eval_curl_amplitude], 
            varname=f"{vel_type} velocity",
            legend_title="Train curl std.",
            mode='std', # or 'curves'
            n_std_plot=1,
            # stride_curves=500,
            # curves_kws=dict(opacity=0.7),
        )
        fig.update_layout(title=f"Response to magnitude {eval_curl_amplitude} curl field")
        filename = f"eval-curl-{eval_curl_amplitude}_{vel_type.lower()}-vels"
        savefig(fig, filename, subdir=fig_subdir)
        
        fig.show()
```

## Summary comparison of performance measures 


### Max speed

```{python}
def max_speed(velocity):
    return jnp.max(jnp.sqrt(jnp.sum(velocity ** 2, axis=-1)), axis=-1)
```

```{python}
all_max_speeds = jax.tree_map(
    lambda x: max_speed(x.mechanics.effector.vel),
    all_states,
    is_leaf=is_module,
)
```

```{python}
import pandas as pd
import seaborn as sns
import plotly.express as px

for eval_curl_amplitude, max_speeds in all_max_speeds.items():
    df = pd.DataFrame(
        jt.map(lambda x: x.flatten(), max_speeds)
    ).melt(value_name='max_speed')
    fig1 = px.box(df, x='variable', y='max_speed', points='all')
    fig2 = px.violin(df, x='variable', y='max_speed', color='variable')
    fig3 = px.violin(df[df['variable'].isin([0, 0.5])], y='max_speed', color='variable', violinmode='overlay')
    fig3.show()
    # fig, ax = plt.subplots()
    # sns.boxplot(ax=ax, data=df, x='variable', y='max_speed', color='grey')
    # ax.set_xlabel('')
    # ax.set_ylabel('Max speed')
    # ax.set_xticks(list(range(len(max_speeds))), list(max_speeds.keys()))
    # plt.show()
```

```{python}
for eval_curl_amplitude, max_speeds in all_max_speeds.items():
    fig = go.Figure()
    for train_curl_std, max_speed in max_speeds.items():
        fig.add_trace(
            go.Violin(
                y=max_speed.flatten(),
                name=train_curl_std,
                box_visible=False,
                meanline_visible=True,
            )
        )
    fig.show()
```

TODO: grouped/split violin plot (https://plotly.com/python/violin/#grouped-violin-plot) for comparing responses across models at the same time as comparing responses to different curl magnitudes; e.g. split shows with/without curl field 

### Max forward velocity

### Max positive and negative lateral velocities

### Max and sum of lateral deviations

### Endpoint error

Final deviation/mean over last N steps

### 





