---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

# Analysis of plant perturbations

In this notebook we begin our analysis of the models trained in the presence of different levels of curl force fields:
- examine differences in baseline performance and state profiles of the models; e.g. differences in velocity profiles
- compare robustness to mechanical disturbances; e.g. differences in endpoint error when subject to large curl fields; 

```{python}
NB_ID = "1-1"

TRAIN_NB_ID = "1"
```

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import OrderedDict, namedtuple
from functools import partial
from typing import Any, Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import numpy as np
import plotly
import plotly.graph_objects as go
from tqdm.auto import tqdm

import feedbax
from feedbax import (
    is_module, 
    is_type,
    load, 
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.intervene import (
    CurlField, 
    FixedField, 
    add_intervenors, 
    schedule_intervenor,
)
import feedbax.plotly as fbp

import rnns_learn_robust_motor_policies
from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.constants import (
    INTERVENOR_LABEL,
    POS_ENDPOINTS_ALIGNED,
)
from rnns_learn_robust_motor_policies.database import (
    ModelRecord,
    add_evaluation,
    add_evaluation_figure,
    get_db_session, 
    get_model_record,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies.measures import (
    MEASURES, 
    MEASURE_LABELS,
    RESPONSE_VAR_LABELS,
    Measure,
    Responses,
    compute_all_measures,
    output_corr,
)
from rnns_learn_robust_motor_policies.misc import lohi, log_version_info
from rnns_learn_robust_motor_policies.train_setup_part1 import setup_task_model_pair
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.plot import (
    add_endpoint_traces,
    get_measure_replicate_comparisons,
    get_violins,
)
from rnns_learn_robust_motor_policies.plot_utils import figleaves
from rnns_learn_robust_motor_policies.setup_utils import (
    get_base_task,
    convert_tasks_to_small,
    query_and_load_model,
    set_model_noise,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_aligned_vars,
    get_pos_endpoints,
    orthogonal_field,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
    tree_subset_dict_level,
)
from rnns_learn_robust_motor_policies.types import PertAmpDict, TrainStdDict
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
version_info = log_version_info(
    jax, eqx, plotly, git_modules=(feedbax, rnns_learn_robust_motor_policies)
)
```

### Initialize model database connection

```{python}
db_session = get_db_session()  # defaults to "DB_DIR/main.db"
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
disturbance_stds_load = [0, 0.5, 1.0, 1.5]
disturbance_type_load: Literal['curl', 'constant'] = 'curl'
feedback_noise_std_load = 0.01
motor_noise_std_load = 0.01
feedback_delay_steps_load = 0
hidden_size = 50
where_train_strs = ["step.net.hidden", "step.net.readout"]
readout_norm_loss_weight = 0.0
readout_norm_value = 2.0
    
# Specify model parameters to use for analysis (None -> use training value)
disturbance_type: Optional[Literal['curl', 'constant']] = None
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

```{python}
params_load_func = lambda disturbance_std_load: dict(
    origin=TRAIN_NB_ID,
    disturbance_std=disturbance_std_load,
    disturbance_type=disturbance_type_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps_load,
    hidden_size=hidden_size,
    # where_train_strs=where_train_strs,
    # readout_norm_loss_weight=readout_norm_loss_weight,
    # readout_norm_value=readout_norm_value,
    # intervention_scaleup_batches=[0,0],
    # n_batches=10000,
)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

```{python}
replicate_criterion = "best_total_loss"

models_base, model_info, replicate_info, n_replicates_included = tree_unzip(TrainStdDict({
    disturbance_std: query_and_load_model(
        db_session,
        setup_task_model_pair,
        params_query=params_load_func(disturbance_std),
        noise_stds=dict(
            feedback=feedback_noise_std,
            motor=motor_noise_std,
        ),
        exclude_underperformers_by=replicate_criterion,
    )
    for disturbance_std in disturbance_stds_load
}))

best_replicate, included_replicates = tree_unzip(TrainStdDict({
    std: (
        replicate_info[std]['best_replicates'][replicate_criterion],
        replicate_info[std]['included_replicates'][replicate_criterion],
    ) 
    for std in disturbance_stds_load
}))
```

## Sort out the evaluation parameters and create a database record

If disturbance type or noise scales were not specified earlier, take them to be the same as those used during training of the model. 

```{python} 
all_eval_parameters = jt.map(
    lambda record: use_record_params_where_none(
        dict(
            disturbance_type=disturbance_type,
            feedback_noise_std=feedback_noise_std,
            motor_noise_std=motor_noise_std,
        ), 
        record,
    ),
    model_info,
    is_leaf=is_type(ModelRecord),
)
```

All the relevant model info (e.g. noise stds) except disturbance std should be the same across models, at this point. Assert that this is the case, and keep the parameters for only one of the models.

```{python}
all_eval_params_flat =[tuple(d.items()) for d in all_eval_parameters.values()]

assert len(set(all_eval_params_flat)) == 1

eval_parameters = all_eval_parameters[disturbance_stds_load[0]]

# Later, use this to access the values of hyperparameters, assuming they 
# are shared between models (e.g. `model_info_0.n_steps`)
model_info_0 = model_info[disturbance_stds_load[0]]
```

Are any of the system noise scales non-zero?

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Disturbance amplitudes

Specify the magnitudes of the disturbance.

<!-- TODO: This shouldn't be here, but with the rest of the hyperparameters. -->

```{python}
disturbance_amplitudes = {
    'curl': [0.0, 0.5, 1.0, 2.0, 4.0],
    'constant': [0.0, 0.05, 0.1, 0.2, 0.4],
}[eval_parameters['disturbance_type']]
```

### Number of evaluations per model and condition

We'll evaluate each condition (reach direction) several times, to see how performance varies with noise.

```{python}
n_evals = dict(
    full=5,
    small=5,
)

if not any_system_noise:
     n_evals = jt.map(lambda _: 1, n_evals)
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    disturbance_amplitudes=disturbance_amplitudes,
    n_evals=n_evals['full'],
    n_evals_small=n_evals['small'],
)
```

## Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    origin=NB_ID,
    models=model_info,
    eval_parameters=eval_parameters,
    version_info=version_info,
)
```

## Set up evaluation tasks 

We'll evaluate on a range of constant curl amplitudes, starting from 0 (no curl/control).

```{python}
if eval_info.disturbance_type == 'curl':   
    def disturbance(amplitude):
        return CurlField.with_params(amplitude=amplitude)    
        
elif eval_info.disturbance_type == 'constant':    
    def disturbance(amplitude):           
        return FixedField.with_params(
            scale=amplitude,
            field=orthogonal_field,  
        ) 
          
else:
    raise ValueError(f"Unknown disturbance type: {eval_info.disturbance_type}")
```

```{python}
task_base = get_base_task(model_info_0.n_steps)
```

```{python}
# Insert the disturbance field component into each model
models = jt.map(
    lambda models: add_intervenors(
        models,
        lambda model: model.step.mechanics,
        # The first key is the model stage where to insert the disturbance field;
        # `None` means prior to the first stage.
        # The field parameters will come from the task, so use an amplitude 0.0 placeholder.
        {None: {INTERVENOR_LABEL: disturbance(0.0)}},
    ),
    models_base,
    is_leaf=is_module,
)

all_tasks = dict()

# Generate tasks with different amplitudes of disturbance field
all_tasks['full'], _ = tree_unzip(jt.map(
    lambda disturbance_amplitude: schedule_intervenor(
        task_base, models[0],
        lambda model: model.step.mechanics,
        disturbance(disturbance_amplitude),
        label=INTERVENOR_LABEL,  
        default_active=False,
    ),
    PertAmpDict(zip(disturbance_amplitudes, disturbance_amplitudes)),
))

# Make smaller versions of the tasks for visualization.
all_tasks['small'] = convert_tasks_to_small(all_tasks['full'])
``` 

And for convenience:

```{python}
example_task = {
    key: jt.leaves(tasks, is_leaf=is_module)[0]
    for key, tasks in all_tasks.items()
}

trial_specs = jt.map(lambda task: task.validation_trials, example_task, is_leaf=is_module)

pos_endpoints = jt.map(get_pos_endpoints, trial_specs, is_leaf=is_module)
```

## Setup colors for plots

Now that we know how many training and evaluation conditions we'll be working with, we can define the color scales for plots once and for all.

```{python}
trials_colors, trials_colors_dark = get_colors_dicts(
    range(n_evals['full']), COLORSCALES['trials'],
)

# by training condition
disturbance_train_stds_colors, disturbance_train_stds_colors_dark = get_colors_dicts(
    disturbance_stds_load, COLORSCALES['disturbance_train_stds'],
)

# by evaluation condition
disturbance_amplitudes_colors, disturbance_amplitudes_colors_dark = get_colors_dicts(
    disturbance_amplitudes, COLORSCALES['disturbance_amplitudes'], 
)
```

## Evaluate the trained models on each evaluation task

In particular, for each trained ensemble of models (i.e. each training condition), evaluate each model in the ensemble on `n_evals` repetitions of each of the reach conditions, for each of the task variants. 

First, define the full evaluation as a function so that we can estimate the memory needed for the result. 

```{python}
def evaluate_all_states(models, tasks, n_evals):
    return jt.map( # Map over task variants
        lambda task: jt.map(  # Map over training conditions (`models` entries)
            lambda models: vmap_eval_ensemble(models, task, n_evals, key_eval),
            models,
            is_leaf=is_module,
        ),
        tasks,
        is_leaf=is_module,
    )
```

```{python}
all_states_bytes = (
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, models, all_tasks['full'], n_evals['full'])),
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, models, all_tasks['small'], n_evals['small'])),
)

print(f"\n{sum(all_states_bytes) / 1e9:.2f} GB of memory estimated to store all states.")
```

Now actually evaluate the states:

```{python}
# Evaluate all task variants (full and small)
all_states = jt.map(
    lambda n, tasks: evaluate_all_states(models, tasks, n),
    n_evals, all_tasks,
)
```

Each state array has the following batch dimensions: `(n_evals, n_replicates, n_conditions, n_steps)`. For a single center-out set in `all_states['small']`, `n_conditions = eval_n_directions`. But since `eval_grid_n != 1` for the full tasks evaluated in `all_states['full']`, then there will be `eval_grid_n ** 2` center-out sets and the third dimension will have size `eval_n_directions * eval_grid_n ** 2`.

```{python}
pp(all_states)
```

TODO: Instead of a two-layer `jt.map`, we could instead define a function (to schedule the intervenor given a `disturbance_amplitude` and then `jt.map` the vmapped `eval_ensemble`) and then vmap it over `disturbance_amplitudes` to get a single-level dict with a bit less overhead.

### Project positions, velocities, and forces into reach direction

In other words, change from x/y components to parallel/orthogonal components, relative to a straight reach.

```{python}
where_vars_to_align = lambda states, pos_endpoints: Responses(
    # Positions with respect to the origin
    states.mechanics.effector.pos - pos_endpoints[0][..., None, :],
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

```{python}   
aligned_vars = {
    variant: jt.map(
        lambda all_states: get_aligned_vars(all_states, where_vars_to_align, pos_endpoints[variant]),
        all_states[variant],
        is_leaf=is_module,
    )
    for variant in all_states
}
```

## Plot example center-out sets

i.e. show trials for multiple reach directions, to show performance across conditions at a glance, for a single model

Note that we plot the small variant of the task here, with only a single center-out set.

```{python}
plot_id = "center_out_sets"
```

```{python}
if not any_system_noise:
    var_labels = ('Position', 'Velocity', 'Control force')
    where_plot = lambda states: (
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
        states.efferent.output,
    )
else:
    var_labels = ('Position', 'Velocity')
    # Forces are very messy when there's noise,
    # and we'll visualize the aligned forces anyway
    where_plot = lambda states: (
        states.mechanics.effector.pos,
        states.mechanics.effector.vel,
    )
```

```{python}
plot_trajectories = lambda states, *args, **kwargs: fbp.trajectories_2D(
    where_plot(states),
    var_labels=var_labels,
    axes_labels=('x', 'y'),
    colorscale=COLORSCALES['reach_condition'],
    legend_title='Reach direction',
    # scatter_kws=dict(line_width=0.5),
    layout_kws=dict(
        width=100 + len(var_labels) * 300,
        height=400,
        legend_tracegroupgap=1,
    ),
    *args, 
    **kwargs,
)
```

### All trials for a replicate

```{python}
plot_id = "center_out_sets/all_evals_single_replicate"
```

```{python}
# If None, plot the replicate with the lowest total training loss
i_replicate = None
```

```{python}
plot_states = jt.map(
    lambda states_by_std: TrainStdDict({
        std: tree_take(
            states, 
            best_replicate[std], 
            axis=1,
        )
        for std, states in states_by_std.items()
    }),
    all_states['small'],
    is_leaf=is_type(TrainStdDict),
)
```

```{python}
if not any_system_noise:
    print("Skipping center-out sets that compare different evaluations, for this zero-noise condition")
else:       
    figs = jt.map(
        partial(
            plot_trajectories, 
            curves_mode='lines', 
            colorscale_axis=1, 
            mean_trajectory_line_width=2.5,
            darken_mean=MEAN_LIGHTEN_FACTOR,
            scatter_kws=dict(line_width=0.5),
        ),
        plot_states,
        is_leaf=is_module,
    )
    
    for disturbance_amplitude in figs:
        for disturbance_train_std, fig in figs[disturbance_amplitude].items():
            if i_replicate is None:
                i_rep = best_replicate[disturbance_train_std]
            else:
                i_rep = i_replicate
            
            # Plot the reach endpoints on the effector position subplot
            # add_endpoint_traces(
            #     fig, pos_endpoints['small'], xaxis='x1', yaxis='y1', colorscale='phase'
            # )
            
            fig_parameters = dict(
                disturbance_train_std=disturbance_train_std,
                disturbance_amplitude=disturbance_amplitude,
                n=n_evals['small'],
                i_model_replicate=i_rep,
            )
            
            add_evaluation_figure(
                db_session,
                eval_info,
                fig,
                plot_id,
                model_records=model_info,
                **fig_parameters,
            )

    # Only display figures for the low-high train and eval conditions.        
    for disturbance_amplitude in lohi(disturbance_amplitudes):
        for disturbance_std in lohi(disturbance_stds_load):
            figs[disturbance_amplitude][disturbance_std].show()
```

### A single trial set, for a single replicate

```{python}
plot_id = "center_out_sets/single_eval_single_replicate"
```

```{python}
i_trial = 0

plot_states_i = tree_take(plot_states, i_trial, 0)

figs = jt.map(
    partial(
        plot_trajectories, 
        mode='markers+lines', 
        ms=3,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states_i,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_std, fig in figs[disturbance_amplitude].items():
        if i_replicate is None:
            i_rep = best_replicate[disturbance_std]
        else:
            i_rep = i_replicate
        
        # add_endpoint_traces(
        #     fig, pos_endpoints['small'], xaxis='x1', yaxis='y1', colorscale='phase'
        # )
        
        fig_parameters = dict(
            disturbance_train_std=disturbance_std,
            disturbance_amplitude=disturbance_amplitude,
            i_model_replicate=i_rep,
            i_random_trial=i_trial,
        )
        
        add_evaluation_figure(
            db_session,
            eval_info,
            fig,
            plot_id,
            model_records=model_info,
            **fig_parameters,
        )
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    for disturbance_std in lohi(disturbance_stds_load):
        figs[disturbance_amplitude][disturbance_std].show()
```

### A single trial, for all replicates

```{python}
plot_id = "center_out_sets/single_eval_all_replicates"
```

```{python}
plot_states = tree_take_multi(all_states['small'], [i_trial], [0])

figs = jt.map(
    partial(
        plot_trajectories, 
        curves_mode='lines', 
        colorscale_axis=1, 
        mean_trajectory_line_width=2.5,
        darken_mean=MEAN_LIGHTEN_FACTOR,
        scatter_kws=dict(line_width=0.75),
    ),
    plot_states,
    is_leaf=is_module,
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_std, fig in figs[disturbance_amplitude].items():       
        # add_endpoint_traces(
        #     fig, pos_endpoints['small'], visible=[False, True], xaxis='x1', yaxis='y1', colorscale='phase'
        # )
        
        fig_parameters = dict(
            disturbance_train_std=disturbance_std,
            disturbance_amplitude=disturbance_amplitude,
            n=n_replicates_included[disturbance_train_std],
            i_random_trial=i_trial,
        )
        
        add_evaluation_figure(
            db_session,
            eval_info,
            fig,
            plot_id,
            model_records=model_info,
            **fig_parameters,
        )
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    for disturbance_std in lohi(disturbance_stds_load):
        figs[disturbance_amplitude][disturbance_std].show()
```

## Plot aligned trajectories

i.e. for a single reach direction, compare multiple trials/replicates different training conditions; visualize how training on different disturbance strengths affects response.

```{python}
plot_id = "aligned_to_reach_condition"
```

```{python}
n_curves_max = 20

plot_condition_trajectories = partial(
    fbp.trajectories_2D,
    var_labels=RESPONSE_VAR_LABELS,
    axes_labels=('x', 'y'),
    # mode='std',
    mean_trajectory_line_width=3,
    n_curves_max=n_curves_max,
    darken_mean=MEAN_LIGHTEN_FACTOR,
    layout_kws=dict(
        width=900,
        height=400,
        legend_tracegroupgap=1,
        margin_t=75,
    ),
    scatter_kws=dict(
        line_width=1, 
        opacity=0.6,
    ),
)
```

### All trials and replicates for a given train-test condition, indexing by trial

```{python}
plot_id = "aligned_to_reach_condition/per_train_test_pair"
```

```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        legend_title="Trial",
        colorscale=COLORSCALES['trials'],
        colorscale_axis=0, 
        curves_mode='lines', 
    ),
    aligned_vars['small'],
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_amplitude in tqdm(figs):
    for disturbance_std, fig in figs[disturbance_amplitude].items():
        n_conditions = all_tasks['small'][disturbance_amplitude].n_validation_trials
        n_dist = n_replicates_included[disturbance_std] * n_conditions
        
        # add_endpoint_traces(fig, POS_ENDPOINTS_ALIGNED['small'], xaxis='x1', yaxis='y1')
        
        fig_parameters = dict(
            disturbance_train_std=disturbance_std,
            disturbance_amplitude=disturbance_amplitude,
            n=min(n_dist, n_curves_max),
        )
        
        add_evaluation_figure(
            db_session,
            eval_info,
            fig,
            plot_id,
            model_records=model_info,
            **fig_parameters,
        )
```

Just show a single example, so we'll know if something is really amiss. But these plots are not very interesting except to show the overall variation for a given noise condition.

```{python}
figleaves(figs)[-1].show()
# for disturbance_amplitude in lohi(disturbance_amplitudes):
#     for disturbance_train_std in lohi(disturbance_train_stds):
#         figs[disturbance_amplitude][disturbance_train_std].show()
```

### Compare disturbance amplitudes, for each trained disturbance std

```{python}
plot_id = "aligned_to_reach_condition/compare_test_conditions"
```

```{python}
# Stack the states for different disturbance amplitudes into single arrays
plot_vars_stacked = tree_stack(aligned_vars['small'].values())

figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['disturbance_amplitudes'],
        colorscale_axis=0,
        legend_title="Field<br>amplitude",
        legend_labels=disturbance_amplitudes,
        curves_mode='lines',
    ),
    plot_vars_stacked,
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_train_std, fig in tqdm(figs.items()):
    n_conditions = all_tasks['small'][disturbance_amplitude].n_validation_trials
    n_trials = n_evals['small'] * n_conditions * n_replicates_included[disturbance_train_std]
    
    # add_endpoint_traces(fig, POS_ENDPOINTS_ALIGNED['small'], xaxis='x1', yaxis='y1')
      
    fig_parameters = dict(
        disturbance_train_std=disturbance_train_std,
        n=min(n_dist, n_curves_max),
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        model_records=model_info,
        **fig_parameters,
    )
```

```{python}
for disturbance_std in lohi(disturbance_stds_load):
    figs[disturbance_std].show()
```

### Compare the trained disturbance stds, for each test disturbance amplitude

```{python}
plot_id = "aligned_to_reach_condition/compare_train_conditions"
```

```{python}
# Only plot a subset of the training conditions
# disturbance_train_stds_plot = [
#     disturbance_train_stds[0],
#     disturbance_train_stds[len(disturbance_train_stds) // 2],
#     disturbance_train_stds[-1],
# ]
disturbance_stds_plot = disturbance_stds_load
stride = 1

# Stack the states for different training disturbance stds into single arrays
# (also taking a subset of the training conditions with `subdict`)
plot_vars_stacked = {
    # concatenate along the replicate axis, which has variable length
    disturbance_amplitude: tree_stack(subdict(vars_, disturbance_stds_plot).values())
    for disturbance_amplitude, vars_ in aligned_vars['small'].items()
}
```
```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['disturbance_train_stds'],
        colorscale_axis=0,
        stride=stride,
        legend_title="Train<br>field std.",
        legend_labels=disturbance_stds_plot,
        curves_mode='lines',
        var_endpoint_ms=0,
        scatter_kws=dict(line_width=0.5, opacity=0.3),
        # ref_endpoints=(pos_endpoints['full'], None),
    ),
    plot_vars_stacked,
    is_leaf=is_type(Responses),
)
```

```{python}
for disturbance_amplitude, fig in tqdm(figs.items()):    
    # add_endpoint_traces(fig, POS_ENDPOINTS_ALIGNED['small'], xaxis='x1', yaxis='y1')

    fig_parameters = dict(
        disturbance_amplitude=disturbance_amplitude,
        # TODO: The number of replicates (`n_replicates_included`) may vary with the disturbance train std!
        # n=min(n_evals['small'] * model_info_0.n_replicates, n_curves_max),
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        model_records=model_info,
        **fig_parameters,
    )
```

```{python}
for disturbance_amplitude in lohi(disturbance_amplitudes):
    figs[disturbance_amplitude].show()
```

## Compare velocity profiles

```{python}
plot_id = "velocity_profiles"
```

Average over trials, directions, and replicates to get an average velocity profile + error bands for each training condition. 

Note that it only makes sense to compare across reach directions if we consider the velocity profiles along the respective directions. For example, the y-profile in one direction should be comparable to the x-profile in an orthogonal direction.

### Forward and lateral velocity profiles

**TODO: Plot on shared time axis**

```{python}
aligned_vel = jt.map(
    lambda responses: responses.vel,
    aligned_vars['full'],
    is_leaf=is_type(Responses),
)
```

```{python}
n_dist = int(np.prod(jt.leaves(aligned_vel)[0].shape[:-2]))

figs = {
    disturbance_amplitude: {
        label: fbp.profiles(
            tree_take(aligned_vel, i, -1)[disturbance_amplitude],
            varname=f"{label} velocity",
            legend_title="Train<br>field std.",
            mode='std', # or 'curves'
            n_std_plot=1,
            hline=dict(y=0, line_color="grey"),
            colors=list(disturbance_train_stds_colors_dark.values()),
            # stride_curves=500,
            # curves_kws=dict(opacity=0.7),
            layout_kws=dict(
                width=600,
                height=400,
                legend_tracegroupgap=1,
            ),
        )
        for i, label in enumerate(("Forward", "Lateral"))
    }
    for disturbance_amplitude in disturbance_amplitudes
}

for disturbance_amplitude in tqdm(disturbance_amplitudes):
    for i, label in enumerate(("Forward", "Lateral")):
        fig = figs[disturbance_amplitude][label]

        fig_parameters = dict(
            disturbance_amplitude=disturbance_amplitude,
            direction=label.lower(),
            n=n_dist,
        )
        
        add_evaluation_figure(
            db_session,
            eval_info,
            fig,
            plot_id,
            **fig_parameters,
        )
        
for disturbance_amplitude in lohi(disturbance_amplitudes):
    figs[disturbance_amplitude]["Forward"].show()
    figs[disturbance_amplitude]["Lateral"].show()
```

## Summary comparison of performance measures 

```{python}
plot_id = "performance_measures"
```

### Calculate all measures

Select which measures to compute. These are conveniently pre-defined as instances of `Measure` in 
`rnns_learn_robust_motor_policies.measures`, and we just need to select which ones we want from `MEASURES`.

```{python}
measure_keys = [
    "max_parallel_vel_forward",
    "max_orthogonal_vel_left",
    "max_orthogonal_vel_right",
    "max_orthogonal_distance_left",
    "sum_orthogonal_distance",
    "end_position_error",
    "end_velocity_error",
    "max_parallel_force_forward",
    "sum_parallel_force",
    "max_orthogonal_force_right",  
    "sum_orthogonal_force_abs",
    "max_net_force",
    "sum_net_force",
]

all_measures = subdict(MEASURES, measure_keys)
```

```{python}   
all_measure_values = compute_all_measures(all_measures, aligned_vars['full'])

# TODO: fix the `pos_endpoints` hardcoding, above
# all_measures_small = compute_all_measures(all_measure_funcs, all_states['small'])
```

### Plot measure distributions by training condition, across evaluation conditions

```{python}
plot_id = "performance_measures/compare_train_conditions"
```

One plot per evaluation condition (disturbance amplitude); one violin per training condition (disturbance std).

Distributions are aggregated over all replicates and trials. 

```{python}
def get_violins_per_measure(measure_values, **kwargs):
    return {
        key: get_violins(
            values, 
            yaxis_title=MEASURE_LABELS[key], 
            xaxis_title="Train field std.",
            **kwargs,
        )
        for key, values in measure_values.items()
    }
    

figs = get_violins_per_measure(
    all_measure_values,
    colors=disturbance_amplitudes_colors_dark,
)
```

```{python}
n_dist = int(np.prod(jt.leaves(all_measure_values)[0].shape)) 

for key in tqdm(all_measures):
    fig_parameters = dict(
        measure_name=key,
        n=n_dist, 
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        figs[key],
        plot_id,
        model_records=model_info,
        **fig_parameters,
    )
```

```{python}
for key in all_measures:
    figs[key].show()
```

### Repeat for just a single reach condition

:::{note}
Omitting this from the output since it's sufficient to see the variation in (e.g.) the velocity profiles to be convinced that there is not huge variation between reach directions/conditions.
:::

```{python}
# plot_id = "performance_measures/compare_train_conditions/single_reach_condition"
```

The distributions should be similar, since the model+task is isotropic.


### Comparison of zero vs. high train disturbance std, for different replicates

**TODO**: Show all replicates, even the excluded ones, but maybe color them differently.

Compare the smallest (zero) and largest training disturbance stds. For the plots above, this would mean keeping just the leftmost and the rightmost violins. However, those plots were aggregated over replicates and trials. Now we would like to examine the variance across model replicates. Thus, for each measure, we will generate one plot for each evaluation condition (disturbance amplitude) as before, but now each containing `n_replicates` *split* violins, where the left half corresponds to the zero training disturbance std, and the right half to the largest training disturbance std.

```{python}
plot_id = "performance_measures/compare_replicates_lowhigh_train_conditions"
```

```{python}
def get_one_measure_plot_per_eval_condition(plot_func, measures, colors, **kwargs):
    return {
        key: PertAmpDict({
            disturbance_amplitude: plot_func(
                measure[disturbance_amplitude], 
                MEASURE_LABELS[key], 
                colors,
                **kwargs,
            )
            for disturbance_amplitude in measure
        })
        for key, measure in measures.items()
    }
```

```{python}   
subset_by_train_stds = partial(tree_subset_dict_level, dict_type=TrainStdDict)

all_measure_values_lohi_stds = subset_by_train_stds(
    all_measure_values,
    lohi(disturbance_stds_load),
)
```

```{python}
replicates_all_lohi_included = jt.reduce(jnp.logical_and, lohi(included_replicates))

figs = get_one_measure_plot_per_eval_condition(
    get_measure_replicate_comparisons, 
    all_measure_values_lohi_stds,
    lohi(disturbance_train_stds_colors_dark),
    included_replicates=np.where(replicates_all_lohi_included)[0],
)
```

```{python}
n_dist = int(np.prod(jt.leaves(all_measure_values_lohi_stds)[0].shape))

for key in tqdm(all_measures):
    for disturbance_amplitude, fig in figs[key].items():

        fig_parameters = dict(
            disturbance_amplitude=disturbance_amplitude,
            measure_name=MEASURE_LABELS[key],
            n=n_dist, 
        )
        
        add_evaluation_figure(
            db_session,
            eval_info,
            fig,
            plot_id,
            model_records=model_info,
            **fig_parameters,
        )
```

```{python}
for key in all_measures:
    figs[key][disturbance_amplitudes[-1]].show()
```

### Summary comparison of no-disturbance vs. high-disturbance evaluation, for no-disturbance vs. high-disturbance training

```{python}
plot_id = "performance_measures/lowhigh_summaries"
```

```{python}
all_measures_lohi = {
    key: subdict(measure, lohi(disturbance_amplitudes))
    for key, measure in all_measure_values_lohi_stds.items()
}
```

```{python}
figs = {
    key: get_violins(
        measure, 
        yaxis_title=MEASURE_LABELS[key], 
        xaxis_title="Train field std.",
        legend_title="smee",
        colors=disturbance_amplitudes_colors_dark,
        layout_kws=dict(
            width=300, height=300, 
        )
    )
    for key, measure in all_measures_lohi.items()
}
```

```{python}
n_dist = int(np.prod(jt.leaves(all_measures_lohi)[0].shape))

for key in tqdm(all_measures):
    fig_parameters = dict(
        measure_name=MEASURE_LABELS[key],
        n=n_dist, 
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        figs[key],
        plot_id,
        model_records=model_info,
        **fig_parameters,
    )
```

```{python}
for key in all_measures:
    figs[key].show()
```

i.e. not one plot for each evaluation condition for each measure, but a single plot for each measure, comparing the distributions for zero vs. high training disturbance std, on zero vs. high evaluation disturbance amplitude.


## Correlation of network activity with output weights

```{python}
plot_id = "performance_measures/compare_train_conditions"
```

```{python}
all_activities = jt.map(
    lambda states: states.net.hidden,
    all_states,
    is_leaf=is_module,
)

all_output_weights = jt.map(
    lambda models: models.step.net.readout.weight,
    models,
    is_leaf=is_module,
)
```


```{python}
all_output_corrs = jt.map(
    lambda activities: TrainStdDict({
        train_std: output_corr(
            activities[train_std], 
            all_output_weights[train_std],
        )
        for train_std in activities
    }),
    all_activities,
    is_leaf=is_type(TrainStdDict),
)
```

```{python}
fig = get_violins(
    all_output_corrs['full'], 
    yaxis_title="Output correlation", 
    xaxis_title="Train field std.",
    colors=disturbance_amplitudes_colors_dark,
)
```


```{python}
n_dist = int(np.prod(jt.leaves(all_output_corrs['full'])[0].shape))

fig_parameters = dict(
    measure_name="output-correlation",
    n=n_dist, 
)

add_evaluation_figure(
    db_session,
    eval_info,
    fig,
    plot_id,
    model_records=model_info,
    **fig_parameters,
)

fig.show()
```