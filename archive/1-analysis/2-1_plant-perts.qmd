---
jupyter: python3
format:
  html:
    toc: true 
execute:
  echo: false
---

```{python}
NB_ID = "2-1"

# TODO: This is clear in the eval_rules file; probably don't need to specify here again
TRAIN_NB_ID = "2"  # Notebook the models were trained in
```

# Analysis of plant perturbations

## Environment setup

```{python}
%load_ext autoreload
%autoreload 2
```

```{python}
import os

os.environ["TF_CUDNN_DETERMINISTIC"] = "1"
```

```{python}
from collections import namedtuple
from functools import partial
from typing import Literal, Optional

import equinox as eqx
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.tree as jt
import numpy as np
import plotly
import plotly.graph_objects as go
from tqdm.auto import tqdm

import feedbax
from feedbax import (
    is_module, 
    is_type,
    load, 
    move_level_to_outside,
    tree_set_scalar,
    tree_stack,
    tree_struct_bytes,
    tree_take, 
    tree_take_multi,
    tree_unzip,
)
from feedbax.intervene import (
    CurlField, 
    FixedField, 
    add_intervenors,
    schedule_intervenor,
)
import feedbax.plotly as fbp
from feedbax.task import TrialSpecDependency

import rnns_learn_robust_motor_policies
from rnns_learn_robust_motor_policies import PROJECT_SEED
from rnns_learn_robust_motor_policies.colors import (
    COLORSCALES, 
    MEAN_LIGHTEN_FACTOR,
    get_colors_dicts,
)
from rnns_learn_robust_motor_policies.constants import (
    INTERVENOR_LABEL,
    POS_ENDPOINTS_ALIGNED,
    REPLICATE_CRITERION,
)
from rnns_learn_robust_motor_policies.database import (
    ModelRecord,
    get_db_session,
    get_model_record,
    add_evaluation,
    add_evaluation_figure,
    use_record_params_where_none,
)
from rnns_learn_robust_motor_policies.measures import (
    MEASURES, 
    MEASURE_LABELS,
    RESPONSE_VAR_LABELS,
    Direction,
    Measure,
    Responses,
    ResponseVar,
    compute_all_measures,
    output_corr,
    signed_max,
)
from rnns_learn_robust_motor_policies.misc import log_version_info, lohi
from rnns_learn_robust_motor_policies.train_setup_part2 import (
    setup_task_model_pair, 
)
from rnns_learn_robust_motor_policies.plot import (
    add_endpoint_traces,
    get_violins,
)
from rnns_learn_robust_motor_policies.plot_utils import (
    PlotlyFigureWidget as PFW,
    figleaves,
    figs_flatten_with_paths,
)
from rnns_learn_robust_motor_policies.post_training import setup_replicate_info
from rnns_learn_robust_motor_policies.setup_utils import (
    get_base_task,
    convert_tasks_to_small,
    query_and_load_model,
    set_model_noise,
    setup_models_only,
)
from rnns_learn_robust_motor_policies.state_utils import (
    get_aligned_vars,
    get_pos_endpoints,
    orthogonal_field,
    vmap_eval_ensemble,
)
from rnns_learn_robust_motor_policies.train_setup import (
    train_pair,
)
from rnns_learn_robust_motor_policies.tree_utils import (
    pp,
    subdict, 
)
from rnns_learn_robust_motor_policies.types import (
    ContextInputDict,
    PertAmpDict,
    TrainingMethodDict,
    TrainStdDict, 
)
```

Log the library versions and the feedbax commit ID, so they appear in any reports generated from this notebook.

```{python}
version_info = log_version_info(
    jax, eqx, plotly, git_modules=(feedbax, rnns_learn_robust_motor_policies)
)
```

### Initialize model database connection

```{python}
db_session = get_db_session()
```

### Hyperparameters

We may want to specify 1) which trained models to load, by their parameters, and 2) how to modify the model parameters for analysis.

```{python}
#| tags: [parameters]

# Specify which trained models to load 
# disturbance_type_load: Literal['curl', 'constant'] = 'curl'
disturbance_stds_load = [0.0, 0.01, 0.02, 0.03, 0.04, 0.08, 0.16, 0.32]
disturbance_type_load: Literal['curl', 'constant'] = 'constant'
feedback_noise_std_load = 0.01
motor_noise_std_load = 0.01
feedback_delay_steps_load = 0
hidden_size = 100

training_methods_load = ['bcs', 'pai-asf']
# disturbance_stds_load = [0.0, 0.5, 1.0, 1.5]

# Specify model parameters to use for analysis (None -> use training value)
# training_method = 'bcs'  # To analyze; subset of `training_methods_load`
training_method = 'pai-asf'
disturbance_type: Optional[Literal['curl', 'constant']] = None
disturbance_train_stds = [0.0, 0.02, 0.04, 0.16]
feedback_noise_std: Optional[float] = None
motor_noise_std: Optional[float] = None

```

```{python}
# context_inputs = [-2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2]
context_inputs = [-2., -1., 0., 1., 2.]

disturbance_amplitudes_by_type = {
    'curl': [0.0, 2.0],
    'constant': [0.0, 0.4],
}
```

These parameters may be passed as strings from the command line in some cases, so we need to cast them to be sure.

```{python}
feedback_noise_std_load = float(feedback_noise_std_load)
motor_noise_std_load = float(motor_noise_std_load)
feedback_delay_steps_load = int(feedback_delay_steps_load)
hidden_size = int(hidden_size)
if feedback_noise_std is not None:
    feedback_noise_std = float(feedback_noise_std)
if motor_noise_std is not None:
    motor_noise_std = float(motor_noise_std)
```

See further below for parameter-based loading of models, as well as the code that modifies the models prior to analysis.

```{python}
params_load = dict(
    origin=TRAIN_NB_ID,
    # disturbance_std=disturbance_std_load,
    disturbance_type=disturbance_type_load,
    feedback_noise_std=feedback_noise_std_load,
    motor_noise_std=motor_noise_std_load,
    feedback_delay_steps=feedback_delay_steps_load,
    hidden_size=hidden_size,
    n_batches=10000,
    learning_rate_0=0.001,
    # intervention_scaleup_batches=[0, 1000],
    training_methods=training_methods_load,
)
```

### RNG setup

```{python}
key = jr.PRNGKey(PROJECT_SEED)
key_init, key_train, key_eval = jr.split(key, 3)
```

## Load and adjust trained models

```{python}
models_base, model_info, replicate_info, n_replicates_included = tree_unzip(
    TrainingMethodDict({
        method_label: TrainStdDict({
            disturbance_std: query_and_load_model(
                db_session,
                setup_task_model_pair,
                params_query=params_load | dict(
                    disturbance_std=disturbance_std,
                    training_method=method_label,
                ),
                noise_stds=dict(
                    feedback=feedback_noise_std,
                    motor=motor_noise_std,
                ),
                exclude_underperformers_by=REPLICATE_CRITERION,
                exclude_method='nan',
            )
            for disturbance_std in disturbance_stds_load
        })
        for method_label in training_methods_load
    })
)

best_replicate, included_replicates = tree_unzip(TrainStdDict({
    std: (
        replicate_info[std]['best_replicates'][REPLICATE_CRITERION],
        replicate_info[std]['included_replicates'][REPLICATE_CRITERION],
    ) 
    for std in disturbance_stds_load
}))
```

Analyze only a subset of training conditions:

```{python}
models_base = subdict(models_base, disturbance_stds_load)
```

## Sort out the evaluation parameters and create a database record

We will either be evaluating on specific disturbance types and noise conditions, or if none are specified here,
keeping the same conditions used during training.

```{python}
all_eval_parameters = jt.map(
    lambda record: use_record_params_where_none(
        dict(
            disturbance_type=disturbance_type,
            feedback_noise_std=feedback_noise_std,
            motor_noise_std=motor_noise_std,
        ), 
        record,
    ),
    model_info,
    is_leaf=is_type(ModelRecord),
)
```

All the relevant model info (e.g. noise stds) except disturbance std should be the same across models, at this point. Assert that this is the case, and keep the parameters for only one of the models.

```{python}
all_eval_params_flat =[tuple(d.items()) for d in all_eval_parameters.values()]

assert len(set(all_eval_params_flat)) == 1

eval_parameters = all_eval_parameters[disturbance_stds_load[0]]

# Later, use this to access the values of hyperparameters, assuming they 
# are shared between models (e.g. `model_info_0.n_steps`)
model_info_0 = model_info[disturbance_stds_load[0]]

disturbance_type = eval_parameters["disturbance_type"]
```

Are any of the system noise scales non-zero?

```{python}
any_system_noise = any(jt.leaves((
    eval_parameters['feedback_noise_std'],
    eval_parameters['motor_noise_std'],
)))
```

### Disturbance amplitudes

```{python}
disturbance_amplitudes = disturbance_amplitudes_by_type[disturbance_type]
```

### Number of evaluations per model and condition

```{python}
n_evals = dict(
    full=5,
    small=5,
)

if not any_system_noise:
     n_evals = jt.map(lambda _: 1, n_evals)
```

### Full parameter dict

```{python}
eval_parameters |= dict(
    disturbance_amplitudes=disturbance_amplitudes,
    context_inputs=context_inputs,
    n_evals=n_evals['full'],
    n_evals_small=n_evals['small'],
)
```

### Initialize a record in the evaluations database

```{python}
eval_info = add_evaluation(
    db_session,
    origin=NB_ID,
    models=model_info,
    eval_parameters=eval_parameters,
    version_info=version_info,
)
```

## Define tasks

### Define the disturbances

```{python}
# Evaluate only a single amplitude, for now;
# we want to see variation over the context input
if disturbance_type == 'curl':  
    def disturbance(amplitude):
        return CurlField.with_params(amplitude=amplitude)    
        
elif disturbance_type == 'constant':   
    def disturbance(field_std):            
        return FixedField.with_params(
            scale=field_std,
            field=orthogonal_field,  
        ) 
          
else:
    raise ValueError(f"Unknown disturbance type: {disturbance_type}")
```

### Set up the base task 

See notebook 1-2a for some explanation of the parameter choices here.

```{python}
task_base, _ = tree_unzip(jt.map( # over disturbance amplitudes
    lambda disturbance_amplitude: schedule_intervenor(  # (implicitly) over train stds
        get_base_task(model_info_0.n_steps),
        jt.leaves(models_base, is_leaf=is_module)[0],
        lambda model: model.step.mechanics,
        disturbance(disturbance_amplitude),
        label=INTERVENOR_LABEL,
        default_active=False,
    ),
    PertAmpDict(zip(disturbance_amplitudes, disturbance_amplitudes)),
))

all_models = jt.map(
    lambda models: add_intervenors(
        models,
        lambda model: model.step.mechanics,
        # The first key is the model stage where to insert the disturbance field;
        # `None` means prior to the first stage.
        # The field parameters will come from the task, so use an amplitude 0.0 placeholder.
        {None: {INTERVENOR_LABEL: disturbance(0.0)}},
    ),
    models_base,
    is_leaf=is_module,
)

# all_models = move_level_to_outside(all_models, TrainingMethodDict)
```

### Set up variants with different context inputs

```{python}
def get_context_input_func(x, n_steps, n_trials):
    return lambda trial_spec, key: (
        jnp.full((n_trials, n_steps), x, dtype=float)
    )

all_tasks = {
    task_size: ContextInputDict({
        context_input: jt.map(
            lambda task: eqx.tree_at( 
                lambda task: task.input_dependencies,
                task, 
                {
                    'context': TrialSpecDependency(
                        get_context_input_func(
                            context_input, 
                            model_info_0.n_steps - 1, 
                            task.n_validation_trials,
                        )
                    )
                },
            ),
            tasks,
            is_leaf=is_module,
        )
        for context_input in context_inputs
    })
    for task_size, tasks in dict(
        full=task_base, 
        small=convert_tasks_to_small(task_base),
    ).items()
}
```

### Assign some things for convenient reference

```{python}
example_task = {
    key: jt.leaves(tasks, is_leaf=is_module)[0]
    for key, tasks in all_tasks.items()
}

trial_specs = jt.map(lambda task: task.validation_trials, example_task, is_leaf=is_module)

pos_endpoints = jt.map(get_pos_endpoints, trial_specs, is_leaf=is_module)
```

## Setup colors for plots

```{python}
trials_colors, trials_colors_dark = get_colors_dicts(
    range(n_evals['full']), COLORSCALES['trials'],
)

# by training condition
disturbance_train_stds_colors, disturbance_train_stds_colors_dark = get_colors_dicts(
    disturbance_train_stds, COLORSCALES['disturbance_train_stds'],
)

# by evaluation condition
# disturbance_amplitudes_colors, disturbance_amplitudes_colors_dark = get_colors_dicts(
#     disturbance_amplitudes, COLORSCALES['disturbance_amplitudes'], 
# )

# by context input 
context_input_colors, context_input_colors_dark = get_colors_dicts(
    context_inputs, COLORSCALES['context_inputs'],
)
```

## Evaluate the trained models on each evaluation task

```{python}
def evaluate_all_states(models, tasks, n_evals):
    return jt.map( # Map over task variants
        lambda models: jt.map(  # Map over training conditions (`models` entries)
            lambda task: vmap_eval_ensemble(models, task, n_evals, key_eval),
            tasks,
            is_leaf=is_module,
        ),
        models,
        is_leaf=is_module,
    )
```

```{python}
all_states_bytes = (
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, all_models, all_tasks['full'], n_evals['full'])),
    tree_struct_bytes(eqx.filter_eval_shape(evaluate_all_states, all_models, all_tasks['small'], n_evals['small'])),
)

print(f"{sum(all_states_bytes) / 1e9:.2f} GB of memory estimated to store all states.")
```

```{python}
# Evaluate all task variants (full and small)
all_states = jt.map(
    lambda n, tasks: evaluate_all_states(all_models, tasks, n),
    n_evals, all_tasks,
)
```

### Project positions, velocities, and forces into reach direction

```{python}
where_vars_to_align = lambda states, pos_endpoints: Responses(
    # Positions with respect to the origin
    states.mechanics.effector.pos - pos_endpoints[0][..., None, :],
    states.mechanics.effector.vel,
    states.efferent.output,
)
```

```{python}   
aligned_vars = {
    variant: jt.map(
        lambda all_states: get_aligned_vars(all_states, where_vars_to_align, pos_endpoints[variant]),
        all_states[variant],
        is_leaf=is_module,
    )
    for variant in all_states
}
```

## Plot aligned trajectories

i.e. for a single reach direction, compare multiple trials/replicates different training conditions; visualize how training on different disturbance strengths affects response.

```{python}
plot_id = "aligned_to_reach_condition"
```

```{python}
n_curves_max = 20

plot_condition_trajectories = partial(
    fbp.trajectories_2D,
    var_labels=RESPONSE_VAR_LABELS,
    axes_labels=('x', 'y'),
    # mode='std',
    mean_trajectory_line_width=3,
    n_curves_max=n_curves_max,
    darken_mean=MEAN_LIGHTEN_FACTOR,
    layout_kws=dict(
        width=900,
        height=400,
        legend_tracegroupgap=1,
        margin_t=75,
    ),
    scatter_kws=dict(
        line_width=1.5, 
        opacity=0.66,
    ),
)
```

### Comparison across context inputs

```{python}
plot_id = "aligned_to_reach_condition/compare_context_inputs"
```

```{python}
plot_vars_stacked = jt.map(
    lambda d: tree_stack(d.values()),
    aligned_vars['small'],
    is_leaf=is_type(ContextInputDict),
)
```

```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['context_inputs'],
        colorscale_axis=0,
        # stride=stride,
        legend_title="Context input",
        legend_labels=context_inputs,
        curves_mode='lines',
        var_endpoint_ms=0,
        scatter_kws=dict(line_width=0.5, opacity=0.3),
        # ref_endpoints=(pos_endpoints, None),
    ),
    plot_vars_stacked,
    is_leaf=is_type(Responses),
)
```


```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    # add_endpoint_traces(fig, POS_ENDPOINTS_ALIGNED['small'], xaxis='x1', yaxis='y1')
    
    fig_parameters = dict(
        training_method=training_method,
        disturbance_type=disturbance_type,
        disturbance_type_train=disturbance_type_load,
        disturbance_train_std=path[0].key,
        disturbance_amplitude=path[1].key,
        # TODO: The number of replicates (`n_replicates_included`) may vary with the disturbance train std!
        # n=min(n_evals['small'] * n_replicates, n_curves_max),
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        model_records=model_info[],
        **fig_parameters,
    )
```

```{python}
for l0 in figs:
    for l1, fig in figs[l0].items():
        print(', '.join([training_method, f"train std: {l0}", f"pert amp: {l1}"]))
        PFW(fig).show()
```

### Comparison of given context values, across training field stds

```{python}
plot_id = "aligned_to_reach_condition/compare_field_stds_by_context_input"
```

```{python}
context_inputs_plot = [-2, 1, 0, 1, 2]

plot_vars = jt.map(
    lambda d: {
        context_input: tree_stack(
            jt.map(lambda arr: arr[context_inputs.index(context_input)], d).values()
        )
        for context_input in context_inputs_plot
    },
    plot_vars_stacked,
    is_leaf=is_type(TrainStdDict),
)
```

```{python}
figs = jt.map(
    partial(
        plot_condition_trajectories, 
        colorscale=COLORSCALES['disturbance_train_stds'],
        colorscale_axis=0,
        # stride=stride,
        legend_title="Train std.",
        legend_labels=disturbance_train_stds,
        curves_mode='lines',
        var_endpoint_ms=0,
        scatter_kws=dict(line_width=0.5, opacity=0.3),
        # ref_endpoints=(pos_endpoints, None),
    ),
    plot_vars,
    is_leaf=is_type(Responses),
)
```


```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):   
    add_endpoint_traces(fig, POS_ENDPOINTS_ALIGNED['small'], xaxis='x1', yaxis='y1')
    
    fig_parameters = dict(
        disturbance_type=disturbance_type,
        disturbance_type_train=disturbance_type_load,
        training_method=training_method,
        context_input=path[0].key,
        disturbance_amplitude=path[1].key,
        # TODO: The number of replicates (`n_replicates_included`) may vary with the disturbance train std!
        # n=min(n_evals['small'] * n_replicates, n_curves_max),
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```

```{python}
for l0 in figs:
    for l1, fig in figs[l0].items():
        print(', '.join([training_method, f"context_input: {l0}", f"pert amp: {l1}"]))
        PFW(fig).show()
```

## Measure distributions

```{python}

# TODO:
measure_keys = [
    "max_parallel_vel_forward",
    "max_orthogonal_vel_signed",
    "max_orthogonal_vel_left",
    # "max_orthogonal_vel_right",  # -2
    "largest_orthogonal_distance",
    "max_orthogonal_distance_left",
    "sum_orthogonal_distance",
    "sum_orthogonal_distance_abs",
    "end_position_error",
    # "end_velocity_error",  # -1
    "max_parallel_force_forward",
    # "sum_parallel_force",  # -2
    # "max_orthogonal_force_right",  # -1
    "sum_orthogonal_force_abs",
    "max_net_force",
    "sum_net_force",
]

custom_measures = custom_measure_labels = dict()

all_measures = subdict(MEASURES, measure_keys) | custom_measures
measure_labels = MEASURE_LABELS | custom_measure_labels

all_measure_values = compute_all_measures(all_measures, aligned_vars['full'])
```

Move the disturbance amplitude level to the outside of each measure.

```{python}
all_measure_values = {
    measure_key: move_level_to_outside(measure_values, PertAmpDict)
    for measure_key, measure_values in all_measure_values.items()
}
```

```{python}
plot_id = "performance_measures/compare_train_and_context_conditions"
```

```{python}
figs = {
    measure_key: PertAmpDict({
        pert_amplitude: get_violins(
            measure_values,
            yaxis_title=measure_labels[measure_key],
            xaxis_title="Context input",
            legend_title="Train std.",
            colors=disturbance_train_stds_colors,
            arr_axis_labels=["Evaluation", "Replicate", "Condition"],
            zero_hline=True,
            layout_kws=dict(
                width=700,
                height=500,
                yaxis_fixedrange=False,
                yaxis_autorange=True,
                # yaxis_range=[0, measure_ranges_lohi[key][1]],
            ),
        )
        for pert_amplitude, measure_values in all_measure_values[measure_key].items()
    })
    for measure_key in all_measure_values
}
```

```{python}
for path, fig in tqdm(figs_flatten_with_paths(figs)):
    print(f"Training method: {training_method}, Measure: {path[0].key}, Pert. amp.: {path[1].key}")
    PFW(fig).show()
```

```{python}
n_dist = int(np.prod(jt.leaves(all_measure_values)[0].shape)) 

for path, fig in tqdm(figs_flatten_with_paths(figs)):
    fig_parameters = dict(
        training_method=training_method,
        measure_name=path[0].key,
        disturbance_amplitude=path[1].key,
        n=n_dist, 
    )
    
    add_evaluation_figure(
        db_session,
        eval_info,
        fig,
        plot_id,
        **fig_parameters,
    )
```


## Output correlation

Note that the readout is fixed, so any change in null vs. potent activity is entirely driven in the recurrent connections by the context input.
